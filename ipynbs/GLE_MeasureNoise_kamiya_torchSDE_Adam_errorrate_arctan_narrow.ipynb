{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-13T06:17:33.394886Z",
     "start_time": "2022-09-13T06:17:25.305986Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14165,
     "status": "ok",
     "timestamp": 1663048733094,
     "user": {
      "displayName": "上道 雅仁.",
      "userId": "02361249356017468398"
     },
     "user_tz": -540
    },
    "id": "jS2L4f3wJYt2",
    "outputId": "d0749e0f-ac53-4930-ff83-1c5f16b2b813"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting optuna\n",
      "  Downloading optuna-3.0.1-py3-none-any.whl (348 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m348.1/348.1 KB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting colorlog\n",
      "  Downloading colorlog-6.7.0-py2.py3-none-any.whl (11 kB)\n",
      "Collecting cmaes>=0.8.2\n",
      "  Downloading cmaes-0.8.2-py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.10.0.0 in /home/uwamichi/.pyenv/versions/3.8.13/lib/python3.8/site-packages (from optuna) (4.2.0)\n",
      "Collecting sqlalchemy>=1.1.0\n",
      "  Downloading SQLAlchemy-1.4.41-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m31.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m31m47.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: PyYAML in /home/uwamichi/.pyenv/versions/3.8.13/lib/python3.8/site-packages (from optuna) (6.0)\n",
      "Collecting cliff\n",
      "  Downloading cliff-4.0.0-py3-none-any.whl (80 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.0/81.0 KB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /home/uwamichi/.pyenv/versions/3.8.13/lib/python3.8/site-packages (from optuna) (21.3)\n",
      "Collecting alembic\n",
      "  Downloading alembic-1.8.1-py3-none-any.whl (209 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.8/209.8 KB\u001b[0m \u001b[31m33.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy in /home/uwamichi/.pyenv/versions/3.8.13/lib/python3.8/site-packages (from optuna) (1.23.1)\n",
      "Collecting tqdm\n",
      "  Downloading tqdm-4.64.1-py2.py3-none-any.whl (78 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 KB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: scipy<1.9.0,>=1.7.0 in /home/uwamichi/.pyenv/versions/3.8.13/lib/python3.8/site-packages (from optuna) (1.8.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/uwamichi/.pyenv/versions/3.8.13/lib/python3.8/site-packages (from packaging>=20.0->optuna) (3.0.9)\n",
      "Collecting greenlet!=0.4.17\n",
      "  Downloading greenlet-1.1.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (157 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m157.1/157.1 KB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: importlib-metadata in /home/uwamichi/.pyenv/versions/3.8.13/lib/python3.8/site-packages (from alembic->optuna) (4.11.4)\n",
      "Requirement already satisfied: importlib-resources in /home/uwamichi/.pyenv/versions/3.8.13/lib/python3.8/site-packages (from alembic->optuna) (5.8.0)\n",
      "Collecting Mako\n",
      "  Downloading Mako-1.2.2-py3-none-any.whl (78 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.7/78.7 KB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting stevedore>=2.0.1\n",
      "  Downloading stevedore-4.0.0-py3-none-any.whl (49 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.5/49.5 KB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting autopage>=0.4.0\n",
      "  Downloading autopage-0.5.1-py3-none-any.whl (29 kB)\n",
      "Collecting cmd2>=1.0.0\n",
      "  Downloading cmd2-2.4.2-py3-none-any.whl (147 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.1/147.1 KB\u001b[0m \u001b[31m36.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting PrettyTable>=0.7.2\n",
      "  Downloading prettytable-3.4.1-py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: wcwidth>=0.1.7 in /home/uwamichi/.pyenv/versions/3.8.13/lib/python3.8/site-packages (from cmd2>=1.0.0->cliff->optuna) (0.2.5)\n",
      "Requirement already satisfied: attrs>=16.3.0 in /home/uwamichi/.pyenv/versions/3.8.13/lib/python3.8/site-packages (from cmd2>=1.0.0->cliff->optuna) (21.4.0)\n",
      "Collecting pyperclip>=1.6\n",
      "  Downloading pyperclip-1.8.2.tar.gz (20 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /home/uwamichi/.pyenv/versions/3.8.13/lib/python3.8/site-packages (from importlib-metadata->alembic->optuna) (3.8.0)\n",
      "Collecting pbr!=2.1.0,>=2.0.0\n",
      "  Downloading pbr-5.10.0-py2.py3-none-any.whl (112 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.4/112.4 KB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: MarkupSafe>=0.9.2 in /home/uwamichi/.pyenv/versions/3.8.13/lib/python3.8/site-packages (from Mako->alembic->optuna) (2.1.1)\n",
      "Using legacy 'setup.py install' for pyperclip, since package 'wheel' is not installed.\n",
      "Installing collected packages: pyperclip, tqdm, PrettyTable, pbr, Mako, greenlet, colorlog, cmd2, cmaes, autopage, stevedore, sqlalchemy, cliff, alembic, optuna\n",
      "  Running setup.py install for pyperclip ... \u001b[?25ldone\n",
      "\u001b[?25hSuccessfully installed Mako-1.2.2 PrettyTable-3.4.1 alembic-1.8.1 autopage-0.5.1 cliff-4.0.0 cmaes-0.8.2 cmd2-2.4.2 colorlog-6.7.0 greenlet-1.1.3 optuna-3.0.1 pbr-5.10.0 pyperclip-1.8.2 sqlalchemy-1.4.41 stevedore-4.0.0 tqdm-4.64.1\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 22.2.2 is available.\n",
      "You should consider upgrading via the '/home/uwamichi/.pyenv/versions/3.8.13/bin/python3.8 -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: torchsde in /home/uwamichi/.pyenv/versions/3.8.13/lib/python3.8/site-packages (0.2.5)\n",
      "Requirement already satisfied: scipy>=1.5 in /home/uwamichi/.pyenv/versions/3.8.13/lib/python3.8/site-packages (from torchsde) (1.8.1)\n",
      "Requirement already satisfied: boltons>=20.2.1 in /home/uwamichi/.pyenv/versions/3.8.13/lib/python3.8/site-packages (from torchsde) (21.0.0)\n",
      "Requirement already satisfied: torch>=1.6.0 in /home/uwamichi/.pyenv/versions/3.8.13/lib/python3.8/site-packages (from torchsde) (1.12.0)\n",
      "Requirement already satisfied: numpy>=1.19.* in /home/uwamichi/.pyenv/versions/3.8.13/lib/python3.8/site-packages (from torchsde) (1.23.1)\n",
      "Requirement already satisfied: trampoline>=0.1.2 in /home/uwamichi/.pyenv/versions/3.8.13/lib/python3.8/site-packages (from torchsde) (0.1.2)\n",
      "Requirement already satisfied: typing-extensions in /home/uwamichi/.pyenv/versions/3.8.13/lib/python3.8/site-packages (from torch>=1.6.0->torchsde) (4.2.0)\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 22.2.2 is available.\n",
      "You should consider upgrading via the '/home/uwamichi/.pyenv/versions/3.8.13/bin/python3.8 -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install optuna\n",
    "!pip install torchsde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-13T06:43:58.565423Z",
     "start_time": "2022-09-13T06:43:58.550779Z"
    },
    "executionInfo": {
     "elapsed": 2320,
     "status": "ok",
     "timestamp": 1663048735407,
     "user": {
      "displayName": "上道 雅仁.",
      "userId": "02361249356017468398"
     },
     "user_tz": -540
    },
    "id": "oqt3xMwXyqHS"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchsde import BrownianInterval, sdeint\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# numbers of cells, freedom, and noise source\n",
    "batch_size, state_size, brownian_size = 25, 6, 2\n",
    "\n",
    "# duration of simulation\n",
    "t_max = 1000\n",
    "\n",
    "# interval of evaluation\n",
    "t_eval = 1\n",
    "\n",
    "# method to solve SDE\n",
    "methodSDE = 'euler'\n",
    "isIto = True\n",
    "\n",
    "# time step to simulate\n",
    "stepSDE = 2e-3\n",
    "\n",
    "# delay (steps with t_eval interval) for autocorrelation calculation\n",
    "delaystep = np.arange(50)\n",
    "\n",
    "# bins for v histogram calculation\n",
    "vbinwidth = 0.1\n",
    "vmin = 0\n",
    "vmax = 5\n",
    "vbins = torch.tensor(np.arange(vmin, vmax+vbinwidth, vbinwidth), dtype=torch.float, device=device)\n",
    "\n",
    "# list of steps at which output is collected\n",
    "ts = torch.arange(0, t_max+t_eval, t_eval, device=device)\n",
    "Nts = ts.size()[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-13T06:43:58.579989Z",
     "start_time": "2022-09-13T06:43:58.570850Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1663048735409,
     "user": {
      "displayName": "上道 雅仁.",
      "userId": "02361249356017468398"
     },
     "user_tz": -540
    },
    "id": "OO-5DcgyaX6m",
    "outputId": "cc6dd522-0ace-4785-b7fc-8993bbaff320"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0000, 0.1000, 0.2000, 0.3000, 0.4000, 0.5000, 0.6000, 0.7000, 0.8000,\n",
      "        0.9000, 1.0000, 1.1000, 1.2000, 1.3000, 1.4000, 1.5000, 1.6000, 1.7000,\n",
      "        1.8000, 1.9000, 2.0000, 2.1000, 2.2000, 2.3000, 2.4000, 2.5000, 2.6000,\n",
      "        2.7000, 2.8000, 2.9000, 3.0000, 3.1000, 3.2000, 3.3000, 3.4000, 3.5000,\n",
      "        3.6000, 3.7000, 3.8000, 3.9000, 4.0000, 4.1000, 4.2000, 4.3000, 4.4000,\n",
      "        4.5000, 4.6000, 4.7000, 4.8000, 4.9000, 5.0000], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(vbins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-13T06:43:58.593371Z",
     "start_time": "2022-09-13T06:43:58.584064Z"
    },
    "executionInfo": {
     "elapsed": 250,
     "status": "ok",
     "timestamp": 1663048779885,
     "user": {
      "displayName": "上道 雅仁.",
      "userId": "02361249356017468398"
     },
     "user_tz": -540
    },
    "id": "NagIZU7WrdjH"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "dirName = \"/home/uwamichi/miura_kamiya/\"\n",
    "\n",
    "csvdir = dirName + \"graph_frameOut_yflip0.142/\"\n",
    "\n",
    "def loadcsv(prefix, surfix):\n",
    "    return np.loadtxt(csvdir+prefix+surfix, delimiter=',', skiprows=1)\n",
    "\n",
    "csv_prefix = 'L2_skip{}_'.format(str(t_eval))\n",
    "csv_surfix = '.csv'\n",
    "\n",
    "savedir = dirName + 'torchSDE_Adam/'\n",
    "os.makedirs(savedir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-13T06:43:58.622704Z",
     "start_time": "2022-09-13T06:43:58.598655Z"
    },
    "executionInfo": {
     "elapsed": 404,
     "status": "ok",
     "timestamp": 1663048780558,
     "user": {
      "displayName": "上道 雅仁.",
      "userId": "02361249356017468398"
     },
     "user_tz": -540
    },
    "id": "_J-blOg-HHtP"
   },
   "outputs": [],
   "source": [
    "class betasigma(nn.Module):\n",
    "    def __init__(self, params):\n",
    "        super(betasigma, self).__init__()\n",
    "\n",
    "        self.layer1 = nn.Linear(1,1,bias=True, device=device)\n",
    "       # self.layer2 = nn.Linear(1,1,bias=False)\n",
    "        self.layer3 = nn.Linear(1,1,bias=False, device=device)\n",
    "\n",
    "        nn.init.constant_(self.layer1.bias, params[0])\n",
    "        nn.init.constant_(self.layer1.weight, params[1])\n",
    "        nn.init.constant_(self.layer3.weight, params[2])\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x has shape [N, 1]\n",
    "        x01 = self.layer1(x)\n",
    "        x2 = self.layer3(torch.square(x))\n",
    "        return x01 + x2\n",
    "\n",
    "\n",
    "class SDE(nn.Module):\n",
    "    noise_type = 'general'\n",
    "    sde_type = 'ito' if isIto else 'stratonovich'\n",
    "\n",
    "    def __init__(self, alpha, beta, gamma, sigma):\n",
    "        super().__init__()\n",
    "        self.alpha = nn.Parameter(torch.tensor([[alpha]], requires_grad=True, device=device))\n",
    "        self.gamma = nn.Parameter(torch.tensor([[gamma]], requires_grad=True, device=device))\n",
    "        self.beta = betasigma(beta)\n",
    "        self.sigma = betasigma(sigma)\n",
    "\n",
    "        self.alpha.register_hook(lambda grad: print('alpha grad', grad))\n",
    "        self.gamma.register_hook(lambda grad: print('gamma grad', grad))\n",
    "        self.beta.layer1.bias.register_hook(lambda grad: print('beta0 grad', grad))\n",
    "        self.beta.layer1.weight.register_hook(lambda grad: print('beta1 grad', grad))\n",
    "        self.beta.layer3.weight.register_hook(lambda grad: print('beta2 grad', grad))\n",
    "        self.sigma.layer1.bias.register_hook(lambda grad: print('sigma0 grad', grad))\n",
    "        self.sigma.layer1.weight.register_hook(lambda grad: print('sigma1 grad', grad))\n",
    "        self.sigma.layer3.weight.register_hook(lambda grad: print('sigma2 grad', grad))\n",
    "\n",
    "        \n",
    "    # Drift\n",
    "    def f(self, t, y):\n",
    "        vsmall = y[:, 2:4]\n",
    "        vlarge = y[:, 4:]\n",
    "\n",
    "        v_abs = torch.norm(vsmall, dim=1, keepdim=True)\n",
    "        \n",
    "        betas = self.beta(v_abs)\n",
    "        dvsmall = -betas*vsmall + self.alpha * vlarge\n",
    "\n",
    "        dvlarge = self.alpha * vsmall - self.gamma * vlarge\n",
    "\n",
    "        return torch.cat((vsmall, dvsmall, dvlarge), 1)  # shape (batch_size, state_size)\n",
    "\n",
    "    # Diffusion\n",
    "    def g(self, t, y):\n",
    "        v_abs = torch.unsqueeze(torch.norm(y[:, 2:4], dim=1, keepdim=True), 2)\n",
    "        sigmas_v = self.sigma(v_abs)\n",
    "        sigmas_vx = torch.cat((sigmas_v, torch.zeros_like(sigmas_v)), 2)\n",
    "        sigmas_vy = torch.cat((torch.zeros_like(sigmas_v), sigmas_v), 2)\n",
    "        sigmas_v = torch.cat((sigmas_vx, sigmas_vy), 1)\n",
    "        sigmas_xy = torch.zeros_like(sigmas_v)\n",
    "        sigmas_vlarge = torch.zeros_like(sigmas_v)\n",
    "\n",
    "        return torch.cat((sigmas_xy, sigmas_v, sigmas_vlarge), 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-13T06:43:58.645080Z",
     "start_time": "2022-09-13T06:43:58.628605Z"
    },
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1663048780559,
     "user": {
      "displayName": "上道 雅仁.",
      "userId": "02361249356017468398"
     },
     "user_tz": -540
    },
    "id": "SyhRFMIOkcWX"
   },
   "outputs": [],
   "source": [
    "class moduleSDE(nn.Module):\n",
    "    def __init__(self, params):\n",
    "        super(moduleSDE, self).__init__()\n",
    "    \n",
    "        self.sde = SDE(params['alpha'], [params['beta0'], params['beta1'], params['beta2']],\n",
    "                       params['gamma'], [params['sigma0'], params['sigma1'], params['sigma2']])\n",
    "        \n",
    "        self.sigmaX = nn.Parameter(torch.tensor([[[params['sigmaX']]]], requires_grad=True, device=device))\n",
    "\n",
    "        self.sigmaX.register_hook(lambda grad: print('sigmaX grad', grad))\n",
    "\n",
    "    def forward(self, yInit, bm, rn):\n",
    "\n",
    "        ys = sdeint(self.sde, yInit, ts, bm=bm, dt=stepSDE, method=methodSDE)\n",
    "\n",
    "        return ys[...,:2] + self.sigmaX * rn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-13T06:43:58.668277Z",
     "start_time": "2022-09-13T06:43:58.655275Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1663048780559,
     "user": {
      "displayName": "上道 雅仁.",
      "userId": "02361249356017468398"
     },
     "user_tz": -540
    },
    "id": "GaGAhb_oCdyc",
    "outputId": "ba87dbdd-2d20-4d9b-d108-bfffe1f6f93e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[0.]], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([[4.]], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([[2.]], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([1.], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([[3.]], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([[6.]], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([5.], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([[7.]], device='cuda:0', requires_grad=True)]\n"
     ]
    }
   ],
   "source": [
    "model = SDE(0.0, [1,2,3], 4.0, [5,6,7])\n",
    "print(list(model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-13T06:43:58.697198Z",
     "start_time": "2022-09-13T06:43:58.671484Z"
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1663048780560,
     "user": {
      "displayName": "上道 雅仁.",
      "userId": "02361249356017468398"
     },
     "user_tz": -540
    },
    "id": "ZiVN0QRhtepT"
   },
   "outputs": [],
   "source": [
    "def binnedSum(y, x, bins):\n",
    "    # out[i] = sum_j(y[j] | bins[i] <= x[j] < bins[i+1])\n",
    "\n",
    "    flg = torch.logical_and(torch.isfinite(x), torch.isfinite(y))\n",
    "\n",
    "    x = x[flg]\n",
    "    y = y[flg]\n",
    "\n",
    "    dbins = (bins[1:] - bins[:-1]).view(1,-1) / 1e3\n",
    "    #print(dbins)\n",
    "    \n",
    "    cond = torch.special.expit((x.view(-1,1) - bins[:-1].view(1, -1))/dbins) - torch.special.expit((x.view(-1,1) - bins[1:].view(1, -1))/dbins)\n",
    "    #print(cond)\n",
    "\n",
    "    #if x.requires_grad:\n",
    "    #    sumc = torch.sum(cond)\n",
    "    #    sumc.backward()\n",
    "\n",
    "    return torch.squeeze(y.view(1,-1) @ cond)\n",
    "\n",
    "def calc_velocity(xy):\n",
    "    return (xy[1:] - xy[:-1])/t_eval\n",
    "\n",
    "def calc_acceleration(v):\n",
    "    return (v[1:] - v[:-1])/t_eval\n",
    "\n",
    "def calc_v_histogram(vabs):\n",
    "    h = binnedSum(torch.ones_like(vabs), vabs, vbins)\n",
    "    h = nn.functional.normalize(h, p=1, dim=0) / (vbins[1:] - vbins[:-1])\n",
    "\n",
    "    #if vabs.requires_grad:\n",
    "    #   sumh = torch.sum(h)\n",
    "    #   sumh.backward()\n",
    "\n",
    "    return h\n",
    "\n",
    "def calc_v_autocorrelation(v):\n",
    "    #print(v.size())\n",
    "    flg = torch.isfinite(v)\n",
    "    #print(torch.count_nonzero(~flg))\n",
    "    if torch.any(~flg):\n",
    "        v[~flg] = torch.zeros_like(v[~flg])\n",
    "        flg = torch.all(flg, dim=2)\n",
    "        Ncor = torch.count_nonzero(torch.logical_and(flg.view(v.size()[0],1,v.size()[1]), flg.view(1,v.size()[0],v.size()[1])), 2)\n",
    "    else:\n",
    "        Ncor = v.size()[1]\n",
    "    #print(Ncor)\n",
    "    v_cor = ((v[:,:,0] @ v[:,:,0].T) + (v[:,:,1] @ v[:,:,1].T)) / Ncor\n",
    "    vac = torch.zeros([delaystep.size], dtype=float, device=device)\n",
    "    for idt, dt in enumerate(delaystep):\n",
    "        vac[idt] = torch.nanmean(torch.diag(v_cor, dt))\n",
    "    return vac\n",
    "\n",
    "def calc_va4(v_normalized, vabs, a):\n",
    "    i_bins = vabs.div(vbinwidth, rounding_mode=\"floor\").to(torch.int32)\n",
    "    Nbins = np.int_(np.divmod(vmax, vbinwidth)[0])\n",
    "    flg_inrange = torch.logical_and(torch.logical_and(vabs>=vmin, i_bins<Nbins), torch.logical_and(torch.all(torch.isfinite(a), 2), torch.all(torch.isfinite(v_normalized), 2)))\n",
    "    j_dummy = torch.zeros_like(i_bins)\n",
    "    a_dummy = torch.ones_like(i_bins)\n",
    "\n",
    "    a_para = v_normalized[:,:,0] * a[:,:,0] + v_normalized[:,:,1] * a[:,:,1]\n",
    "    a_perp = v_normalized[:,:,0] * a[:,:,1] - v_normalized[:,:,1] * a[:,:,0]\n",
    "\n",
    "    inds_sparse = torch.concat((i_bins[flg_inrange].view(1,-1), j_dummy[flg_inrange].view(1,-1)), 0)\n",
    "    #print('inds', inds_sparse)\n",
    "\n",
    "    a_para_sum = torch.sparse_coo_tensor(inds_sparse, a_para[flg_inrange], size=[Nbins,1], dtype=float, device=device).to_dense()\n",
    "    a_perp_sum = torch.sparse_coo_tensor(inds_sparse, a_perp[flg_inrange], size=[Nbins,1], dtype=float, device=device).to_dense()\n",
    "\n",
    "    count_vabs = torch.sparse_coo_tensor(inds_sparse, a_dummy[flg_inrange], size=[Nbins,1], dtype=float, device=device).to_dense()\n",
    "    #print('count', count_vabs)\n",
    "\n",
    "    a_para_mean = a_para_sum/count_vabs #torch.maximum(count_vabs, torch.tensor([1.0]))\n",
    "    #print(a_para_mean)\n",
    "    a_perp_mean = a_perp_sum/count_vabs #torch.maximum(count_vabs, torch.tensor([1.0]))\n",
    "\n",
    "    d_a_para_mean = a_para[flg_inrange] - a_para_mean[:,0][i_bins[flg_inrange].to(torch.int64)]\n",
    "    d_a_perp_mean = a_perp[flg_inrange] - a_perp_mean[:,0][i_bins[flg_inrange].to(torch.int64)]\n",
    "    #print(a_para_var)\n",
    "    #print(a_perp_var)\n",
    "\n",
    "    flg_for_std = (count_vabs>=2)[:,0][i_bins[flg_inrange].to(torch.int64)]\n",
    "\n",
    "    d_a_para_mean = d_a_para_mean[flg_for_std]\n",
    "    d_a_perp_mean = d_a_perp_mean[flg_for_std]\n",
    "\n",
    "    inds_sparse = inds_sparse.T[flg_for_std].T\n",
    "\n",
    "    a_para_std = torch.sqrt(torch.sparse_coo_tensor(inds_sparse, torch.square(d_a_para_mean), size=[Nbins,1], dtype=float, device=device).to_dense() / (count_vabs-1))#torch.maximum(count_vabs-1, torch.tensor([1.0])))\n",
    "    a_perp_std = torch.sqrt(torch.sparse_coo_tensor(inds_sparse, torch.square(d_a_perp_mean), size=[Nbins,1], dtype=float, device=device).to_dense() / (count_vabs-1))#torch.maximum(count_vabs-1, torch.tensor([1.0])))\n",
    "\n",
    "    #print(a_para_std)\n",
    "    #print(a_perp_std)\n",
    "\n",
    "    #if vabs.requires_grad:\n",
    "        #a_para_var.register_hook(lambda grad: print('a_para_var grad', grad))\n",
    "        #a_perp_var.register_hook(lambda grad: print('a_perp_var grad', grad))\n",
    "\n",
    "    return a_para_mean, a_perp_mean, a_para_std, a_perp_std#, count_vabs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-13T06:43:58.804718Z",
     "start_time": "2022-09-13T06:43:58.704062Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8381,
     "status": "ok",
     "timestamp": 1663048788930,
     "user": {
      "displayName": "上道 雅仁.",
      "userId": "02361249356017468398"
     },
     "user_tz": -540
    },
    "id": "ZHtZpdgYO4bV",
    "outputId": "62c4f1d9-36df-46e8-cb3b-3d6dcdcb6c52"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([7.7382e-02, 1.9442e-01, 3.1161e-01, 3.9839e-01, 5.0349e-01, 5.6932e-01,\n",
      "        6.3120e-01, 7.1836e-01, 7.8103e-01, 8.0358e-01, 7.9743e-01, 7.4059e-01,\n",
      "        6.6378e-01, 5.5074e-01, 4.8309e-01, 3.6264e-01, 2.9296e-01, 2.4451e-01,\n",
      "        1.8147e-01, 1.4667e-01, 1.1110e-01, 8.5229e-02, 7.8882e-02, 6.4705e-02,\n",
      "        4.6304e-02, 4.0165e-02, 2.9394e-02, 2.3630e-02, 1.5841e-02, 1.0321e-02,\n",
      "        8.7670e-03, 6.7941e-03, 1.9412e-03, 4.2059e-03, 3.5588e-03, 3.5588e-03,\n",
      "        2.5882e-03, 1.9412e-03, 1.6176e-03, 1.6177e-03, 6.4706e-04, 3.2353e-04,\n",
      "        1.6176e-03, 0.0000e+00, 3.2353e-04, 6.4706e-04, 9.7059e-04, 0.0000e+00,\n",
      "        0.0000e+00, 6.4706e-04], device='cuda:0')\n",
      "tensor([1.4615, 1.2273, 1.1752, 1.1293, 1.0923, 1.0602, 1.0255, 0.9928, 0.9759,\n",
      "        0.9502, 0.9306, 0.9140, 0.8981, 0.8798, 0.8652, 0.8493, 0.8334, 0.8222,\n",
      "        0.8067, 0.7953, 0.7796, 0.7705, 0.7567, 0.7428, 0.7337, 0.7200, 0.7034,\n",
      "        0.6944, 0.6840, 0.6693, 0.6632, 0.6571, 0.6506, 0.6373, 0.6277, 0.6207,\n",
      "        0.6125, 0.5980, 0.5950, 0.5875, 0.5753, 0.5647, 0.5578, 0.5513, 0.5423,\n",
      "        0.5365, 0.5318, 0.5246, 0.5144, 0.5107], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "tensor([[-1.9393e-02, -2.2693e-02,  2.9426e-01,  2.9498e-01],\n",
      "        [-2.2598e-02, -1.7053e-03,  2.9900e-01,  3.1958e-01],\n",
      "        [-1.9371e-02, -1.6612e-02,  5.6726e-01,  4.4850e-01],\n",
      "        [ 3.0748e-02, -1.2093e-02,  3.5723e-01,  3.1023e-01],\n",
      "        [ 2.6655e-02, -1.6621e-02,  4.0142e-01,  3.6954e-01],\n",
      "        [ 4.1019e-02,  3.7665e-03,  3.7173e-01,  3.7385e-01],\n",
      "        [ 2.9419e-02,  5.2978e-03,  3.7960e-01,  3.5172e-01],\n",
      "        [ 1.5984e-02, -1.4418e-02,  3.7653e-01,  3.6976e-01],\n",
      "        [ 4.2992e-02, -9.1624e-03,  4.5007e-01,  3.7815e-01],\n",
      "        [-9.7018e-03, -1.6308e-02,  3.8955e-01,  3.3338e-01],\n",
      "        [-3.3102e-02, -1.5984e-03,  4.4881e-01,  3.6179e-01],\n",
      "        [-3.2280e-02,  8.3006e-03,  3.9669e-01,  3.4577e-01],\n",
      "        [-9.0745e-02, -1.4153e-04,  4.2685e-01,  3.6820e-01],\n",
      "        [-1.3679e-01, -3.7557e-03,  4.0134e-01,  3.6040e-01],\n",
      "        [-1.6891e-01, -3.9895e-03,  4.4452e-01,  3.5225e-01],\n",
      "        [-2.2292e-01,  1.1962e-02,  4.3924e-01,  3.6010e-01],\n",
      "        [-2.6433e-01,  2.0189e-02,  4.6096e-01,  3.5261e-01],\n",
      "        [-2.9307e-01, -1.2407e-02,  5.2289e-01,  4.2085e-01],\n",
      "        [-3.1284e-01,  5.2788e-04,  5.1778e-01,  3.6941e-01],\n",
      "        [-3.6083e-01, -1.9096e-02,  5.4413e-01,  3.8449e-01],\n",
      "        [-3.5241e-01,  1.9485e-02,  5.4234e-01,  3.7055e-01],\n",
      "        [-4.1316e-01, -3.6041e-02,  5.6399e-01,  4.3412e-01],\n",
      "        [-4.4222e-01,  1.6082e-02,  6.7227e-01,  4.1762e-01],\n",
      "        [-5.0193e-01, -2.9446e-02,  6.4030e-01,  4.0322e-01],\n",
      "        [-4.1263e-01,  2.2502e-02,  5.7535e-01,  3.3932e-01],\n",
      "        [-6.0397e-01,  3.4003e-04,  5.8788e-01,  3.4270e-01],\n",
      "        [-7.0563e-01, -8.2850e-03,  7.0002e-01,  3.7798e-01],\n",
      "        [-7.4207e-01,  2.4857e-02,  6.5806e-01,  3.9044e-01],\n",
      "        [-9.9758e-01, -2.1438e-02,  7.8587e-01,  3.9765e-01],\n",
      "        [-1.0413e+00, -3.4262e-02,  5.5983e-01,  3.3924e-01],\n",
      "        [-9.7411e-01, -8.4978e-02,  9.4732e-01,  4.4687e-01],\n",
      "        [-1.2507e+00,  4.0216e-02,  1.4376e+00,  3.9177e-01],\n",
      "        [-1.4889e+00,  1.9244e-01,  4.8581e-01,  2.8443e-01],\n",
      "        [-1.0797e+00,  1.2813e-01,  9.0146e-01,  2.8690e-01],\n",
      "        [-1.4049e+00, -1.0056e-03,  5.3312e-01,  3.7031e-01],\n",
      "        [-1.7116e+00,  1.3740e-01,  7.9739e-01,  5.6027e-01],\n",
      "        [-1.0145e+00, -4.2079e-02,  7.0944e-01,  3.0480e-01],\n",
      "        [-1.0234e+00,  2.5731e-01,  6.6910e-01,  6.3784e-01],\n",
      "        [-1.9097e+00,  1.7395e-01,  1.0935e+00,  2.2497e-01],\n",
      "        [-1.2552e+00,  7.2781e-02,  6.8792e-01,  4.4819e-01],\n",
      "        [-3.2986e+00, -3.4221e-01,         nan,         nan],\n",
      "        [-1.2520e+00, -1.0546e-01,         nan,         nan],\n",
      "        [-3.1170e+00,  9.3221e-02,  2.2980e+00,  7.8269e-01],\n",
      "        [        nan,         nan, -0.0000e+00, -0.0000e+00],\n",
      "        [-2.2959e+00, -7.2525e-02,         nan,         nan],\n",
      "        [-2.7187e+00,  2.1283e-01,  1.3674e+00,  2.6526e-01],\n",
      "        [-1.5748e+00, -2.0694e-02,  3.7296e-01,  4.1456e-01],\n",
      "        [        nan,         nan, -0.0000e+00, -0.0000e+00],\n",
      "        [        nan,         nan, -0.0000e+00, -0.0000e+00]], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "tensor(5.5764, device='cuda:0')\n",
      "tensor([1.4615, 1.2273, 1.1752, 1.1293, 1.0923, 1.0602, 1.0255, 0.9928, 0.9759,\n",
      "        0.9502, 0.9306, 0.9140, 0.8981, 0.8798, 0.8652, 0.8493, 0.8334, 0.8222,\n",
      "        0.8067, 0.7953, 0.7796, 0.7705, 0.7567, 0.7428, 0.7337, 0.7200, 0.7034,\n",
      "        0.6944, 0.6840, 0.6693, 0.6632, 0.6571, 0.6506, 0.6373, 0.6277, 0.6207,\n",
      "        0.6125, 0.5980, 0.5950, 0.5875, 0.5753, 0.5647, 0.5578, 0.5513, 0.5423,\n",
      "        0.5365, 0.5318, 0.5246, 0.5144, 0.5107], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "tensor([60.3086,  0.3812, 22.5552,  6.6702], device='cuda:0',\n",
      "       dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "\n",
    "    x_csv = loadcsv(csv_prefix, 'x'+csv_surfix)\n",
    "    y_csv = loadcsv(csv_prefix, 'y'+csv_surfix)\n",
    "\n",
    "    xy_csv = torch.tensor(np.concatenate((np.expand_dims(x_csv, 2), np.expand_dims(y_csv, 2)), 2), dtype=torch.float, device=device)\n",
    "\n",
    "    v_csv = calc_velocity(xy_csv)\n",
    "    a_csv = calc_acceleration(v_csv)\n",
    "\n",
    "    vabs_csv = torch.norm(v_csv, dim=2)\n",
    "\n",
    "    v_normalized_csv = nn.functional.normalize(v_csv, dim=2)\n",
    "\n",
    "    hist_csv = calc_v_histogram(vabs_csv)\n",
    "    vac_csv = calc_v_autocorrelation(v_csv)\n",
    "    va4_csv = calc_va4(v_normalized_csv[:-1], vabs_csv[:-1], a_csv)\n",
    "    va4_csv = torch.concat(va4_csv, 1)\n",
    "\n",
    "    print(hist_csv)\n",
    "    print(vac_csv)\n",
    "    print(va4_csv)\n",
    "\n",
    "    hist_norm = torch.nansum(hist_csv**2)\n",
    "    print(hist_norm)\n",
    "\n",
    "    vac_norm = torch.abs(vac_csv)\n",
    "    print(vac_norm)\n",
    "\n",
    "    va4_norm = torch.nansum(va4_csv**2, dim=0)\n",
    "    print(va4_norm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-13T06:43:58.818438Z",
     "start_time": "2022-09-13T06:43:58.814974Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 37,
     "status": "ok",
     "timestamp": 1663048788933,
     "user": {
      "displayName": "上道 雅仁.",
      "userId": "02361249356017468398"
     },
     "user_tz": -540
    },
    "id": "NxeNg8ZYLBYj",
    "outputId": "36363416-2cf8-4914-cbdc-6b5891e4768c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "print(xy_csv.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-13T06:43:58.842727Z",
     "start_time": "2022-09-13T06:43:58.827251Z"
    },
    "executionInfo": {
     "elapsed": 29,
     "status": "ok",
     "timestamp": 1663048788935,
     "user": {
      "displayName": "上道 雅仁.",
      "userId": "02361249356017468398"
     },
     "user_tz": -540
    },
    "id": "9DckwdRcsdC9"
   },
   "outputs": [],
   "source": [
    "def compare_v_histogram(vabs):\n",
    "    h = calc_v_histogram(vabs)\n",
    "    dh = h - hist_csv\n",
    "    flg = torch.isfinite(dh)\n",
    "    hist_norm_ = torch.nansum(hist_csv[flg]**2)\n",
    "\n",
    "    dhsq = torch.square(dh[flg])\n",
    "    dhsumsq = torch.sum(dhsq)\n",
    "\n",
    "    out = dhsumsq / hist_norm_\n",
    "\n",
    "    #print('vhist', out)\n",
    "    #if vabs.requires_grad:\n",
    "        #h.register_hook(lambda grad: print('hist grad', grad))\n",
    "        #dh.register_hook(lambda grad: print('d hist grad', grad))\n",
    "        #dhsq.register_hook(lambda grad: print('d hist square grad', grad))\n",
    "        #dhsumsq.register_hook(lambda grad: print('d hist square sum grad', grad))\n",
    "\n",
    "        #out.backward()\n",
    "    #print(2*dh[flg]/hist_norm_)\n",
    "    return out\n",
    "\n",
    "def compare_v_autocorrelation(v):\n",
    "    vac = calc_v_autocorrelation(v)\n",
    "    #if v.requires_grad:\n",
    "      #vac.register_hook(lambda grad: print('vac grad', grad))\n",
    "    dvac = vac - vac_csv\n",
    "    flg = torch.isfinite(dvac)\n",
    "    vac_norm_ = torch.abs(vac_csv[flg])\n",
    "    return torch.mean(torch.abs(dvac[flg])/vac_norm_)\n",
    "\n",
    "def compare_acceleration(v_normalized, vabs, a):\n",
    "\n",
    "    a_para_mean, a_perp_mean, a_para_std, a_perp_std = calc_va4(v_normalized, vabs, a)\n",
    "    #print(a_para_mean - va4_csv[:,:1])\n",
    "\n",
    "    flg0 = torch.logical_and(torch.isfinite(a_para_mean), torch.isfinite(va4_csv[:,:1]))\n",
    "    flg1 = torch.logical_and(torch.isfinite(a_perp_mean), torch.isfinite(va4_csv[:,1:2]))\n",
    "    flg2 = torch.logical_and(torch.isfinite(a_para_std), torch.isfinite(va4_csv[:,2:3]))\n",
    "    flg3 = torch.logical_and(torch.isfinite(a_perp_std), torch.isfinite(va4_csv[:,3:4]))\n",
    "\n",
    "    #flg0 = torch.logical_and(count_vabs>=1, va4_csv[:,4:]>=1)\n",
    "    #flg1 = torch.logical_and(count_vabs>=1, va4_csv[:,4:]>=1)\n",
    "    #flg2 = torch.logical_and(count_vabs>=2, va4_csv[:,4:]>=2)\n",
    "    #flg3 = torch.logical_and(count_vabs>=2, va4_csv[:,4:]>=2)\n",
    "\n",
    "    a_para_mean = torch.where(flg0, a_para_mean, torch.tensor(0.0, device=device))\n",
    "    a_perp_mean = torch.where(flg1, a_perp_mean, torch.tensor(0.0, device=device))\n",
    "    a_para_std = torch.where(flg2, a_para_std, torch.tensor(0.0, device=device))\n",
    "    a_perp_std = torch.where(flg3, a_perp_std, torch.tensor(0.0, device=device))\n",
    "\n",
    "    d_para_mean = a_para_mean - torch.where(flg0, va4_csv[:,:1], torch.tensor(0.0, device=device))\n",
    "    d_perp_mean = a_perp_mean - torch.where(flg1, va4_csv[:,1:2], torch.tensor(0.0, device=device))\n",
    "    d_para_std = a_para_std - torch.where(flg2, va4_csv[:,2:3], torch.tensor(0.0, device=device))\n",
    "    d_perp_std = a_perp_std - torch.where(flg3, va4_csv[:,3:4], torch.tensor(0.0, device=device))\n",
    "\n",
    "    a_para_mean_norm = torch.sum(torch.abs(va4_csv[:,:1][flg0]))\n",
    "    a_perp_mean_norm = torch.sum(torch.abs(va4_csv[:,1:2][flg1]))\n",
    "    a_para_std_norm = torch.sum(torch.abs(va4_csv[:,2:3][flg2]))\n",
    "    a_perp_std_norm = torch.sum(torch.abs(va4_csv[:,3:4][flg3]))\n",
    "\n",
    "    dif_para_mean = torch.sum(torch.abs(d_para_mean[flg0])) / a_para_mean_norm\n",
    "    dif_perp_mean = torch.sum(torch.abs(d_perp_mean[flg1])) / a_perp_mean_norm\n",
    "    dif_para_std = torch.sum(torch.abs(d_para_std[flg2])) / a_para_std_norm\n",
    "    dif_perp_std = torch.sum(torch.abs(d_perp_std[flg3])) / a_perp_std_norm\n",
    "\n",
    "    #if vabs.requires_grad:\n",
    "        #a_para_mean.register_hook(lambda grad: print('a_para_mean grad', grad))\n",
    "        #a_perp_mean.register_hook(lambda grad: print('a_perp_mean grad', grad))\n",
    "        #a_para_std.register_hook(lambda grad: print('a_para_std grad', grad))\n",
    "        #a_perp_std.register_hook(lambda grad: print('a_perp_std grad', grad))\n",
    "\n",
    "        #dif_para_std.backward()\n",
    "        #d_para_std[flg2][0].backward()\n",
    "\n",
    "    return dif_para_mean, dif_perp_mean, dif_para_std, dif_perp_std\n",
    "\n",
    "def eval_function(xy):\n",
    "\n",
    "    v = calc_velocity(xy)\n",
    "    a = calc_acceleration(v)\n",
    "\n",
    "    vabs = torch.norm(v, dim=2)\n",
    "\n",
    "    v_normalized = nn.functional.normalize(v, dim=2)\n",
    "    #j_vals = torch.zeros([5])\n",
    "\n",
    "    j_hist = compare_v_histogram(vabs)\n",
    "    j_vac = compare_v_autocorrelation(v)\n",
    "    j_av0, j_av1, j_av2, j_av3 = compare_acceleration(v_normalized[:-1], vabs[:-1], a)\n",
    "\n",
    "    j_vals = torch.concat((j_hist.view(1), j_vac.view(1),\n",
    "                           j_av0.view(1), j_av2.view(1), j_av3.view(1)))\n",
    "    # warning: if a member of concat has NaN grad, others also show NaN grad even if its grad was finite before concat.\n",
    "\n",
    "    #print('jhist', j_hist)\n",
    "\n",
    "    #if xy.requires_grad:\n",
    "        #sumv = torch.sum(v)\n",
    "        #sumv.backward()\n",
    "        #j_vals[0].backward()\n",
    "        #j_av2.backward()\n",
    "\n",
    "    return j_vals\n",
    "\n",
    "def treatOuts(x):\n",
    "    return torch.arctan(6*(x**3))*2/np.pi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-13T06:43:58.851781Z",
     "start_time": "2022-09-13T06:43:58.845357Z"
    },
    "executionInfo": {
     "elapsed": 27,
     "status": "ok",
     "timestamp": 1663048788938,
     "user": {
      "displayName": "上道 雅仁.",
      "userId": "02361249356017468398"
     },
     "user_tz": -540
    },
    "id": "G_4QjrFjOmIS"
   },
   "outputs": [],
   "source": [
    "class customLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # 初期化処理\n",
    "        # self.param = ... \n",
    "\n",
    "    def forward(self, outputs):\n",
    "        '''\n",
    "        outputs: 予測結果(ネットワークの出力)\n",
    "         targets: 正解\n",
    "        '''\n",
    "        # 損失の計算\n",
    "\n",
    "        J = eval_function(outputs)\n",
    "        print(J)\n",
    "\n",
    "        J = J * torch.tensor([1.0, 10.0, 1.0, 1.0, 1.0], device=device)\n",
    "\n",
    "        #J0.register_hook(lambda grad: print('J_hist back', grad))\n",
    "        #J1.register_hook(lambda grad: print('J_vac back', grad))\n",
    "        #J2.register_hook(lambda grad: print('J_va_para_mean back', grad))\n",
    "        #J3.register_hook(lambda grad: print('J_va_para_sd back', grad))\n",
    "        #J4.register_hook(lambda grad: print('J_va_perp_sd back', grad))\n",
    "\n",
    "        return torch.sum(J)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-13T06:43:58.894663Z",
     "start_time": "2022-09-13T06:43:58.854111Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2695,
     "status": "ok",
     "timestamp": 1663048791609,
     "user": {
      "displayName": "上道 雅仁.",
      "userId": "02361249356017468398"
     },
     "user_tz": -540
    },
    "id": "1fhAxWwctN2c",
    "outputId": "6cdbac9b-6219-46b0-989d-03c96cd6a033"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 1.4556e-16, 1.9564e-16],\n",
      "       device='cuda:0', dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "J_csv = eval_function(xy_csv)\n",
    "\n",
    "print(J_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-13T06:43:58.905771Z",
     "start_time": "2022-09-13T06:43:58.896955Z"
    },
    "executionInfo": {
     "elapsed": 245,
     "status": "ok",
     "timestamp": 1663048791847,
     "user": {
      "displayName": "上道 雅仁.",
      "userId": "02361249356017468398"
     },
     "user_tz": -540
    },
    "id": "Lp0xexKv5irE"
   },
   "outputs": [],
   "source": [
    "def printparams(model):\n",
    "    alpha = model.sde.alpha[0,0].cpu().detach().numpy()\n",
    "    beta0 = model.sde.beta.layer1.bias[0].cpu().detach().numpy()\n",
    "    beta1 = model.sde.beta.layer1.weight[0,0].cpu().detach().numpy()\n",
    "    beta2 = model.sde.beta.layer3.weight[0,0].cpu().detach().numpy()\n",
    "    gamma = model.sde.gamma[0,0].cpu().detach().numpy()\n",
    "    sigma0 = model.sde.sigma.layer1.bias[0].cpu().detach().numpy()\n",
    "    sigma1 = model.sde.sigma.layer1.weight[0,0].cpu().detach().numpy()\n",
    "    sigma2 = model.sde.sigma.layer3.weight[0,0].cpu().detach().numpy()\n",
    "    sigmaX = model.sigmaX[0,0,0].cpu().detach().numpy()\n",
    "\n",
    "    print('alpha: {}, beta0: {}, beta1: {}, beta2: {}, '.format(alpha, beta0, beta1, beta2))\n",
    "    print('gamma: {}, sigma0: {}, sigma1: {}, sigma2: {}, sigmaX: {}'.format(gamma, sigma0, sigma1, sigma2, sigmaX))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-09-13T06:43:58.591Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9BGtMCDyJdch",
    "outputId": "bacb5294-f7f2-43a7-c118-8aef67e25890"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha: 0.07410000264644623, beta0: 0.11599999666213989, beta1: 0.0, beta2: 0.0, \n",
      "gamma: 0.0640999972820282, sigma0: 0.26600000262260437, sigma1: 0.0, sigma2: 0.0, sigmaX: 0.1550000011920929\n",
      "forward done\n",
      "tensor([0.0085, 0.0304, 0.4758, 0.5312, 0.3572], device='cuda:0',\n",
      "       dtype=torch.float64, grad_fn=<CatBackward0>)\n",
      "sigmaX grad tensor([[[-1.6259]]], device='cuda:0')\n",
      "sigma2 grad tensor([[-59.7364]], device='cuda:0')\n",
      "sigma0 grad tensor([-48.5446], device='cuda:0')\n",
      "sigma1 grad tensor([[-48.2326]], device='cuda:0')\n",
      "gamma grad tensor([[246.0461]], device='cuda:0')\n",
      "alpha grad tensor([[-510.3936]], device='cuda:0')\n",
      "beta2 grad tensor([[611.8146]], device='cuda:0')\n",
      "beta0 grad tensor([257.2993], device='cuda:0')\n",
      "beta1 grad tensor([[374.8483]], device='cuda:0')\n",
      "Epoch 0 | Loss: 1.6772\n",
      "alpha: 0.07510000467300415, beta0: 0.11499999463558197, beta1: -0.0009999999310821295, beta2: -0.0009999999310821295, \n",
      "gamma: 0.06309999525547028, sigma0: 0.2669999897480011, sigma1: 0.0010000000474974513, sigma2: 0.0010000000474974513, sigmaX: 0.15600000321865082\n",
      "forward done\n",
      "tensor([0.0294, 0.5650, 0.7673, 0.3397, 0.2398], device='cuda:0',\n",
      "       dtype=torch.float64, grad_fn=<CatBackward0>)\n",
      "sigmaX grad tensor([[[2.4745]]], device='cuda:0')\n",
      "sigma2 grad tensor([[403.8822]], device='cuda:0')\n",
      "sigma0 grad tensor([167.3470], device='cuda:0')\n",
      "sigma1 grad tensor([[231.0851]], device='cuda:0')\n",
      "gamma grad tensor([[-1297.9360]], device='cuda:0')\n",
      "alpha grad tensor([[2499.8433]], device='cuda:0')\n",
      "beta2 grad tensor([[-5280.5068]], device='cuda:0')\n",
      "beta0 grad tensor([-1191.3931], device='cuda:0')\n",
      "beta1 grad tensor([[-2333.8418]], device='cuda:0')\n",
      "Epoch 1 | Loss: 7.0261\n",
      "alpha: 0.07450486719608307, beta0: 0.11558599770069122, beta1: -0.0003714775084517896, beta2: -0.00033788406290113926, \n",
      "gamma: 0.06370638310909271, sigma0: 0.26647189259529114, sigma1: 0.00040838620043359697, sigma2: 0.0003618543269112706, sigmaX: 0.15574583411216736\n",
      "forward done\n",
      "tensor([0.0148, 0.0420, 0.5046, 0.4771, 0.3298], device='cuda:0',\n",
      "       dtype=torch.float64, grad_fn=<CatBackward0>)\n",
      "sigmaX grad tensor([[[-0.4359]]], device='cuda:0')\n",
      "sigma2 grad tensor([[21.7223]], device='cuda:0')\n",
      "sigma0 grad tensor([17.3033], device='cuda:0')\n",
      "sigma1 grad tensor([[17.4875]], device='cuda:0')\n",
      "gamma grad tensor([[-200.9716]], device='cuda:0')\n",
      "alpha grad tensor([[368.5474]], device='cuda:0')\n",
      "beta2 grad tensor([[-432.8965]], device='cuda:0')\n",
      "beta0 grad tensor([-160.3889], device='cuda:0')\n",
      "beta1 grad tensor([[-247.4916]], device='cuda:0')\n",
      "Epoch 2 | Loss: 1.7467\n",
      "alpha: 0.07395818084478378, beta0: 0.11611849069595337, beta1: 0.00017828373529482633, beta2: 0.0002241194451926276, \n",
      "gamma: 0.0642659142613411, sigma0: 0.26600250601768494, sigma1: -9.490003139944747e-05, sigma2: -0.0001646940945647657, sigmaX: 0.15564456582069397\n",
      "forward done\n",
      "tensor([0.0239, 0.1547, 0.5367, 0.5132, 0.3540], device='cuda:0',\n",
      "       dtype=torch.float64, grad_fn=<CatBackward0>)\n",
      "sigmaX grad tensor([[[0.2470]]], device='cuda:0')\n",
      "sigma2 grad tensor([[-73.7339]], device='cuda:0')\n",
      "sigma0 grad tensor([-62.8509], device='cuda:0')\n",
      "sigma1 grad tensor([[-60.2514]], device='cuda:0')\n",
      "gamma grad tensor([[360.9695]], device='cuda:0')\n",
      "alpha grad tensor([[-725.5637]], device='cuda:0')\n",
      "beta2 grad tensor([[833.3370]], device='cuda:0')\n",
      "beta0 grad tensor([354.1700], device='cuda:0')\n",
      "beta1 grad tensor([[511.0184]], device='cuda:0')\n",
      "Epoch 3 | Loss: 2.9748\n",
      "alpha: 0.07368474453687668, beta0: 0.11637657135725021, beta1: 0.000496256398037076, beta2: 0.0005891494802199304, \n",
      "gamma: 0.06455666571855545, sigma0: 0.265837162733078, sigma1: -0.00035091384779661894, sigma2: -0.0004859098989982158, sigmaX: 0.15551403164863586\n",
      "forward done\n",
      "tensor([0.0296, 0.2255, 0.4461, 0.5415, 0.3949], device='cuda:0',\n",
      "       dtype=torch.float64, grad_fn=<CatBackward0>)\n",
      "sigmaX grad tensor([[[-0.2952]]], device='cuda:0')\n",
      "sigma2 grad tensor([[-54.2776]], device='cuda:0')\n",
      "sigma0 grad tensor([-54.2493], device='cuda:0')\n",
      "sigma1 grad tensor([[-47.7568]], device='cuda:0')\n",
      "gamma grad tensor([[323.6138]], device='cuda:0')\n",
      "alpha grad tensor([[-646.0444]], device='cuda:0')\n",
      "beta2 grad tensor([[633.4317]], device='cuda:0')\n",
      "beta0 grad tensor([312.7338], device='cuda:0')\n",
      "beta1 grad tensor([[419.7726]], device='cuda:0')\n",
      "Epoch 4 | Loss: 3.6676\n",
      "alpha: 0.07358818501234055, beta0: 0.11645875126123428, beta1: 0.0006681343074887991, beta2: 0.0008319086045958102, \n",
      "gamma: 0.064671590924263, sigma0: 0.26585590839385986, sigma1: -0.0004584542184602469, sigma2: -0.0006843465962447226, sigmaX: 0.15545769035816193\n",
      "forward done\n",
      "tensor([0.0253, 0.2279, 0.4141, 0.5407, 0.4119], device='cuda:0',\n",
      "       dtype=torch.float64, grad_fn=<CatBackward0>)\n",
      "sigmaX grad tensor([[[0.6526]]], device='cuda:0')\n",
      "sigma2 grad tensor([[-53.3076]], device='cuda:0')\n",
      "sigma0 grad tensor([-54.1385], device='cuda:0')\n",
      "sigma1 grad tensor([[-47.5622]], device='cuda:0')\n",
      "gamma grad tensor([[297.2364]], device='cuda:0')\n",
      "alpha grad tensor([[-605.7064]], device='cuda:0')\n",
      "beta2 grad tensor([[619.9473]], device='cuda:0')\n",
      "beta0 grad tensor([297.3652], device='cuda:0')\n",
      "beta1 grad tensor([[404.4938]], device='cuda:0')\n",
      "Epoch 5 | Loss: 3.6709\n",
      "alpha: 0.07361923158168793, beta0: 0.11641263961791992, beta1: 0.0007296727853827178, beta2: 0.0009804976871237159, \n",
      "gamma: 0.06466148048639297, sigma0: 0.26601219177246094, sigma1: -0.00045130233047530055, sigma2: -0.0007879746844992042, sigmaX: 0.15529963374137878\n",
      "forward done\n",
      "tensor([0.0314, 0.3403, 0.3602, 0.6095, 0.4518], device='cuda:0',\n",
      "       dtype=torch.float64, grad_fn=<CatBackward0>)\n",
      "sigmaX grad tensor([[[4.6886]]], device='cuda:0')\n",
      "sigma2 grad tensor([[-40.8244]], device='cuda:0')\n",
      "sigma0 grad tensor([-46.5818], device='cuda:0')\n",
      "sigma1 grad tensor([[-39.2806]], device='cuda:0')\n",
      "gamma grad tensor([[255.6666]], device='cuda:0')\n",
      "alpha grad tensor([[-521.4979]], device='cuda:0')\n",
      "beta2 grad tensor([[495.0697]], device='cuda:0')\n",
      "beta0 grad tensor([256.2618], device='cuda:0')\n",
      "beta1 grad tensor([[336.7603]], device='cuda:0')\n",
      "Epoch 6 | Loss: 4.8557\n",
      "alpha: 0.07373815029859543, beta0: 0.11627844721078873, beta1: 0.0007151706959120929, beta2: 0.00106397969648242, \n",
      "gamma: 0.0645647868514061, sigma0: 0.2662595808506012, sigma1: -0.00036738067865371704, sigma2: -0.0008292159764096141, sigmaX: 0.15480060875415802\n",
      "forward done\n",
      "tensor([0.0373, 0.3083, 0.4476, 0.5426, 0.4137], device='cuda:0',\n",
      "       dtype=torch.float64, grad_fn=<CatBackward0>)\n",
      "sigmaX grad tensor([[[1.7873]]], device='cuda:0')\n",
      "sigma2 grad tensor([[-47.8723]], device='cuda:0')\n",
      "sigma0 grad tensor([-48.1983], device='cuda:0')\n",
      "sigma1 grad tensor([[-41.5329]], device='cuda:0')\n",
      "gamma grad tensor([[265.6703]], device='cuda:0')\n",
      "alpha grad tensor([[-539.7643]], device='cuda:0')\n",
      "beta2 grad tensor([[513.1470]], device='cuda:0')\n",
      "beta0 grad tensor([263.7951], device='cuda:0')\n",
      "beta1 grad tensor([[345.7890]], device='cuda:0')\n",
      "Epoch 7 | Loss: 4.5238\n",
      "alpha: 0.07393308728933334, beta0: 0.11606858670711517, beta1: 0.000634988653473556, beta2: 0.001091006095521152, \n",
      "gamma: 0.06439276784658432, sigma0: 0.2665848731994629, sigma1: -0.00021487372578121722, sigma2: -0.0008096044766716659, sigmaX: 0.15423089265823364\n",
      "forward done\n",
      "tensor([0.0331, 0.2783, 0.3794, 0.5327, 0.3784], device='cuda:0',\n",
      "       dtype=torch.float64, grad_fn=<CatBackward0>)\n",
      "sigmaX grad tensor([[[-0.9470]]], device='cuda:0')\n",
      "sigma2 grad tensor([[-48.8548]], device='cuda:0')\n",
      "sigma0 grad tensor([-50.2063], device='cuda:0')\n",
      "sigma1 grad tensor([[-43.4249]], device='cuda:0')\n",
      "gamma grad tensor([[263.6053]], device='cuda:0')\n",
      "alpha grad tensor([[-543.7610]], device='cuda:0')\n",
      "beta2 grad tensor([[542.7377]], device='cuda:0')\n",
      "beta0 grad tensor([268.8337], device='cuda:0')\n",
      "beta1 grad tensor([[359.4949]], device='cuda:0')\n",
      "Epoch 8 | Loss: 4.1071\n",
      "alpha: 0.07419297099113464, beta0: 0.11579316854476929, beta1: 0.0004959598300047219, beta2: 0.0010668683098629117, \n",
      "gamma: 0.06415744125843048, sigma0: 0.26697874069213867, sigma1: -5.11328664742905e-07, sigma2: -0.0007365678320638835, sigmaX: 0.15380926430225372\n",
      "forward done\n",
      "tensor([0.0266, 0.2951, 0.4148, 0.5067, 0.3541], device='cuda:0',\n",
      "       dtype=torch.float64, grad_fn=<CatBackward0>)\n",
      "sigmaX grad tensor([[[0.7691]]], device='cuda:0')\n",
      "sigma2 grad tensor([[-49.4980]], device='cuda:0')\n",
      "sigma0 grad tensor([-48.7376], device='cuda:0')\n",
      "sigma1 grad tensor([[-42.7935]], device='cuda:0')\n",
      "gamma grad tensor([[263.1134]], device='cuda:0')\n",
      "alpha grad tensor([[-543.2915]], device='cuda:0')\n",
      "beta2 grad tensor([[535.9163]], device='cuda:0')\n",
      "beta0 grad tensor([268.1695], device='cuda:0')\n",
      "beta1 grad tensor([[356.7459]], device='cuda:0')\n",
      "Epoch 9 | Loss: 4.2529\n",
      "alpha: 0.07450859248638153, beta0: 0.11546173691749573, beta1: 0.0003062512550968677, beta2: 0.0009986321674659848, \n",
      "gamma: 0.06386766582727432, sigma0: 0.26742780208587646, sigma1: 0.00026627787156030536, sigma2: -0.0006161839119158685, sigmaX: 0.15337401628494263\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward done\n",
      "tensor([0.0170, 0.1048, 0.5544, 0.4901, 0.3332], device='cuda:0',\n",
      "       dtype=torch.float64, grad_fn=<CatBackward0>)\n",
      "sigmaX grad tensor([[[0.9699]]], device='cuda:0')\n",
      "sigma2 grad tensor([[-73.7192]], device='cuda:0')\n",
      "sigma0 grad tensor([-60.0082], device='cuda:0')\n",
      "sigma1 grad tensor([[-57.8827]], device='cuda:0')\n",
      "gamma grad tensor([[383.8922]], device='cuda:0')\n",
      "alpha grad tensor([[-757.1809]], device='cuda:0')\n",
      "beta2 grad tensor([[862.1444]], device='cuda:0')\n",
      "beta0 grad tensor([361.3021], device='cuda:0')\n",
      "beta1 grad tensor([[522.0949]], device='cuda:0')\n",
      "Epoch 10 | Loss: 2.4426\n",
      "alpha: 0.07490062713623047, beta0: 0.1150565966963768, beta1: 4.415980583871715e-05, beta2: 0.0008641349268145859, \n",
      "gamma: 0.06349893659353256, sigma0: 0.2679418921470642, sigma1: 0.000602503539994359, sigma2: -0.00042852171463891864, sigmaX: 0.15291202068328857\n",
      "forward done\n",
      "tensor([0.0111, 0.0459, 0.3986, 0.5393, 0.3953], device='cuda:0',\n",
      "       dtype=torch.float64, grad_fn=<CatBackward0>)\n",
      "sigmaX grad tensor([[[-0.0629]]], device='cuda:0')\n",
      "sigma2 grad tensor([[-32.4729]], device='cuda:0')\n",
      "sigma0 grad tensor([-30.0845], device='cuda:0')\n",
      "sigma1 grad tensor([[-27.6972]], device='cuda:0')\n",
      "gamma grad tensor([[150.9270]], device='cuda:0')\n",
      "alpha grad tensor([[-313.3806]], device='cuda:0')\n",
      "beta2 grad tensor([[352.4590]], device='cuda:0')\n",
      "beta0 grad tensor([158.2262], device='cuda:0')\n",
      "beta1 grad tensor([[223.2689]], device='cuda:0')\n",
      "Epoch 11 | Loss: 1.8034\n",
      "alpha: 0.07529976963996887, beta0: 0.11464346200227737, beta1: -0.00023128505563363433, beta2: 0.0007132490281946957, \n",
      "gamma: 0.06312323361635208, sigma0: 0.26846256852149963, sigma1: 0.0009520584717392921, sigma2: -0.00022456356964539737, sigmaX: 0.15250162780284882\n",
      "forward done\n",
      "tensor([0.0128, 0.0771, 0.6105, 0.4985, 0.3232], device='cuda:0',\n",
      "       dtype=torch.float64, grad_fn=<CatBackward0>)\n",
      "sigmaX grad tensor([[[-0.4405]]], device='cuda:0')\n",
      "sigma2 grad tensor([[89.5257]], device='cuda:0')\n",
      "sigma0 grad tensor([70.2288], device='cuda:0')\n",
      "sigma1 grad tensor([[68.4782]], device='cuda:0')\n",
      "gamma grad tensor([[-537.8233]], device='cuda:0')\n",
      "alpha grad tensor([[1026.3918]], device='cuda:0')\n",
      "beta2 grad tensor([[-1258.8281]], device='cuda:0')\n",
      "beta0 grad tensor([-478.3229], device='cuda:0')\n",
      "beta1 grad tensor([[-727.4934]], device='cuda:0')\n",
      "Epoch 12 | Loss: 2.2165\n",
      "alpha: 0.07549058645963669, beta0: 0.11443552374839783, beta1: -0.00034284559660591185, beta2: 0.0006859180866740644, \n",
      "gamma: 0.06295676529407501, sigma0: 0.26877233386039734, sigma1: 0.001140496926382184, sigma2: -0.00014098011888563633, sigmaX: 0.15216749906539917\n",
      "forward done\n",
      "tensor([0.0128, 0.1432, 0.5562, 0.5311, 0.3412], device='cuda:0',\n",
      "       dtype=torch.float64, grad_fn=<CatBackward0>)\n",
      "sigmaX grad tensor([[[-1.0136]]], device='cuda:0')\n",
      "sigma2 grad tensor([[95.3552]], device='cuda:0')\n",
      "sigma0 grad tensor([78.5859], device='cuda:0')\n",
      "sigma1 grad tensor([[77.9592]], device='cuda:0')\n",
      "gamma grad tensor([[-664.8790]], device='cuda:0')\n",
      "alpha grad tensor([[1241.5244]], device='cuda:0')\n",
      "beta2 grad tensor([[-1535.4993]], device='cuda:0')\n",
      "beta0 grad tensor([-568.2303], device='cuda:0')\n",
      "beta1 grad tensor([[-880.6563]], device='cuda:0')\n",
      "Epoch 13 | Loss: 2.8735\n",
      "alpha: 0.07548043131828308, beta0: 0.1144234910607338, beta1: -0.0002908834139816463, beta2: 0.0007867524400353432, \n",
      "gamma: 0.0629950612783432, sigma0: 0.2688894271850586, sigma1: 0.0011754578445106745, sigma2: -0.00016809413500595838, sigmaX: 0.1519491970539093\n",
      "forward done\n",
      "tensor([0.0141, 0.1903, 0.6954, 0.4515, 0.2388], device='cuda:0',\n",
      "       dtype=torch.float64, grad_fn=<CatBackward0>)\n",
      "sigmaX grad tensor([[[-0.0491]]], device='cuda:0')\n",
      "sigma2 grad tensor([[104.0695]], device='cuda:0')\n",
      "sigma0 grad tensor([79.7079], device='cuda:0')\n",
      "sigma1 grad tensor([[80.6008]], device='cuda:0')\n",
      "gamma grad tensor([[-667.2777]], device='cuda:0')\n",
      "alpha grad tensor([[1251.5199]], device='cuda:0')\n",
      "beta2 grad tensor([[-1588.0009]], device='cuda:0')\n",
      "beta0 grad tensor([-574.7189], device='cuda:0')\n",
      "beta1 grad tensor([[-897.7922]], device='cuda:0')\n",
      "Epoch 14 | Loss: 3.3024\n",
      "alpha: 0.07530821859836578, beta0: 0.1145714819431305, beta1: -0.00010124425170943141, beta2: 0.0010002063354477286, \n",
      "gamma: 0.06319615244865417, sigma0: 0.26884543895721436, sigma1: 0.001076585496775806, sigma2: -0.000300043320748955, sigmaX: 0.15175551176071167\n",
      "forward done\n",
      "tensor([0.0133, 0.0833, 0.4544, 0.4671, 0.3320], device='cuda:0',\n",
      "       dtype=torch.float64, grad_fn=<CatBackward0>)\n",
      "sigmaX grad tensor([[[-0.2526]]], device='cuda:0')\n",
      "sigma2 grad tensor([[-61.8259]], device='cuda:0')\n",
      "sigma0 grad tensor([-61.4363], device='cuda:0')\n",
      "sigma1 grad tensor([[-54.7998]], device='cuda:0')\n",
      "gamma grad tensor([[444.0096]], device='cuda:0')\n",
      "alpha grad tensor([[-857.4227]], device='cuda:0')\n",
      "beta2 grad tensor([[917.3318]], device='cuda:0')\n",
      "beta0 grad tensor([401.6996], device='cuda:0')\n",
      "beta1 grad tensor([[573.0155]], device='cuda:0')\n",
      "Epoch 15 | Loss: 2.1000\n",
      "alpha: 0.07526609301567078, beta0: 0.11459363251924515, beta1: -2.3798984329914674e-05, beta2: 0.0011194576509296894, \n",
      "gamma: 0.0632636621594429, sigma0: 0.26891636848449707, sigma1: 0.0010755672119557858, sigma2: -0.00035452735028229654, sigmaX: 0.15160015225410461\n",
      "forward done\n",
      "tensor([0.0110, 0.0495, 0.5228, 0.5179, 0.3457], device='cuda:0',\n",
      "       dtype=torch.float64, grad_fn=<CatBackward0>)\n",
      "sigmaX grad tensor([[[0.1988]]], device='cuda:0')\n",
      "sigma2 grad tensor([[11.0668]], device='cuda:0')\n",
      "sigma0 grad tensor([12.3283], device='cuda:0')\n",
      "sigma1 grad tensor([[11.2239]], device='cuda:0')\n",
      "gamma grad tensor([[-183.9185]], device='cuda:0')\n",
      "alpha grad tensor([[326.9282]], device='cuda:0')\n",
      "beta2 grad tensor([[-340.2662]], device='cuda:0')\n",
      "beta0 grad tensor([-136.7746], device='cuda:0')\n",
      "beta1 grad tensor([[-203.8563]], device='cuda:0')\n",
      "Epoch 16 | Loss: 1.8927\n",
      "alpha: 0.07518590241670609, beta0: 0.11465084552764893, beta1: 7.901591015979648e-05, beta2: 0.0012544054770842195, \n",
      "gamma: 0.06337040662765503, sigma0: 0.2689584791660309, sigma1: 0.0010566611308604479, sigma2: -0.0004155042115598917, sigmaX: 0.15144339203834534\n",
      "forward done\n",
      "tensor([0.0176, 0.0485, 0.5179, 0.5146, 0.3478], device='cuda:0',\n",
      "       dtype=torch.float64, grad_fn=<CatBackward0>)\n",
      "sigmaX grad tensor([[[1.0088]]], device='cuda:0')\n",
      "sigma2 grad tensor([[-33.9110]], device='cuda:0')\n",
      "sigma0 grad tensor([-28.8429], device='cuda:0')\n",
      "sigma1 grad tensor([[-27.3395]], device='cuda:0')\n",
      "gamma grad tensor([[138.5531]], device='cuda:0')\n",
      "alpha grad tensor([[-291.6184]], device='cuda:0')\n",
      "beta2 grad tensor([[342.8291]], device='cuda:0')\n",
      "beta0 grad tensor([148.3578], device='cuda:0')\n",
      "beta1 grad tensor([[212.9698]], device='cuda:0')\n",
      "Epoch 17 | Loss: 1.8825\n",
      "alpha: 0.07515107840299606, beta0: 0.11466208100318909, beta1: 0.00013777049025520682, beta2: 0.0013494528830051422, \n",
      "gamma: 0.06343241035938263, sigma0: 0.2690485715866089, sigma1: 0.0010835847351700068, sigma2: -0.00043509475653991103, sigmaX: 0.15122263133525848\n",
      "forward done\n",
      "tensor([0.0085, 0.0463, 0.4086, 0.5259, 0.3937], device='cuda:0',\n",
      "       dtype=torch.float64, grad_fn=<CatBackward0>)\n",
      "sigmaX grad tensor([[[-0.1631]]], device='cuda:0')\n",
      "sigma2 grad tensor([[8.6504]], device='cuda:0')\n",
      "sigma0 grad tensor([9.2417], device='cuda:0')\n",
      "sigma1 grad tensor([[8.3446]], device='cuda:0')\n",
      "gamma grad tensor([[-161.6965]], device='cuda:0')\n",
      "alpha grad tensor([[282.3171]], device='cuda:0')\n",
      "beta2 grad tensor([[-282.4835]], device='cuda:0')\n",
      "beta0 grad tensor([-115.4444], device='cuda:0')\n",
      "beta1 grad tensor([[-170.8383]], device='cuda:0')\n",
      "Epoch 18 | Loss: 1.7992\n",
      "alpha: 0.0750826746225357, beta0: 0.11470407247543335, beta1: 0.00021890425705350935, beta2: 0.0014584565069526434, \n",
      "gamma: 0.06352933496236801, sigma0: 0.26911357045173645, sigma1: 0.001094480394385755, sigma2: -0.00046207656851038337, sigmaX: 0.15103517472743988\n",
      "forward done\n",
      "tensor([0.0122, 0.0643, 0.4392, 0.5464, 0.3650], device='cuda:0',\n",
      "       dtype=torch.float64, grad_fn=<CatBackward0>)\n",
      "sigmaX grad tensor([[[1.0111]]], device='cuda:0')\n",
      "sigma2 grad tensor([[-61.0664]], device='cuda:0')\n",
      "sigma0 grad tensor([-54.8077], device='cuda:0')\n",
      "sigma1 grad tensor([[-50.7118]], device='cuda:0')\n",
      "gamma grad tensor([[381.4565]], device='cuda:0')\n",
      "alpha grad tensor([[-739.4495]], device='cuda:0')\n",
      "beta2 grad tensor([[802.5164]], device='cuda:0')\n",
      "beta0 grad tensor([347.3874], device='cuda:0')\n",
      "beta1 grad tensor([[496.1094]], device='cuda:0')\n",
      "Epoch 19 | Loss: 2.0061\n",
      "alpha: 0.07511734217405319, beta0: 0.11464659124612808, beta1: 0.0002111631038133055, beta2: 0.0014921189285814762, \n",
      "gamma: 0.06352048367261887, sigma0: 0.26927050948143005, sigma1: 0.001186451525427401, sigma2: -0.00042146918713115156, sigmaX: 0.150785893201828\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward done\n",
      "tensor([0.0159, 0.1637, 0.5103, 0.5236, 0.3549], device='cuda:0',\n",
      "       dtype=torch.float64, grad_fn=<CatBackward0>)\n",
      "sigmaX grad tensor([[[0.2187]]], device='cuda:0')\n",
      "sigma2 grad tensor([[-60.1144]], device='cuda:0')\n",
      "sigma0 grad tensor([-53.9573], device='cuda:0')\n",
      "sigma1 grad tensor([[-50.1960]], device='cuda:0')\n",
      "gamma grad tensor([[354.7244]], device='cuda:0')\n",
      "alpha grad tensor([[-701.6175]], device='cuda:0')\n",
      "beta2 grad tensor([[760.8355]], device='cuda:0')\n",
      "beta0 grad tensor([333.9830], device='cuda:0')\n",
      "beta1 grad tensor([[473.6198]], device='cuda:0')\n",
      "Epoch 20 | Loss: 3.0419\n",
      "alpha: 0.07523879408836365, beta0: 0.11450426280498505, beta1: 0.00012730581511277705, beta2: 0.0014610043726861477, \n",
      "gamma: 0.06342418491840363, sigma0: 0.2695074677467346, sigma1: 0.0013502816436812282, sigma2: -0.00032069094595499337, sigmaX: 0.1505415141582489\n",
      "forward done\n",
      "tensor([0.0097, 0.0345, 0.4786, 0.5317, 0.3620], device='cuda:0',\n",
      "       dtype=torch.float64, grad_fn=<CatBackward0>)\n",
      "sigmaX grad tensor([[[-1.5420]]], device='cuda:0')\n",
      "sigma2 grad tensor([[40.4705]], device='cuda:0')\n",
      "sigma0 grad tensor([36.3857], device='cuda:0')\n",
      "sigma1 grad tensor([[34.3114]], device='cuda:0')\n",
      "gamma grad tensor([[-313.6754]], device='cuda:0')\n",
      "alpha grad tensor([[591.1296]], device='cuda:0')\n",
      "beta2 grad tensor([[-679.3661]], device='cuda:0')\n",
      "beta0 grad tensor([-267.7152], device='cuda:0')\n",
      "beta1 grad tensor([[-402.8831]], device='cuda:0')\n",
      "Epoch 21 | Loss: 1.7271\n",
      "alpha: 0.07527194172143936, beta0: 0.1144489273428917, beta1: 0.00011721145710907876, beta2: 0.001488085021264851, \n",
      "gamma: 0.06341565400362015, sigma0: 0.2696559727191925, sigma1: 0.0014427215792238712, sigma2: -0.00027278895140625536, sigmaX: 0.1504480391740799\n",
      "forward done\n",
      "tensor([0.0105, 0.0932, 0.4460, 0.5208, 0.3627], device='cuda:0',\n",
      "       dtype=torch.float64, grad_fn=<CatBackward0>)\n",
      "sigmaX grad tensor([[[-1.4318]]], device='cuda:0')\n",
      "sigma2 grad tensor([[-64.7909]], device='cuda:0')\n",
      "sigma0 grad tensor([-56.7403], device='cuda:0')\n",
      "sigma1 grad tensor([[-53.9014]], device='cuda:0')\n",
      "gamma grad tensor([[398.2959]], device='cuda:0')\n",
      "alpha grad tensor([[-775.7740]], device='cuda:0')\n",
      "beta2 grad tensor([[854.8404]], device='cuda:0')\n",
      "beta0 grad tensor([365.1773], device='cuda:0')\n",
      "beta1 grad tensor([[525.7905]], device='cuda:0')\n",
      "Epoch 22 | Loss: 2.2725\n",
      "alpha: 0.07540090382099152, beta0: 0.11430063098669052, beta1: 2.2669677491649054e-05, beta2: 0.0014428287977352738, \n",
      "gamma: 0.06330946832895279, sigma0: 0.2698899507522583, sigma1: 0.0016131621086969972, sigma2: -0.00015973203699104488, sigmaX: 0.15047742426395416\n",
      "forward done\n",
      "tensor([0.0143, 0.0272, 0.5059, 0.4967, 0.3321], device='cuda:0',\n",
      "       dtype=torch.float64, grad_fn=<CatBackward0>)\n",
      "sigmaX grad tensor([[[-1.4247]]], device='cuda:0')\n",
      "sigma2 grad tensor([[0.3901]], device='cuda:0')\n",
      "sigma0 grad tensor([0.7024], device='cuda:0')\n",
      "sigma1 grad tensor([[0.3659]], device='cuda:0')\n",
      "gamma grad tensor([[-109.9294]], device='cuda:0')\n",
      "alpha grad tensor([[181.0777]], device='cuda:0')\n",
      "beta2 grad tensor([[-165.7989]], device='cuda:0')\n",
      "beta0 grad tensor([-66.4744], device='cuda:0')\n",
      "beta1 grad tensor([[-98.8556]], device='cuda:0')\n",
      "Epoch 23 | Loss: 1.6213\n",
      "alpha: 0.07549479603767395, beta0: 0.11418384313583374, beta1: -4.713764792541042e-05, beta2: 0.0014153002994135022, \n",
      "gamma: 0.06324038654565811, sigma0: 0.27010175585746765, sigma1: 0.0017677868017926812, sigma2: -5.7193006796296686e-05, sigmaX: 0.15061436593532562\n",
      "forward done\n",
      "tensor([0.0101, 0.0383, 0.6127, 0.5207, 0.3680], device='cuda:0',\n",
      "       dtype=torch.float64, grad_fn=<CatBackward0>)\n",
      "sigmaX grad tensor([[[0.3318]]], device='cuda:0')\n",
      "sigma2 grad tensor([[15.8132]], device='cuda:0')\n",
      "sigma0 grad tensor([14.1424], device='cuda:0')\n",
      "sigma1 grad tensor([[13.4523]], device='cuda:0')\n",
      "gamma grad tensor([[-178.5625]], device='cuda:0')\n",
      "alpha grad tensor([[323.1548]], device='cuda:0')\n",
      "beta2 grad tensor([[-341.7843]], device='cuda:0')\n",
      "beta0 grad tensor([-137.6282], device='cuda:0')\n",
      "beta1 grad tensor([[-204.8634]], device='cuda:0')\n",
      "Epoch 24 | Loss: 1.8943\n",
      "alpha: 0.07553785294294357, beta0: 0.11411575973033905, beta1: -7.657293463125825e-05, beta2: 0.0014187489869073033, \n",
      "gamma: 0.06322271376848221, sigma0: 0.27026864886283875, sigma1: 0.0018861760618165135, sigma2: 1.871339372883085e-05, sigmaX: 0.15071287751197815\n",
      "forward done\n",
      "tensor([0.0137, 0.0734, 0.4468, 0.5494, 0.3685], device='cuda:0',\n",
      "       dtype=torch.float64, grad_fn=<CatBackward0>)\n",
      "sigmaX grad tensor([[[-0.1832]]], device='cuda:0')\n",
      "sigma2 grad tensor([[-63.5971]], device='cuda:0')\n",
      "sigma0 grad tensor([-58.7743], device='cuda:0')\n",
      "sigma1 grad tensor([[-53.8507]], device='cuda:0')\n",
      "gamma grad tensor([[440.9617]], device='cuda:0')\n",
      "alpha grad tensor([[-849.1116]], device='cuda:0')\n",
      "beta2 grad tensor([[923.7532]], device='cuda:0')\n",
      "beta0 grad tensor([395.8937], device='cuda:0')\n",
      "beta1 grad tensor([[569.0766]], device='cuda:0')\n",
      "Epoch 25 | Loss: 2.1122\n",
      "alpha: 0.07568617165088654, beta0: 0.11394651234149933, beta1: -0.00019718937983270735, beta2: 0.0013447136152535677, \n",
      "gamma: 0.06309673935174942, sigma0: 0.2705242335796356, sigma1: 0.0020818200428038836, sigma2: 0.0001576980866957456, sigmaX: 0.1508171558380127\n",
      "forward done\n",
      "tensor([0.0117, 0.0651, 0.5597, 0.5201, 0.3483], device='cuda:0',\n",
      "       dtype=torch.float64, grad_fn=<CatBackward0>)\n",
      "sigmaX grad tensor([[[-0.6101]]], device='cuda:0')\n",
      "sigma2 grad tensor([[75.0600]], device='cuda:0')\n",
      "sigma0 grad tensor([65.6778], device='cuda:0')\n",
      "sigma1 grad tensor([[63.2197]], device='cuda:0')\n",
      "gamma grad tensor([[-490.6834]], device='cuda:0')\n",
      "alpha grad tensor([[951.6588]], device='cuda:0')\n",
      "beta2 grad tensor([[-1112.9513]], device='cuda:0')\n",
      "beta0 grad tensor([-445.4627], device='cuda:0')\n",
      "beta1 grad tensor([[-664.8927]], device='cuda:0')\n",
      "Epoch 26 | Loss: 2.0905\n",
      "alpha: 0.07569634169340134, beta0: 0.11391662806272507, beta1: -0.00019568487186916173, beta2: 0.0013709673658013344, \n",
      "gamma: 0.06310588121414185, sigma0: 0.2706339657306671, sigma1: 0.0021526676137000322, sigma2: 0.00019969647109974176, sigmaX: 0.15096071362495422\n",
      "forward done\n",
      "tensor([0.0142, 0.0704, 0.6212, 0.5011, 0.3124], device='cuda:0',\n",
      "       dtype=torch.float64, grad_fn=<CatBackward0>)\n",
      "sigmaX grad tensor([[[-1.3949]]], device='cuda:0')\n",
      "sigma2 grad tensor([[72.7489]], device='cuda:0')\n",
      "sigma0 grad tensor([64.6356], device='cuda:0')\n",
      "sigma1 grad tensor([[61.1824]], device='cuda:0')\n",
      "gamma grad tensor([[-517.1361]], device='cuda:0')\n",
      "alpha grad tensor([[984.9954]], device='cuda:0')\n",
      "beta2 grad tensor([[-1155.6975]], device='cuda:0')\n",
      "beta0 grad tensor([-455.7393], device='cuda:0')\n",
      "beta1 grad tensor([[-683.4650]], device='cuda:0')\n",
      "Epoch 27 | Loss: 2.1524\n",
      "alpha: 0.07558120042085648, beta0: 0.11401186138391495, beta1: -8.302400237880647e-05, beta2: 0.0014905398711562157, \n",
      "gamma: 0.06323981285095215, sigma0: 0.2706170678138733, sigma1: 0.002115795621648431, sigma2: 0.0001568260631756857, sigmaX: 0.1511998176574707\n",
      "forward done\n",
      "tensor([0.0118, 0.1395, 0.5273, 0.5499, 0.3718], device='cuda:0',\n",
      "       dtype=torch.float64, grad_fn=<CatBackward0>)\n",
      "sigmaX grad tensor([[[0.7326]]], device='cuda:0')\n",
      "sigma2 grad tensor([[71.3420]], device='cuda:0')\n",
      "sigma0 grad tensor([67.3663], device='cuda:0')\n",
      "sigma1 grad tensor([[62.0267]], device='cuda:0')\n",
      "gamma grad tensor([[-588.2329]], device='cuda:0')\n",
      "alpha grad tensor([[1097.1580]], device='cuda:0')\n",
      "beta2 grad tensor([[-1245.9955]], device='cuda:0')\n",
      "beta0 grad tensor([-499.3125], device='cuda:0')\n",
      "beta1 grad tensor([[-743.5330]], device='cuda:0')\n",
      "Epoch 28 | Loss: 2.8559\n",
      "alpha: 0.075343556702137, beta0: 0.11422755569219589, beta1: 0.00013703371223527938, beta2: 0.0017006491543725133, \n",
      "gamma: 0.06349825114011765, sigma0: 0.27048370242118835, sigma1: 0.00198144419118762, sigma2: 3.881667726091109e-05, sigmaX: 0.15135781466960907\n",
      "forward done\n",
      "tensor([0.0078, 0.0315, 0.5689, 0.4930, 0.3384], device='cuda:0',\n",
      "       dtype=torch.float64, grad_fn=<CatBackward0>)\n",
      "sigmaX grad tensor([[[0.0779]]], device='cuda:0')\n",
      "sigma2 grad tensor([[-27.0744]], device='cuda:0')\n",
      "sigma0 grad tensor([-21.6482], device='cuda:0')\n",
      "sigma1 grad tensor([[-21.3359]], device='cuda:0')\n",
      "gamma grad tensor([[71.1904]], device='cuda:0')\n",
      "alpha grad tensor([[-165.8368]], device='cuda:0')\n",
      "beta2 grad tensor([[215.8089]], device='cuda:0')\n",
      "beta0 grad tensor([92.1764], device='cuda:0')\n",
      "beta1 grad tensor([[133.1096]], device='cuda:0')\n",
      "Epoch 29 | Loss: 1.7227\n",
      "alpha: 0.07514795660972595, beta0: 0.11439937353134155, beta1: 0.0003156933526042849, beta2: 0.0018739320803433657, \n",
      "gamma: 0.06371642649173737, sigma0: 0.27040085196495056, sigma1: 0.0018944808980450034, sigma2: -3.8110429159132764e-05, sigmaX: 0.15149544179439545\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward done\n",
      "tensor([0.0245, 0.2306, 0.5780, 0.5424, 0.3780], device='cuda:0',\n",
      "       dtype=torch.float64, grad_fn=<CatBackward0>)\n",
      "sigmaX grad tensor([[[-1.5553]]], device='cuda:0')\n",
      "sigma2 grad tensor([[-37.4139]], device='cuda:0')\n",
      "sigma0 grad tensor([-47.1308], device='cuda:0')\n",
      "sigma1 grad tensor([[-37.4502]], device='cuda:0')\n",
      "gamma grad tensor([[358.5782]], device='cuda:0')\n",
      "alpha grad tensor([[-686.2162]], device='cuda:0')\n",
      "beta2 grad tensor([[656.1478]], device='cuda:0')\n",
      "beta0 grad tensor([318.2199], device='cuda:0')\n",
      "beta1 grad tensor([[429.7277]], device='cuda:0')\n",
      "Epoch 30 | Loss: 3.8288\n",
      "alpha: 0.07505779713392258, beta0: 0.11446943879127502, beta1: 0.0004068848502356559, beta2: 0.001975764986127615, \n",
      "gamma: 0.06382633745670319, sigma0: 0.2704099118709564, sigma1: 0.001877912669442594, sigma2: -6.554186984430999e-05, sigmaX: 0.15174244344234467\n",
      "forward done\n",
      "tensor([0.0104, 0.0281, 0.5187, 0.4614, 0.3255], device='cuda:0',\n",
      "       dtype=torch.float64, grad_fn=<CatBackward0>)\n",
      "sigmaX grad tensor([[[0.6453]]], device='cuda:0')\n",
      "sigma2 grad tensor([[-24.2189]], device='cuda:0')\n",
      "sigma0 grad tensor([-19.9970], device='cuda:0')\n",
      "sigma1 grad tensor([[-19.3117]], device='cuda:0')\n",
      "gamma grad tensor([[48.1891]], device='cuda:0')\n",
      "alpha grad tensor([[-122.4678]], device='cuda:0')\n",
      "beta2 grad tensor([[163.5212]], device='cuda:0')\n",
      "beta0 grad tensor([73.6224], device='cuda:0')\n",
      "beta1 grad tensor([[103.8547]], device='cuda:0')\n",
      "Epoch 31 | Loss: 1.5969\n",
      "alpha: 0.07499122619628906, beta0: 0.11451338976621628, beta1: 0.0004727216437458992, beta2: 0.002054561860859394, \n",
      "gamma: 0.06391467154026031, sigma0: 0.2704540491104126, sigma1: 0.001895256689749658, sigma2: -6.271175516303629e-05, sigmaX: 0.15191444754600525\n",
      "forward done\n",
      "tensor([0.0118, 0.1063, 0.4607, 0.5485, 0.3666], device='cuda:0',\n",
      "       dtype=torch.float64, grad_fn=<CatBackward0>)\n",
      "sigmaX grad tensor([[[-0.6762]]], device='cuda:0')\n",
      "sigma2 grad tensor([[-52.5374]], device='cuda:0')\n",
      "sigma0 grad tensor([-54.2191], device='cuda:0')\n",
      "sigma1 grad tensor([[-48.2092]], device='cuda:0')\n",
      "gamma grad tensor([[397.7491]], device='cuda:0')\n",
      "alpha grad tensor([[-765.1869]], device='cuda:0')\n",
      "beta2 grad tensor([[755.9995]], device='cuda:0')\n",
      "beta0 grad tensor([355.6072], device='cuda:0')\n",
      "beta1 grad tensor([[490.7057]], device='cuda:0')\n",
      "Epoch 32 | Loss: 2.4504\n",
      "alpha: 0.07502833753824234, beta0: 0.11445719748735428, beta1: 0.00045063503785058856, beta2: 0.0020610319916158915, \n",
      "gamma: 0.06389699131250381, sigma0: 0.2705908715724945, sigma1: 0.0019920244812965393, sigma2: 5.485258611770405e-07, sigmaX: 0.15212547779083252\n",
      "forward done\n",
      "tensor([0.0112, 0.1878, 0.3896, 0.5545, 0.3900], device='cuda:0',\n",
      "       dtype=torch.float64, grad_fn=<CatBackward0>)\n",
      "sigmaX grad tensor([[[0.2177]]], device='cuda:0')\n",
      "sigma2 grad tensor([[-43.0575]], device='cuda:0')\n",
      "sigma0 grad tensor([-50.1078], device='cuda:0')\n",
      "sigma1 grad tensor([[-42.1510]], device='cuda:0')\n",
      "gamma grad tensor([[328.9531]], device='cuda:0')\n",
      "alpha grad tensor([[-650.8264]], device='cuda:0')\n",
      "beta2 grad tensor([[639.6837]], device='cuda:0')\n",
      "beta0 grad tensor([309.5704], device='cuda:0')\n",
      "beta1 grad tensor([[421.7437]], device='cuda:0')\n",
      "Epoch 33 | Loss: 3.2232\n",
      "alpha: 0.07514443248510361, beta0: 0.11432304233312607, beta1: 0.00036028065369464457, beta2: 0.002011587843298912, \n",
      "gamma: 0.06380055099725723, sigma0: 0.27080389857292175, sigma1: 0.0021507961209863424, sigma2: 0.00010812926484504715, sigmaX: 0.1522996872663498\n",
      "forward done\n",
      "tensor([0.0108, 0.1667, 0.3713, 0.5839, 0.4054], device='cuda:0',\n",
      "       dtype=torch.float64, grad_fn=<CatBackward0>)\n",
      "sigmaX grad tensor([[[-0.2109]]], device='cuda:0')\n",
      "sigma2 grad tensor([[-42.9843]], device='cuda:0')\n",
      "sigma0 grad tensor([-50.1060], device='cuda:0')\n",
      "sigma1 grad tensor([[-42.1812]], device='cuda:0')\n",
      "gamma grad tensor([[376.0250]], device='cuda:0')\n",
      "alpha grad tensor([[-722.7658]], device='cuda:0')\n",
      "beta2 grad tensor([[698.5002]], device='cuda:0')\n",
      "beta0 grad tensor([335.8388], device='cuda:0')\n",
      "beta1 grad tensor([[458.8726]], device='cuda:0')\n",
      "Epoch 34 | Loss: 3.0386\n",
      "alpha: 0.07534069567918777, beta0: 0.11411164700984955, beta1: 0.00020180855062790215, beta2: 0.0019059792393818498, \n",
      "gamma: 0.06362174451351166, sigma0: 0.27108561992645264, sigma1: 0.002366025233641267, sigma2: 0.0002562638255767524, sigmaX: 0.15247584879398346\n",
      "forward done\n",
      "tensor([0.0098, 0.0534, 0.4724, 0.4941, 0.3557], device='cuda:0',\n",
      "       dtype=torch.float64, grad_fn=<CatBackward0>)\n",
      "sigmaX grad tensor([[[-0.6415]]], device='cuda:0')\n",
      "sigma2 grad tensor([[-60.4863]], device='cuda:0')\n",
      "sigma0 grad tensor([-56.6177], device='cuda:0')\n",
      "sigma1 grad tensor([[-52.0735]], device='cuda:0')\n",
      "gamma grad tensor([[438.2266]], device='cuda:0')\n",
      "alpha grad tensor([[-837.4298]], device='cuda:0')\n",
      "beta2 grad tensor([[872.0239]], device='cuda:0')\n",
      "beta0 grad tensor([387.2686], device='cuda:0')\n",
      "beta1 grad tensor([[548.0128]], device='cuda:0')\n",
      "Epoch 35 | Loss: 1.8662\n",
      "alpha: 0.07562263309955597, beta0: 0.11381782591342926, beta1: -3.2661602745065466e-05, beta2: 0.0017343558138236403, \n",
      "gamma: 0.06335463374853134, sigma0: 0.27143990993499756, sigma1: 0.0026484928093850613, sigma2: 0.0004616380902007222, sigmaX: 0.15268948674201965\n",
      "forward done\n",
      "tensor([0.0106, 0.0479, 0.5468, 0.5027, 0.3236], device='cuda:0',\n",
      "       dtype=torch.float64, grad_fn=<CatBackward0>)\n",
      "sigmaX grad tensor([[[0.8748]]], device='cuda:0')\n",
      "sigma2 grad tensor([[61.2692]], device='cuda:0')\n",
      "sigma0 grad tensor([60.3863], device='cuda:0')\n",
      "sigma1 grad tensor([[54.7203]], device='cuda:0')\n",
      "gamma grad tensor([[-508.0095]], device='cuda:0')\n",
      "alpha grad tensor([[958.7072]], device='cuda:0')\n",
      "beta2 grad tensor([[-1066.7737]], device='cuda:0')\n",
      "beta0 grad tensor([-438.8325], device='cuda:0')\n",
      "beta1 grad tensor([[-646.2463]], device='cuda:0')\n",
      "Epoch 36 | Loss: 1.8631\n",
      "alpha: 0.0757540613412857, beta0: 0.1136724054813385, beta1: -0.00013501443027053028, beta2: 0.0016731192590668797, \n",
      "gamma: 0.06323905289173126, sigma0: 0.27164986729621887, sigma1: 0.002809671452268958, sigma2: 0.0005746660172007978, sigmaX: 0.15280884504318237\n",
      "forward done\n",
      "tensor([0.0126, 0.0603, 0.5370, 0.5157, 0.3478], device='cuda:0',\n",
      "       dtype=torch.float64, grad_fn=<CatBackward0>)\n",
      "sigmaX grad tensor([[[0.1079]]], device='cuda:0')\n",
      "sigma2 grad tensor([[41.1958]], device='cuda:0')\n",
      "sigma0 grad tensor([41.0814], device='cuda:0')\n",
      "sigma1 grad tensor([[36.7880]], device='cuda:0')\n",
      "gamma grad tensor([[-396.1879]], device='cuda:0')\n",
      "alpha grad tensor([[735.4338]], device='cuda:0')\n",
      "beta2 grad tensor([[-818.8179]], device='cuda:0')\n",
      "beta0 grad tensor([-329.0213], device='cuda:0')\n",
      "beta1 grad tensor([[-488.2564]], device='cuda:0')\n",
      "Epoch 37 | Loss: 2.0157\n",
      "alpha: 0.07578004896640778, beta0: 0.11362935602664948, beta1: -0.0001456870959373191, beta2: 0.0016896456945687532, \n",
      "gamma: 0.06323076784610748, sigma0: 0.27176612615585327, sigma1: 0.0028926944360136986, sigma2: 0.0006279769586399198, sigmaX: 0.1529082953929901\n",
      "forward done\n",
      "tensor([0.0124, 0.1275, 0.5113, 0.4782, 0.3205], device='cuda:0',\n",
      "       dtype=torch.float64, grad_fn=<CatBackward0>)\n",
      "sigmaX grad tensor([[[0.2245]]], device='cuda:0')\n",
      "sigma2 grad tensor([[85.3440]], device='cuda:0')\n",
      "sigma0 grad tensor([67.0909], device='cuda:0')\n",
      "sigma1 grad tensor([[66.8331]], device='cuda:0')\n",
      "gamma grad tensor([[-551.9193]], device='cuda:0')\n",
      "alpha grad tensor([[1043.4465]], device='cuda:0')\n",
      "beta2 grad tensor([[-1255.2518]], device='cuda:0')\n",
      "beta0 grad tensor([-479.5961], device='cuda:0')\n",
      "beta1 grad tensor([[-729.8614]], device='cuda:0')\n",
      "Epoch 38 | Loss: 2.5977\n",
      "alpha: 0.07567352801561356, beta0: 0.11371805518865585, beta1: -3.400306013645604e-05, beta2: 0.0018142384942620993, \n",
      "gamma: 0.0633552223443985, sigma0: 0.27175071835517883, sigma1: 0.0028532342985272408, sigma2: 0.0005739899934269488, sigmaX: 0.15297943353652954\n",
      "forward done\n",
      "tensor([0.0097, 0.0304, 0.4772, 0.5038, 0.3392], device='cuda:0',\n",
      "       dtype=torch.float64, grad_fn=<CatBackward0>)\n",
      "sigmaX grad tensor([[[-0.0909]]], device='cuda:0')\n",
      "sigma2 grad tensor([[-27.7331]], device='cuda:0')\n",
      "sigma0 grad tensor([-24.7441], device='cuda:0')\n",
      "sigma1 grad tensor([[-23.2022]], device='cuda:0')\n",
      "gamma grad tensor([[99.7053]], device='cuda:0')\n",
      "alpha grad tensor([[-218.6408]], device='cuda:0')\n",
      "beta2 grad tensor([[254.9258]], device='cuda:0')\n",
      "beta0 grad tensor([116.1948], device='cuda:0')\n",
      "beta1 grad tensor([[163.2208]], device='cuda:0')\n",
      "Epoch 39 | Loss: 1.6334\n",
      "alpha: 0.07560411840677261, beta0: 0.1137675866484642, beta1: 4.016543971374631e-05, beta2: 0.0019050356931984425, \n",
      "gamma: 0.0634443461894989, sigma0: 0.2717811167240143, sigma1: 0.0028572611045092344, sigma2: 0.0005583533202297986, sigmaX: 0.15305210649967194\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward done\n",
      "tensor([0.0100, 0.0304, 0.4837, 0.4990, 0.3663], device='cuda:0',\n",
      "       dtype=torch.float64, grad_fn=<CatBackward0>)\n",
      "sigmaX grad tensor([[[-0.2304]]], device='cuda:0')\n",
      "sigma2 grad tensor([[-10.1342]], device='cuda:0')\n",
      "sigma0 grad tensor([-5.4110], device='cuda:0')\n",
      "sigma1 grad tensor([[-6.5924]], device='cuda:0')\n",
      "gamma grad tensor([[-53.7364]], device='cuda:0')\n",
      "alpha grad tensor([[73.3772]], device='cuda:0')\n",
      "beta2 grad tensor([[-41.0929]], device='cuda:0')\n",
      "beta0 grad tensor([-17.8090], device='cuda:0')\n",
      "beta1 grad tensor([[-25.2310]], device='cuda:0')\n",
      "Epoch 40 | Loss: 1.6629\n",
      "alpha: 0.07553163915872574, beta0: 0.1138174831867218, beta1: 0.00011194160470040515, beta2: 0.001991336466744542, \n",
      "gamma: 0.0635385513305664, sigma0: 0.271818608045578, sigma1: 0.002872378332540393, sigma2: 0.0005564770544879138, sigmaX: 0.15313850343227386\n",
      "forward done\n",
      "tensor([0.0153, 0.0690, 0.5456, 0.5079, 0.3430], device='cuda:0',\n",
      "       dtype=torch.float64, grad_fn=<CatBackward0>)\n",
      "sigmaX grad tensor([[[-0.2095]]], device='cuda:0')\n",
      "sigma2 grad tensor([[-56.4419]], device='cuda:0')\n",
      "sigma0 grad tensor([-54.7196], device='cuda:0')\n",
      "sigma1 grad tensor([[-49.1313]], device='cuda:0')\n",
      "gamma grad tensor([[422.2092]], device='cuda:0')\n",
      "alpha grad tensor([[-813.1976]], device='cuda:0')\n",
      "beta2 grad tensor([[857.2609]], device='cuda:0')\n",
      "beta0 grad tensor([377.3986], device='cuda:0')\n",
      "beta1 grad tensor([[536.5489]], device='cuda:0')\n",
      "Epoch 41 | Loss: 2.1017\n",
      "alpha: 0.07556957751512527, beta0: 0.11376034468412399, beta1: 8.547553443349898e-05, beta2: 0.001992393983528018, \n",
      "gamma: 0.06352033466100693, sigma0: 0.27195149660110474, sigma1: 0.002971559762954712, sigma2: 0.0006239303620532155, sigmaX: 0.15323574841022491\n",
      "forward done\n",
      "tensor([0.0124, 0.0666, 0.4558, 0.5020, 0.3096], device='cuda:0',\n",
      "       dtype=torch.float64, grad_fn=<CatBackward0>)\n",
      "sigmaX grad tensor([[[0.0584]]], device='cuda:0')\n",
      "sigma2 grad tensor([[-57.1783]], device='cuda:0')\n",
      "sigma0 grad tensor([-53.9579], device='cuda:0')\n",
      "sigma1 grad tensor([[-48.8558]], device='cuda:0')\n",
      "gamma grad tensor([[385.2565]], device='cuda:0')\n",
      "alpha grad tensor([[-750.6436]], device='cuda:0')\n",
      "beta2 grad tensor([[823.3817]], device='cuda:0')\n",
      "beta0 grad tensor([352.3426], device='cuda:0')\n",
      "beta1 grad tensor([[506.5744]], device='cuda:0')\n",
      "Epoch 42 | Loss: 1.9460\n",
      "alpha: 0.0756988599896431, beta0: 0.11361368745565414, beta1: -2.4612700144643895e-05, beta2: 0.001919150585308671, \n",
      "gamma: 0.06341004371643066, sigma0: 0.2721687853336334, sigma1: 0.0031462882179766893, sigma2: 0.0007553485338576138, sigmaX: 0.15331895649433136\n",
      "forward done\n",
      "tensor([0.0094, 0.0293, 0.4517, 0.5354, 0.3695], device='cuda:0',\n",
      "       dtype=torch.float64, grad_fn=<CatBackward0>)\n",
      "sigmaX grad tensor([[[-1.4798]]], device='cuda:0')\n",
      "sigma2 grad tensor([[-2.3021]], device='cuda:0')\n",
      "sigma0 grad tensor([0.8588], device='cuda:0')\n",
      "sigma1 grad tensor([[-0.6329]], device='cuda:0')\n",
      "gamma grad tensor([[-100.4839]], device='cuda:0')\n",
      "alpha grad tensor([[165.9865]], device='cuda:0')\n",
      "beta2 grad tensor([[-146.0744]], device='cuda:0')\n",
      "beta0 grad tensor([-61.2675], device='cuda:0')\n",
      "beta1 grad tensor([[-89.0981]], device='cuda:0')\n",
      "Epoch 43 | Loss: 1.6587\n",
      "alpha: 0.07579515129327774, beta0: 0.1134970560669899, beta1: -0.0001094041217584163, beta2: 0.0018658406333997846, \n",
      "gamma: 0.06333452463150024, sigma0: 0.27236485481262207, sigma1: 0.003306345082819462, sigma2: 0.0008777560433372855, sigmaX: 0.15352505445480347\n",
      "forward done\n",
      "tensor([0.0103, 0.0449, 0.5480, 0.4619, 0.3141], device='cuda:0',\n",
      "       dtype=torch.float64, grad_fn=<CatBackward0>)\n",
      "sigmaX grad tensor([[[0.7880]]], device='cuda:0')\n",
      "sigma2 grad tensor([[-4.4217]], device='cuda:0')\n",
      "sigma0 grad tensor([-0.5145], device='cuda:0')\n",
      "sigma1 grad tensor([[-1.7728]], device='cuda:0')\n",
      "gamma grad tensor([[-94.4375]], device='cuda:0')\n",
      "alpha grad tensor([[152.3152]], device='cuda:0')\n",
      "beta2 grad tensor([[-133.5925]], device='cuda:0')\n",
      "beta0 grad tensor([-53.8996], device='cuda:0')\n",
      "beta1 grad tensor([[-80.1273]], device='cuda:0')\n",
      "Epoch 44 | Loss: 1.7837\n",
      "alpha: 0.07586298882961273, beta0: 0.11340587586164474, beta1: -0.00017258328443858773, beta2: 0.0018296546768397093, \n",
      "gamma: 0.06328935921192169, sigma0: 0.27254414558410645, sigma1: 0.003455076366662979, sigma2: 0.0009946436621248722, sigmaX: 0.15364086627960205\n",
      "forward done\n",
      "tensor([0.0127, 0.0809, 0.4299, 0.5231, 0.3813], device='cuda:0',\n",
      "       dtype=torch.float64, grad_fn=<CatBackward0>)\n",
      "sigmaX grad tensor([[[-0.8529]]], device='cuda:0')\n",
      "sigma2 grad tensor([[68.1799]], device='cuda:0')\n",
      "sigma0 grad tensor([61.4240], device='cuda:0')\n",
      "sigma1 grad tensor([[57.6620]], device='cuda:0')\n",
      "gamma grad tensor([[-534.8525]], device='cuda:0')\n",
      "alpha grad tensor([[1006.4824]], device='cuda:0')\n",
      "beta2 grad tensor([[-1172.3369]], device='cuda:0')\n",
      "beta0 grad tensor([-459.6583], device='cuda:0')\n",
      "beta1 grad tensor([[-692.2917]], device='cuda:0')\n",
      "Epoch 45 | Loss: 2.1556\n",
      "alpha: 0.07579444348812103, beta0: 0.1134502962231636, beta1: -0.00010950498835882172, beta2: 0.0019047672394663095, \n",
      "gamma: 0.06338084489107132, sigma0: 0.2725916802883148, sigma1: 0.003486113389953971, sigma2: 0.0010141417151317, sigmaX: 0.15382221341133118\n",
      "forward done\n",
      "tensor([0.0127, 0.1118, 0.4898, 0.5072, 0.3283], device='cuda:0',\n",
      "       dtype=torch.float64, grad_fn=<CatBackward0>)\n",
      "sigmaX grad tensor([[[0.6094]]], device='cuda:0')\n",
      "sigma2 grad tensor([[71.8302]], device='cuda:0')\n",
      "sigma0 grad tensor([65.3658], device='cuda:0')\n",
      "sigma1 grad tensor([[61.8380]], device='cuda:0')\n",
      "gamma grad tensor([[-549.1531]], device='cuda:0')\n",
      "alpha grad tensor([[1036.6332]], device='cuda:0')\n",
      "beta2 grad tensor([[-1191.6565]], device='cuda:0')\n",
      "beta0 grad tensor([-475.2778], device='cuda:0')\n",
      "beta1 grad tensor([[-711.5226]], device='cuda:0')\n",
      "Epoch 46 | Loss: 2.4561\n",
      "alpha: 0.0756014883518219, beta0: 0.11361908167600632, beta1: 6.912503158673644e-05, beta2: 0.0020812605507671833, \n",
      "gamma: 0.06359642744064331, sigma0: 0.2725144028663635, sigma1: 0.003404202638193965, sigma2: 0.0009410589700564742, sigmaX: 0.1539313793182373\n",
      "forward done\n",
      "tensor([0.0115, 0.0491, 0.4381, 0.4986, 0.3535], device='cuda:0',\n",
      "       dtype=torch.float64, grad_fn=<CatBackward0>)\n",
      "sigmaX grad tensor([[[-0.1903]]], device='cuda:0')\n",
      "sigma2 grad tensor([[-33.9838]], device='cuda:0')\n",
      "sigma0 grad tensor([-34.1669], device='cuda:0')\n",
      "sigma1 grad tensor([[-30.2902]], device='cuda:0')\n",
      "gamma grad tensor([[207.0840]], device='cuda:0')\n",
      "alpha grad tensor([[-414.6255]], device='cuda:0')\n",
      "beta2 grad tensor([[455.5914]], device='cuda:0')\n",
      "beta0 grad tensor([201.4181], device='cuda:0')\n",
      "beta1 grad tensor([[286.4517]], device='cuda:0')\n",
      "Epoch 47 | Loss: 1.7927\n",
      "alpha: 0.07547968626022339, beta0: 0.11371693760156631, beta1: 0.00018153118435293436, beta2: 0.002199367852881551, \n",
      "gamma: 0.06374096870422363, sigma0: 0.2725074291229248, sigma1: 0.0033841088879853487, sigma2: 0.0009179882472380996, sigmaX: 0.15404798090457916\n",
      "forward done\n",
      "tensor([0.0122, 0.0350, 0.5530, 0.5041, 0.3315], device='cuda:0',\n",
      "       dtype=torch.float64, grad_fn=<CatBackward0>)\n",
      "sigmaX grad tensor([[[-0.0615]]], device='cuda:0')\n",
      "sigma2 grad tensor([[-10.2853]], device='cuda:0')\n",
      "sigma0 grad tensor([-8.0553], device='cuda:0')\n",
      "sigma1 grad tensor([[-8.1249]], device='cuda:0')\n",
      "gamma grad tensor([[-8.5761]], device='cuda:0')\n",
      "alpha grad tensor([[-9.7371]], device='cuda:0')\n",
      "beta2 grad tensor([[33.0818]], device='cuda:0')\n",
      "beta0 grad tensor([17.0970], device='cuda:0')\n",
      "beta1 grad tensor([[22.6904]], device='cuda:0')\n",
      "Epoch 48 | Loss: 1.7513\n",
      "alpha: 0.07537022978067398, beta0: 0.11380115896463394, beta1: 0.0002797383931465447, beta2: 0.0023036396596580744, \n",
      "gamma: 0.06387448310852051, sigma0: 0.2725161015987396, sigma1: 0.003380512585863471, sigma2: 0.0009102356270886958, sigmaX: 0.15415965020656586\n",
      "forward done\n",
      "tensor([0.0108, 0.0913, 0.5531, 0.5247, 0.3814], device='cuda:0',\n",
      "       dtype=torch.float64, grad_fn=<CatBackward0>)\n",
      "sigmaX grad tensor([[[1.1608]]], device='cuda:0')\n",
      "sigma2 grad tensor([[-48.7048]], device='cuda:0')\n",
      "sigma0 grad tensor([-52.7798], device='cuda:0')\n",
      "sigma1 grad tensor([[-45.4150]], device='cuda:0')\n",
      "gamma grad tensor([[377.6204]], device='cuda:0')\n",
      "alpha grad tensor([[-738.2149]], device='cuda:0')\n",
      "beta2 grad tensor([[764.6554]], device='cuda:0')\n",
      "beta0 grad tensor([346.6407], device='cuda:0')\n",
      "beta1 grad tensor([[485.5011]], device='cuda:0')\n",
      "Epoch 49 | Loss: 2.3827\n",
      "alpha: 0.07536718994379044, beta0: 0.11378118395805359, beta1: 0.0002832871687132865, beta2: 0.002326316898688674, \n",
      "gamma: 0.0639009028673172, sigma0: 0.27262216806411743, sigma1: 0.003459470346570015, sigma2: 0.0009661532239988446, sigmaX: 0.15415295958518982\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward done\n",
      "tensor([0.0147, 0.0752, 0.5087, 0.5146, 0.3552], device='cuda:0',\n",
      "       dtype=torch.float64, grad_fn=<CatBackward0>)\n",
      "sigmaX grad tensor([[[-0.5943]]], device='cuda:0')\n",
      "sigma2 grad tensor([[-55.6666]], device='cuda:0')\n",
      "sigma0 grad tensor([-54.1112], device='cuda:0')\n",
      "sigma1 grad tensor([[-48.0363]], device='cuda:0')\n",
      "gamma grad tensor([[379.9392]], device='cuda:0')\n",
      "alpha grad tensor([[-742.4689]], device='cuda:0')\n",
      "beta2 grad tensor([[800.7454]], device='cuda:0')\n",
      "beta0 grad tensor([349.3748], device='cuda:0')\n",
      "beta1 grad tensor([[496.4950]], device='cuda:0')\n",
      "Epoch 50 | Loss: 2.1454\n",
      "alpha: 0.07546055316925049, beta0: 0.1136666089296341, beta1: 0.00019933721341658384, beta2: 0.0022715351078659296, \n",
      "gamma: 0.06383022665977478, sigma0: 0.27281826734542847, sigma1: 0.003617800073698163, sigma2: 0.001088950433768332, sigmaX: 0.15420202910900116\n",
      "forward done\n",
      "tensor([0.0073, 0.0725, 0.4740, 0.4783, 0.3343], device='cuda:0',\n",
      "       dtype=torch.float64, grad_fn=<CatBackward0>)\n",
      "sigmaX grad tensor([[[-1.4286]]], device='cuda:0')\n",
      "sigma2 grad tensor([[63.0850]], device='cuda:0')\n",
      "sigma0 grad tensor([58.2653], device='cuda:0')\n",
      "sigma1 grad tensor([[53.0184]], device='cuda:0')\n",
      "gamma grad tensor([[-498.3779]], device='cuda:0')\n",
      "alpha grad tensor([[939.0740]], device='cuda:0')\n",
      "beta2 grad tensor([[-1025.4784]], device='cuda:0')\n",
      "beta0 grad tensor([-428.7986], device='cuda:0')\n",
      "beta1 grad tensor([[-623.5430]], device='cuda:0')\n",
      "Epoch 51 | Loss: 2.0191\n",
      "alpha: 0.0754232183098793, beta0: 0.11368197202682495, beta1: 0.00023315534053836018, beta2: 0.002318644430488348, \n",
      "gamma: 0.06389020383358002, sigma0: 0.27288618683815, sigma1: 0.0036641533952206373, sigma2: 0.001117672654800117, sigmaX: 0.15437738597393036\n",
      "forward done\n",
      "tensor([0.0114, 0.1174, 0.6022, 0.4784, 0.3040], device='cuda:0',\n",
      "       dtype=torch.float64, grad_fn=<CatBackward0>)\n",
      "sigmaX grad tensor([[[1.0212]]], device='cuda:0')\n",
      "sigma2 grad tensor([[-51.7309]], device='cuda:0')\n",
      "sigma0 grad tensor([-52.3873], device='cuda:0')\n",
      "sigma1 grad tensor([[-46.4895]], device='cuda:0')\n",
      "gamma grad tensor([[368.0741]], device='cuda:0')\n",
      "alpha grad tensor([[-723.6737]], device='cuda:0')\n",
      "beta2 grad tensor([[752.1202]], device='cuda:0')\n",
      "beta0 grad tensor([341.4646], device='cuda:0')\n",
      "beta1 grad tensor([[477.2351]], device='cuda:0')\n",
      "Epoch 52 | Loss: 2.5699\n",
      "alpha: 0.07548287510871887, beta0: 0.11360174417495728, beta1: 0.00017987852334044874, beta2: 0.0022901915945112705, \n",
      "gamma: 0.06385312229394913, sigma0: 0.273044615983963, sigma1: 0.0037905287463217974, sigma2: 0.0012113170232623816, sigmaX: 0.154441237449646\n",
      "forward done\n",
      "tensor([0.0125, 0.0317, 0.4833, 0.5296, 0.3590], device='cuda:0',\n",
      "       dtype=torch.float64, grad_fn=<CatBackward0>)\n",
      "sigmaX grad tensor([[[-0.7640]]], device='cuda:0')\n",
      "sigma2 grad tensor([[-40.2953]], device='cuda:0')\n",
      "sigma0 grad tensor([-35.0005], device='cuda:0')\n",
      "sigma1 grad tensor([[-33.2718]], device='cuda:0')\n",
      "gamma grad tensor([[225.3592]], device='cuda:0')\n",
      "alpha grad tensor([[-449.8288]], device='cuda:0')\n",
      "beta2 grad tensor([[510.8745]], device='cuda:0')\n",
      "beta0 grad tensor([216.3136], device='cuda:0')\n",
      "beta1 grad tensor([[313.4850]], device='cuda:0')\n",
      "Epoch 53 | Loss: 1.7014\n",
      "alpha: 0.07559522241353989, beta0: 0.11346917599439621, beta1: 7.623623969266191e-05, beta2: 0.0022158112842589617, \n",
      "gamma: 0.06376339495182037, sigma0: 0.27325329184532166, sigma1: 0.003965779673308134, sigma2: 0.0013492060825228691, sigmaX: 0.15456967055797577\n",
      "forward done\n",
      "tensor([0.0064, 0.0256, 0.4347, 0.5014, 0.3480], device='cuda:0',\n",
      "       dtype=torch.float64, grad_fn=<CatBackward0>)\n",
      "sigmaX grad tensor([[[-0.3604]]], device='cuda:0')\n",
      "sigma2 grad tensor([[14.2389]], device='cuda:0')\n",
      "sigma0 grad tensor([16.5571], device='cuda:0')\n",
      "sigma1 grad tensor([[13.8158]], device='cuda:0')\n",
      "gamma grad tensor([[-183.5834]], device='cuda:0')\n",
      "alpha grad tensor([[340.0900]], device='cuda:0')\n",
      "beta2 grad tensor([[-361.0895]], device='cuda:0')\n",
      "beta0 grad tensor([-148.2144], device='cuda:0')\n",
      "beta1 grad tensor([[-217.6088]], device='cuda:0')\n",
      "Epoch 54 | Loss: 1.5465\n",
      "alpha: 0.07565264403820038, beta0: 0.11339034885168076, beta1: 2.0983865397283807e-05, beta2: 0.002182953990995884, \n",
      "gamma: 0.06372812390327454, sigma0: 0.27341148257255554, sigma1: 0.004099361598491669, sigma2: 0.0014554851222783327, sigmaX: 0.15471981465816498\n",
      "forward done\n",
      "tensor([0.0103, 0.0205, 0.5100, 0.5329, 0.3974], device='cuda:0',\n",
      "       dtype=torch.float64, grad_fn=<CatBackward0>)\n",
      "sigmaX grad tensor([[[-0.8538]]], device='cuda:0')\n",
      "sigma2 grad tensor([[19.4640]], device='cuda:0')\n",
      "sigma0 grad tensor([21.7023], device='cuda:0')\n",
      "sigma1 grad tensor([[18.8026]], device='cuda:0')\n",
      "gamma grad tensor([[-228.0800]], device='cuda:0')\n",
      "alpha grad tensor([[417.3345]], device='cuda:0')\n",
      "beta2 grad tensor([[-450.6987]], device='cuda:0')\n",
      "beta0 grad tensor([-183.0483], device='cuda:0')\n",
      "beta1 grad tensor([[-271.0722]], device='cuda:0')\n",
      "Epoch 55 | Loss: 1.6557\n",
      "alpha: 0.07564979046583176, beta0: 0.1133703738451004, beta1: 1.9512090148054995e-05, beta2: 0.0021966940257698298, \n",
      "gamma: 0.06375376135110855, sigma0: 0.2735137641429901, sigma1: 0.004185571800917387, sigma2: 0.0015258678467944264, sigmaX: 0.15493528544902802\n",
      "forward done\n",
      "tensor([0.0151, 0.0316, 0.4452, 0.5034, 0.3414], device='cuda:0',\n",
      "       dtype=torch.float64, grad_fn=<CatBackward0>)\n",
      "sigmaX grad tensor([[[0.4173]]], device='cuda:0')\n",
      "sigma2 grad tensor([[4.3687]], device='cuda:0')\n",
      "sigma0 grad tensor([5.4978], device='cuda:0')\n",
      "sigma1 grad tensor([[4.3923]], device='cuda:0')\n",
      "gamma grad tensor([[-126.2028]], device='cuda:0')\n",
      "alpha grad tensor([[212.9631]], device='cuda:0')\n",
      "beta2 grad tensor([[-209.6936]], device='cuda:0')\n",
      "beta0 grad tensor([-84.4934], device='cuda:0')\n",
      "beta1 grad tensor([[-125.8354]], device='cuda:0')\n",
      "Epoch 56 | Loss: 1.6213\n",
      "alpha: 0.07561898231506348, beta0: 0.113376185297966, beta1: 4.092403105460107e-05, beta2: 0.002229587873443961, \n",
      "gamma: 0.06380912661552429, sigma0: 0.2735961079597473, sigma1: 0.004255605395883322, sigma2: 0.0015838691033422947, sigmaX: 0.1550913006067276\n",
      "forward done\n",
      "tensor([0.0137, 0.0370, 0.4857, 0.5098, 0.3512], device='cuda:0',\n",
      "       dtype=torch.float64, grad_fn=<CatBackward0>)\n",
      "sigmaX grad tensor([[[-0.3964]]], device='cuda:0')\n",
      "sigma2 grad tensor([[27.9641]], device='cuda:0')\n",
      "sigma0 grad tensor([28.7240], device='cuda:0')\n",
      "sigma1 grad tensor([[25.7330]], device='cuda:0')\n",
      "gamma grad tensor([[-263.7072]], device='cuda:0')\n",
      "alpha grad tensor([[492.3668]], device='cuda:0')\n",
      "beta2 grad tensor([[-559.4093]], device='cuda:0')\n",
      "beta0 grad tensor([-221.6527], device='cuda:0')\n",
      "beta1 grad tensor([[-332.1811]], device='cuda:0')\n",
      "Epoch 57 | Loss: 1.7304\n",
      "alpha: 0.07552555948495865, beta0: 0.11344457417726517, beta1: 0.00012067946227034554, beta2: 0.0023141850251704454, \n",
      "gamma: 0.0639265924692154, sigma0: 0.27361539006233215, sigma1: 0.004270466510206461, sigma2: 0.001598393195308745, sigmaX: 0.15527042746543884\n",
      "forward done\n",
      "tensor([0.0082, 0.0463, 0.5166, 0.4766, 0.2946], device='cuda:0',\n",
      "       dtype=torch.float64, grad_fn=<CatBackward0>)\n",
      "sigmaX grad tensor([[[-0.3544]]], device='cuda:0')\n",
      "sigma2 grad tensor([[-55.3658]], device='cuda:0')\n",
      "sigma0 grad tensor([-54.2228], device='cuda:0')\n",
      "sigma1 grad tensor([[-48.1099]], device='cuda:0')\n",
      "gamma grad tensor([[405.8647]], device='cuda:0')\n",
      "alpha grad tensor([[-789.4135]], device='cuda:0')\n",
      "beta2 grad tensor([[841.1735]], device='cuda:0')\n",
      "beta0 grad tensor([369.7109], device='cuda:0')\n",
      "beta1 grad tensor([[525.6893]], device='cuda:0')\n",
      "Epoch 58 | Loss: 1.7594\n",
      "alpha: 0.07554657012224197, beta0: 0.11340087652206421, beta1: 9.689376020105556e-05, beta2: 0.002307884395122528, \n",
      "gamma: 0.06392857432365417, sigma0: 0.27373695373535156, sigma1: 0.004374818876385689, sigma2: 0.0016871484695002437, sigmaX: 0.15546679496765137\n",
      "forward done\n",
      "tensor([0.0076, 0.0536, 0.4188, 0.5189, 0.3946], device='cuda:0',\n",
      "       dtype=torch.float64, grad_fn=<CatBackward0>)\n",
      "sigmaX grad tensor([[[-1.5213]]], device='cuda:0')\n",
      "sigma2 grad tensor([[-54.5260]], device='cuda:0')\n",
      "sigma0 grad tensor([-53.7939], device='cuda:0')\n",
      "sigma1 grad tensor([[-48.5822]], device='cuda:0')\n",
      "gamma grad tensor([[394.2583]], device='cuda:0')\n",
      "alpha grad tensor([[-771.5898]], device='cuda:0')\n",
      "beta2 grad tensor([[831.7972]], device='cuda:0')\n",
      "beta0 grad tensor([362.5487], device='cuda:0')\n",
      "beta1 grad tensor([[518.0956]], device='cuda:0')\n",
      "Epoch 59 | Loss: 1.8761\n",
      "alpha: 0.07566803693771362, beta0: 0.11325850337743759, beta1: -1.883082222775556e-05, beta2: 0.0022202630061656237, \n",
      "gamma: 0.0638296902179718, sigma0: 0.27394962310791016, sigma1: 0.004560806788504124, sigma2: 0.0018420825945213437, sigmaX: 0.15578603744506836\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward done\n",
      "tensor([0.0138, 0.1242, 0.5367, 0.4968, 0.3634], device='cuda:0',\n",
      "       dtype=torch.float64, grad_fn=<CatBackward0>)\n",
      "sigmaX grad tensor([[[-0.7234]]], device='cuda:0')\n",
      "sigma2 grad tensor([[82.5785]], device='cuda:0')\n",
      "sigma0 grad tensor([65.1563], device='cuda:0')\n",
      "sigma1 grad tensor([[65.0885]], device='cuda:0')\n",
      "gamma grad tensor([[-471.1457]], device='cuda:0')\n",
      "alpha grad tensor([[922.1339]], device='cuda:0')\n",
      "beta2 grad tensor([[-1117.5850]], device='cuda:0')\n",
      "beta0 grad tensor([-435.0347], device='cuda:0')\n",
      "beta1 grad tensor([[-656.1675]], device='cuda:0')\n",
      "Epoch 60 | Loss: 2.6530\n",
      "alpha: 0.07565483450889587, beta0: 0.11325407028198242, beta1: -3.60832905244024e-06, beta2: 0.002251430181786418, \n",
      "gamma: 0.06386087089776993, sigma0: 0.27401554584503174, sigma1: 0.004604792688041925, sigma2: 0.0018682318041101098, sigmaX: 0.15614348649978638\n",
      "forward done\n",
      "tensor([0.0140, 0.0320, 0.5641, 0.4785, 0.3506], device='cuda:0',\n",
      "       dtype=torch.float64, grad_fn=<CatBackward0>)\n",
      "sigmaX grad tensor([[[-0.2897]]], device='cuda:0')\n",
      "sigma2 grad tensor([[34.6228]], device='cuda:0')\n",
      "sigma0 grad tensor([29.4334], device='cuda:0')\n",
      "sigma1 grad tensor([[28.4241]], device='cuda:0')\n",
      "gamma grad tensor([[-244.3376]], device='cuda:0')\n",
      "alpha grad tensor([[462.7322]], device='cuda:0')\n",
      "beta2 grad tensor([[-533.8809]], device='cuda:0')\n",
      "beta0 grad tensor([-211.5144], device='cuda:0')\n",
      "beta1 grad tensor([[-317.0778]], device='cuda:0')\n",
      "Epoch 61 | Loss: 1.7275\n",
      "alpha: 0.07558134198188782, beta0: 0.11331022530794144, beta1: 6.79783770465292e-05, beta2: 0.002332445699721575, \n",
      "gamma: 0.0639515221118927, sigma0: 0.27401861548423767, sigma1: 0.004590736236423254, sigma2: 0.001844261772930622, sigmaX: 0.15649542212486267\n",
      "forward done\n",
      "tensor([0.0148, 0.0194, 0.5664, 0.4655, 0.3330], device='cuda:0',\n",
      "       dtype=torch.float64, grad_fn=<CatBackward0>)\n",
      "sigmaX grad tensor([[[-0.1708]]], device='cuda:0')\n",
      "sigma2 grad tensor([[-15.7487]], device='cuda:0')\n",
      "sigma0 grad tensor([-12.9840], device='cuda:0')\n",
      "sigma1 grad tensor([[-12.5391]], device='cuda:0')\n",
      "gamma grad tensor([[69.0348]], device='cuda:0')\n",
      "alpha grad tensor([[-150.1012]], device='cuda:0')\n",
      "beta2 grad tensor([[178.6017]], device='cuda:0')\n",
      "beta0 grad tensor([75.4612], device='cuda:0')\n",
      "beta1 grad tensor([[109.6858]], device='cuda:0')\n",
      "Epoch 62 | Loss: 1.5741\n",
      "alpha: 0.07553480565547943, beta0: 0.11333952844142914, beta1: 0.00011275458382442594, beta2: 0.0023881320375949144, \n",
      "gamma: 0.06401596963405609, sigma0: 0.27404654026031494, sigma1: 0.004601936787366867, sigma2: 0.0018443656153976917, sigmaX: 0.15683118999004364\n",
      "forward done\n",
      "tensor([0.0104, 0.0458, 0.5772, 0.4676, 0.3509], device='cuda:0',\n",
      "       dtype=torch.float64, grad_fn=<CatBackward0>)\n",
      "sigmaX grad tensor([[[0.9715]]], device='cuda:0')\n",
      "sigma2 grad tensor([[47.8089]], device='cuda:0')\n",
      "sigma0 grad tensor([43.8272], device='cuda:0')\n",
      "sigma1 grad tensor([[40.9458]], device='cuda:0')\n",
      "gamma grad tensor([[-337.3218]], device='cuda:0')\n",
      "alpha grad tensor([[650.6918]], device='cuda:0')\n",
      "beta2 grad tensor([[-753.6217]], device='cuda:0')\n",
      "beta0 grad tensor([-302.2756], device='cuda:0')\n",
      "beta1 grad tensor([[-449.3692]], device='cuda:0')\n",
      "Epoch 63 | Loss: 1.8645\n",
      "alpha: 0.07540545612573624, beta0: 0.11345276981592178, beta1: 0.00023592663637828082, beta2: 0.00251379469409585, \n",
      "gamma: 0.06416113674640656, sigma0: 0.27398666739463806, sigma1: 0.004533607978373766, sigma2: 0.0017778534675017, sigmaX: 0.157039612531662\n",
      "forward done\n",
      "tensor([0.0141, 0.0596, 0.4997, 0.4775, 0.3587], device='cuda:0',\n",
      "       dtype=torch.float64, grad_fn=<CatBackward0>)\n",
      "sigmaX grad tensor([[[-0.3309]]], device='cuda:0')\n",
      "sigma2 grad tensor([[-46.5976]], device='cuda:0')\n",
      "sigma0 grad tensor([-43.5243], device='cuda:0')\n",
      "sigma1 grad tensor([[-39.5980]], device='cuda:0')\n",
      "gamma grad tensor([[295.3916]], device='cuda:0')\n",
      "alpha grad tensor([[-583.8577]], device='cuda:0')\n",
      "beta2 grad tensor([[665.5055]], device='cuda:0')\n",
      "beta0 grad tensor([278.0467], device='cuda:0')\n",
      "beta1 grad tensor([[404.8202]], device='cuda:0')\n",
      "Epoch 64 | Loss: 1.9459\n",
      "alpha: 0.07536733895540237, beta0: 0.11347488313913345, beta1: 0.0002723377547226846, beta2: 0.002560532186180353, \n",
      "gamma: 0.06421572715044022, sigma0: 0.274017333984375, sigma1: 0.004548016935586929, sigma2: 0.0017829338321462274, sigmaX: 0.15726077556610107\n",
      "forward done\n",
      "tensor([0.0157, 0.1413, 0.4416, 0.5222, 0.4018], device='cuda:0',\n",
      "       dtype=torch.float64, grad_fn=<CatBackward0>)\n",
      "sigmaX grad tensor([[[0.0266]]], device='cuda:0')\n",
      "sigma2 grad tensor([[-54.9741]], device='cuda:0')\n",
      "sigma0 grad tensor([-48.5976], device='cuda:0')\n",
      "sigma1 grad tensor([[-45.6294]], device='cuda:0')\n",
      "gamma grad tensor([[324.9123]], device='cuda:0')\n",
      "alpha grad tensor([[-649.0430]], device='cuda:0')\n",
      "beta2 grad tensor([[726.9109]], device='cuda:0')\n",
      "beta0 grad tensor([310.1230], device='cuda:0')\n",
      "beta1 grad tensor([[446.6018]], device='cuda:0')\n",
      "Epoch 65 | Loss: 2.7939\n",
      "alpha: 0.0754205584526062, beta0: 0.11340536177158356, beta1: 0.00022244059073273093, beta2: 0.0025294669903814793, \n",
      "gamma: 0.0641806498169899, sigma0: 0.27413955330848694, sigma1: 0.004648857284337282, sigma2: 0.001864694175310433, sigmaX: 0.15745876729488373\n",
      "forward done\n",
      "tensor([0.0138, 0.0289, 0.4647, 0.5272, 0.3742], device='cuda:0',\n",
      "       dtype=torch.float64, grad_fn=<CatBackward0>)\n",
      "sigmaX grad tensor([[[1.8253]]], device='cuda:0')\n",
      "sigma2 grad tensor([[-5.7243]], device='cuda:0')\n",
      "sigma0 grad tensor([-2.9907], device='cuda:0')\n",
      "sigma1 grad tensor([[-3.0998]], device='cuda:0')\n",
      "gamma grad tensor([[-67.9943]], device='cuda:0')\n",
      "alpha grad tensor([[101.8845]], device='cuda:0')\n",
      "beta2 grad tensor([[-86.2249]], device='cuda:0')\n",
      "beta0 grad tensor([-32.2241], device='cuda:0')\n",
      "beta1 grad tensor([[-50.0958]], device='cuda:0')\n",
      "Epoch 66 | Loss: 1.6689\n",
      "alpha: 0.0754549652338028, beta0: 0.11335168033838272, beta1: 0.00018654274754226208, beta2: 0.002510053338482976, \n",
      "gamma: 0.06416662037372589, sigma0: 0.27425625920295715, sigma1: 0.004746324382722378, sigma2: 0.001946939155459404, sigmaX: 0.15745767951011658\n",
      "forward done\n",
      "tensor([0.0104, 0.0237, 0.5283, 0.5190, 0.3619], device='cuda:0',\n",
      "       dtype=torch.float64, grad_fn=<CatBackward0>)\n",
      "sigmaX grad tensor([[[0.6968]]], device='cuda:0')\n",
      "sigma2 grad tensor([[-25.5446]], device='cuda:0')\n",
      "sigma0 grad tensor([-21.8790], device='cuda:0')\n",
      "sigma1 grad tensor([[-21.2036]], device='cuda:0')\n",
      "gamma grad tensor([[91.8986]], device='cuda:0')\n",
      "alpha grad tensor([[-209.0097]], device='cuda:0')\n",
      "beta2 grad tensor([[261.7361]], device='cuda:0')\n",
      "beta0 grad tensor([110.1639], device='cuda:0')\n",
      "beta1 grad tensor([[160.6343]], device='cuda:0')\n",
      "Epoch 67 | Loss: 1.6570\n",
      "alpha: 0.0755147635936737, beta0: 0.11327080428600311, beta1: 0.00012381495616864413, beta2: 0.0024656830355525017, \n",
      "gamma: 0.06412971019744873, sigma0: 0.2744051218032837, sigma1: 0.0048759919591248035, sigma2: 0.0020578200928866863, sigmaX: 0.15738911926746368\n",
      "forward done\n",
      "tensor([0.0085, 0.0575, 0.4418, 0.4815, 0.3811], device='cuda:0',\n",
      "       dtype=torch.float64, grad_fn=<CatBackward0>)\n",
      "sigmaX grad tensor([[[-0.2046]]], device='cuda:0')\n",
      "sigma2 grad tensor([[-45.6180]], device='cuda:0')\n",
      "sigma0 grad tensor([-53.4991], device='cuda:0')\n",
      "sigma1 grad tensor([[-44.8044]], device='cuda:0')\n",
      "gamma grad tensor([[398.5894]], device='cuda:0')\n",
      "alpha grad tensor([[-775.1111]], device='cuda:0')\n",
      "beta2 grad tensor([[819.7173]], device='cuda:0')\n",
      "beta0 grad tensor([363.8540], device='cuda:0')\n",
      "beta1 grad tensor([[515.6304]], device='cuda:0')\n",
      "Epoch 68 | Loss: 1.8882\n",
      "alpha: 0.07567454129457474, beta0: 0.11309172958135605, beta1: -2.962948383355979e-05, beta2: 0.0023416278418153524, \n",
      "gamma: 0.06399165838956833, sigma0: 0.27464449405670166, sigma1: 0.005080509465187788, sigma2: 0.002223274437710643, sigmaX: 0.1573469489812851\n",
      "forward done\n",
      "tensor([0.0128, 0.0972, 0.4427, 0.5456, 0.4251], device='cuda:0',\n",
      "       dtype=torch.float64, grad_fn=<CatBackward0>)\n",
      "sigmaX grad tensor([[[1.4002]]], device='cuda:0')\n",
      "sigma2 grad tensor([[-47.9745]], device='cuda:0')\n",
      "sigma0 grad tensor([-51.9033], device='cuda:0')\n",
      "sigma1 grad tensor([[-44.0898]], device='cuda:0')\n",
      "gamma grad tensor([[399.5387]], device='cuda:0')\n",
      "alpha grad tensor([[-774.1703]], device='cuda:0')\n",
      "beta2 grad tensor([[814.4944]], device='cuda:0')\n",
      "beta0 grad tensor([361.3810], device='cuda:0')\n",
      "beta1 grad tensor([[510.5200]], device='cuda:0')\n",
      "Epoch 69 | Loss: 2.3979\n",
      "alpha: 0.07592398673295975, beta0: 0.11282516270875931, beta1: -0.00026385748060420156, beta2: 0.002146051963791251, \n",
      "gamma: 0.06376248598098755, sigma0: 0.2749621272087097, sigma1: 0.005351294297724962, sigma2: 0.002441628836095333, sigmaX: 0.15717335045337677\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward done\n",
      "tensor([0.0157, 0.1607, 0.5355, 0.4715, 0.3405], device='cuda:0',\n",
      "       dtype=torch.float64, grad_fn=<CatBackward0>)\n",
      "sigmaX grad tensor([[[1.1294]]], device='cuda:0')\n",
      "sigma2 grad tensor([[79.2512]], device='cuda:0')\n",
      "sigma0 grad tensor([66.9544], device='cuda:0')\n",
      "sigma1 grad tensor([[64.3517]], device='cuda:0')\n",
      "gamma grad tensor([[-531.3256]], device='cuda:0')\n",
      "alpha grad tensor([[1018.7272]], device='cuda:0')\n",
      "beta2 grad tensor([[-1263.5509]], device='cuda:0')\n",
      "beta0 grad tensor([-473.4440], device='cuda:0')\n",
      "beta1 grad tensor([[-726.7317]], device='cuda:0')\n",
      "Epoch 70 | Loss: 2.9699\n",
      "alpha: 0.07600870728492737, beta0: 0.1127241998910904, beta1: -0.00033745658583939075, beta2: 0.0021000842098146677, \n",
      "gamma: 0.06369636207818985, sigma0: 0.2751151919364929, sigma1: 0.005468415562063456, sigma2: 0.002524198964238167, sigmaX: 0.15690790116786957\n",
      "forward done\n",
      "tensor([0.0251, 0.2256, 0.5298, 0.4727, 0.3297], device='cuda:0',\n",
      "       dtype=torch.float64, grad_fn=<CatBackward0>)\n",
      "sigmaX grad tensor([[[0.6136]]], device='cuda:0')\n",
      "sigma2 grad tensor([[80.7822]], device='cuda:0')\n",
      "sigma0 grad tensor([69.3914], device='cuda:0')\n",
      "sigma1 grad tensor([[67.0053]], device='cuda:0')\n",
      "gamma grad tensor([[-640.2907]], device='cuda:0')\n",
      "alpha grad tensor([[1192.2288]], device='cuda:0')\n",
      "beta2 grad tensor([[-1450.0055]], device='cuda:0')\n",
      "beta0 grad tensor([-541.6149], device='cuda:0')\n",
      "beta1 grad tensor([[-835.6555]], device='cuda:0')\n",
      "Epoch 71 | Loss: 3.6137\n",
      "alpha: 0.07592414319515228, beta0: 0.11278977245092392, beta1: -0.000248380791163072, beta2: 0.00220603053458035, \n",
      "gamma: 0.06380262970924377, sigma0: 0.275116890668869, sigma1: 0.005443244706839323, sigma2: 0.002482903189957142, sigmaX: 0.15660816431045532\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "\n",
    "#datadirName = \"/content/drive/MyDrive/trajectories_miura/\"\n",
    "\n",
    "alpha_init = 0.0741\n",
    "beta0_init = 0.116\n",
    "beta1_init = 0.0\n",
    "beta2_init = 0.0\n",
    "gamma_init = 0.0641\n",
    "sigma0_init = 0.266\n",
    "sigma1_init = 0.0\n",
    "sigma2_init = 0.0\n",
    "sigmaX_init = 0.155\n",
    "\n",
    "initIns = {'alpha': alpha_init,\n",
    "           'beta0': beta0_init,\n",
    "           'beta1': beta1_init,\n",
    "           'beta2': beta2_init,\n",
    "           'gamma': gamma_init,\n",
    "           'sigma0': sigma0_init,\n",
    "           'sigma1': sigma1_init,\n",
    "           'sigma2': sigma2_init,\n",
    "           'sigmaX': sigmaX_init}\n",
    "\n",
    "model = moduleSDE(initIns)\n",
    "\n",
    "printparams(model)\n",
    "\n",
    "lossfunc = customLoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)#, weight_decay=5e-4)\n",
    "\n",
    "loss_log = []\n",
    "\n",
    "# learnig loop\n",
    "for epoch in range(200):\n",
    "    if 'out' in globals():\n",
    "        del out\n",
    "    if 'loss' in globals():\n",
    "        del loss\n",
    "        \n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    bm = BrownianInterval(t0=ts[0], \n",
    "                          t1=ts[-1], \n",
    "                          size=(batch_size, brownian_size),\n",
    "                          dt=stepSDE,\n",
    "                          device=device)\n",
    "\n",
    "    y0 = torch.rand((batch_size,1), device=device)\n",
    "    y0 = torch.concat((torch.zeros_like(y0), torch.zeros_like(y0),          #x,y = 0,0\n",
    "                     torch.cos(y0*(2*np.pi)), torch.sin(y0*(2*np.pi)),    #|v| = 1\n",
    "                     torch.zeros_like(y0), torch.zeros_like(y0)), 1)      #|V| = 0\n",
    "\n",
    "    rnoise = torch.randn((Nts, batch_size, 2), device=device)\n",
    "\n",
    "    #with torch.autograd.detect_anomaly():\n",
    "    out = model(y0, bm, rnoise)\n",
    "    #print('out', out)\n",
    "    print('forward done')\n",
    "\n",
    "    #out.register_hook(lambda grad: print('trajectory back', grad))#(grad != grad).any().item()))\n",
    "\n",
    "    loss = lossfunc(out)\n",
    "    #print('loss {}'.format(str(loss)))\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    print('Epoch %d | Loss: %.4f' % (epoch, loss.item()))\n",
    "    \n",
    "    loss_log.append(loss.item())\n",
    "\n",
    "    printparams(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-09-13T06:43:58.594Z"
    },
    "id": "CXVcQSJczvlj"
   },
   "outputs": [],
   "source": [
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-09-13T06:43:58.597Z"
    },
    "id": "MZNhI5GFFacj"
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "now = datetime.datetime.now()\n",
    "\n",
    "def extractParams(model):\n",
    "    alpha = model.sde.alpha[0,0].cpu().detach().numpy()\n",
    "    beta0 = model.sde.beta.layer1.bias[0].cpu().detach().numpy()\n",
    "    beta1 = model.sde.beta.layer1.weight[0,0].cpu().detach().numpy()\n",
    "    beta2 = model.sde.beta.layer3.weight[0,0].cpu().detach().numpy()\n",
    "    gamma = model.sde.gamma[0,0].cpu().detach().numpy()\n",
    "    sigma0 = model.sde.sigma.layer1.bias[0].cpu().detach().numpy()\n",
    "    sigma1 = model.sde.sigma.layer1.weight[0,0].cpu().detach().numpy()\n",
    "    sigma2 = model.sde.sigma.layer3.weight[0,0].cpu().detach().numpy()\n",
    "    sigmaX = model.sigmaX[0,0,0].cpu().detach().numpy()\n",
    "\n",
    "    return {'alpha': alpha, \n",
    "            'beta0': beta0,\n",
    "            'beta1': beta1, \n",
    "            'beta2': beta2,\n",
    "            'gamma': gamma, \n",
    "            'sigma0': sigma0,\n",
    "            'sigma1': sigma1,\n",
    "            'sigma2': sigma2, \n",
    "            'sigmaX': sigmaX}\n",
    "\n",
    "bestParams = extractParams(model)\n",
    "\n",
    "np.savez(savedir+'bestParams_' + now.strftime('%Y%m%d_%H%M%S'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rktzzGJTn161"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOyexgXk7VjzKwoeEmDr4SK",
   "collapsed_sections": [],
   "provenance": [
    {
     "file_id": "1iThXlCWDmK8QDx5A3VzYH6m7GkQczcPP",
     "timestamp": 1646628037404
    },
    {
     "file_id": "1cC1UC5ySJ6V1P8CJlI1rat7GHYmXTtcb",
     "timestamp": 1646395520305
    },
    {
     "file_id": "1Q2O4CtsXiZkN21hP3997Mxzf1ef1RmDJ",
     "timestamp": 1646225534045
    }
   ]
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
