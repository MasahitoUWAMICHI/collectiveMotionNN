{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f855b76a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-27T02:41:12.495201Z",
     "start_time": "2022-10-27T02:41:12.484468Z"
    }
   },
   "outputs": [],
   "source": [
    "#!pip install dgl-cu113 dglgo -f https://data.dgl.ai/wheels/repo.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7807020c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-27T02:41:12.508939Z",
     "start_time": "2022-10-27T02:41:12.503975Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['DGLBACKEND'] = 'pytorch'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d12568c6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-27T02:41:12.523328Z",
     "start_time": "2022-10-27T02:41:12.517785Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import dgl\n",
    "import dgl.function as fn\n",
    "\n",
    "import networkx as nx\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import gc\n",
    "\n",
    "# デバイス設定\n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "61e1f3ee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-27T02:41:12.536072Z",
     "start_time": "2022-10-27T02:41:12.527158Z"
    }
   },
   "outputs": [],
   "source": [
    "def printNPZ(npz):\n",
    "    for kw in npz.files:\n",
    "        print(kw, npz[kw])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0ba5ba52",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-27T02:41:12.573038Z",
     "start_time": "2022-10-27T02:41:12.538897Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v0 1.0\n",
      "r 1.0\n",
      "D 0.1\n",
      "A 0.0\n",
      "L 20\n",
      "rho 1.0\n",
      "beta 1.0\n",
      "A_CFs [0.1 0.9]\n",
      "A_CIL 0.0\n",
      "cellType_ratio [0.5 0.5]\n",
      "quiv_colors ['k' 'r']\n",
      "kappa 0.5\n",
      "A_Macdonalds [2.  0.2]\n",
      "batch_size 400\n",
      "state_size 3\n",
      "brownian_size 1\n",
      "periodic True\n",
      "t_max 1000\n",
      "methodSDE heun\n",
      "isIto False\n",
      "stepSDE 0.01\n"
     ]
    }
   ],
   "source": [
    "dirName = './HiraiwaModel_chem20220916_150816/'\n",
    "savedirName = dirName + 'ActiveNet_vp_rotsym_multiStep_batchNorm/'\n",
    "os.makedirs(savedirName, exist_ok=True)\n",
    "\n",
    "params = np.load(dirName+'params.npz')\n",
    "#traj = np.load(dirName+'result.npz')\n",
    "\n",
    "printNPZ(params)\n",
    "#printNPZ(traj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c4e73591",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-27T02:41:12.589948Z",
     "start_time": "2022-10-27T02:41:12.575371Z"
    }
   },
   "outputs": [],
   "source": [
    "if params['periodic']:\n",
    "    L = torch.tensor(params['L'])\n",
    "    def calc_dr(r1, r2):\n",
    "        dr = torch.remainder((r1 - r2), L)\n",
    "        dr[dr > L/2] = dr[dr > L/2] - L\n",
    "        return dr\n",
    "else:\n",
    "    def calc_dr(r1, r2):\n",
    "        return r1 - r2\n",
    "    \n",
    "def makeGraph(x_data, r_thresh):\n",
    "        Ndata = x_data.size(0)\n",
    "        dx = calc_dr(torch.unsqueeze(x_data, 0), torch.unsqueeze(x_data, 1))\n",
    "        dx = torch.sum(dx**2, dim=2)\n",
    "        edges = torch.argwhere(dx < r_thresh/2)\n",
    "        return dgl.graph((edges[:,0], edges[:,1]), num_nodes=Ndata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e3e01d8a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-27T02:41:12.602022Z",
     "start_time": "2022-10-27T02:41:12.592924Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['./HiraiwaModel_chem20220916_150816/20221013_130509', './HiraiwaModel_chem20220916_150816/20221013_222508', './HiraiwaModel_chem20220916_150816/20221014_020927', './HiraiwaModel_chem20220916_150816/20221014_055334', './HiraiwaModel_chem20220916_150816/20221014_093729', './HiraiwaModel_chem20220916_150816/ActiveNet_celltypes', './HiraiwaModel_chem20220916_150816/ActiveNet_vp_rotsym', './HiraiwaModel_chem20220916_150816/20221022_000642', './HiraiwaModel_chem20220916_150816/ActiveNet_vp_rotsym_dropout', './HiraiwaModel_chem20220916_150816/ActiveNet_vp_rotsym_batchNorm', './HiraiwaModel_chem20220916_150816/20221024_041213', './HiraiwaModel_chem20220916_150816/20221024_080435', './HiraiwaModel_chem20220916_150816/20221024_115625', './HiraiwaModel_chem20220916_150816/20221024_155544', './HiraiwaModel_chem20220916_150816/20221024_195452', './HiraiwaModel_chem20220916_150816/ActiveNet_vp_rotsym_multiStep_batchNorm', './HiraiwaModel_chem20220916_150816/ActiveNet_vp_rotsym_multiStep_transfer_batchNorm']\n",
      "['./HiraiwaModel_chem20220916_150816/20221013_130509', './HiraiwaModel_chem20220916_150816/20221013_222508', './HiraiwaModel_chem20220916_150816/20221014_020927', './HiraiwaModel_chem20220916_150816/20221014_055334', './HiraiwaModel_chem20220916_150816/20221014_093729', './HiraiwaModel_chem20220916_150816/20221022_000642', './HiraiwaModel_chem20220916_150816/20221024_041213', './HiraiwaModel_chem20220916_150816/20221024_080435', './HiraiwaModel_chem20220916_150816/20221024_115625', './HiraiwaModel_chem20220916_150816/20221024_155544', './HiraiwaModel_chem20220916_150816/20221024_195452']\n"
     ]
    }
   ],
   "source": [
    "subdir_list = [f.path for f in os.scandir(dirName) if f.is_dir()]\n",
    "\n",
    "print(subdir_list)\n",
    "\n",
    "datadir_list = [f for f in subdir_list if 'result.npz' in [ff.name for ff in os.scandir(f)]]\n",
    "\n",
    "print(datadir_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "19f22447",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-27T02:41:12.612184Z",
     "start_time": "2022-10-27T02:41:12.605432Z"
    }
   },
   "outputs": [],
   "source": [
    "class myDataset(Dataset):\n",
    "    def __init__(self, data_x, celltype_List, t_yseq=1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.data_x = data_x # List of tensors\n",
    "        #self.data_y = data_y\n",
    "        self.celltype_List = celltype_List\n",
    "        \n",
    "        self.data_len = np.array([xx.size(0) for xx in self.data_x])\n",
    "        self.t_yseq = t_yseq\n",
    "        \n",
    "        self.data_len_cumsum = np.cumsum(self.data_len - (self.t_yseq - 1))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return (self.data_len - (self.t_yseq - 1)).sum()\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        id_List = np.argwhere(index<self.data_len_cumsum)[0,0]\n",
    "        \n",
    "        if id_List:\n",
    "            id_tensor = index - self.data_len_cumsum[id_List-1]\n",
    "        else:\n",
    "            id_tensor = index\n",
    "        \n",
    "        return self.data_x[id_List][id_tensor], self.data_x[id_List][id_tensor:(id_tensor+self.t_yseq)], self.celltype_List[id_List]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3e9e2f01",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-27T02:41:13.115415Z",
     "start_time": "2022-10-27T02:41:12.616541Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "601"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dr_thresh = 7\n",
    "dt = 1\n",
    "batch_size = 8\n",
    "\n",
    "T_pred = 3\n",
    "\n",
    "N_data = len(datadir_list)\n",
    "\n",
    "#TR_VA_rate = np.array([0.6, 0.2])\n",
    "\n",
    "TR_last = 5\n",
    "VA_last = 7\n",
    "\n",
    "shuffle_inds = np.arange(N_data, dtype=int)\n",
    "np.random.shuffle(shuffle_inds)\n",
    "\n",
    "train_inds = shuffle_inds[:TR_last]\n",
    "valid_inds = shuffle_inds[TR_last:VA_last]\n",
    "test_inds = shuffle_inds[VA_last:]\n",
    "\n",
    "celltype_lst = []\n",
    "\n",
    "train_x = []\n",
    "valid_x = []\n",
    "test_x = []\n",
    "\n",
    "train_y = []\n",
    "valid_y = []\n",
    "test_y = []\n",
    "\n",
    "train_ct = []\n",
    "valid_ct = []\n",
    "test_ct = []\n",
    "\n",
    "for i_dir, subdirName in enumerate(datadir_list):\n",
    "    \n",
    "    traj = np.load(subdirName+'/result.npz')\n",
    "    \n",
    "    xy_t = torch.tensor(traj['xy'])#[:-1,:,:])\n",
    "    #v_t = calc_dr(torch.tensor(traj['xy'][1:,:,:]), torch.tensor(traj['xy'][:-1,:,:])) / dt\n",
    "    p_t = torch.unsqueeze(torch.tensor(traj['theta']), dim=2)#[:-1,:]), dim=2)\n",
    "    #w_t = torch.unsqueeze(torch.tensor((traj['theta'][1:,:]-traj['theta'][:-1,:])%(2*np.pi)/dt), dim=2)\n",
    "    \n",
    "    if i_dir in train_inds:\n",
    "        train_x.append(torch.concat((xy_t, p_t), -1))\n",
    "        #train_y.append(torch.concat((v_t, w_t), -1))\n",
    "        train_ct.append(torch.tensor(traj['celltype_label']).view(-1,1))\n",
    "\n",
    "    if i_dir in valid_inds:\n",
    "        valid_x.append(torch.concat((xy_t, p_t), -1))\n",
    "        #valid_y.append(torch.concat((v_t, w_t), -1))\n",
    "        valid_ct.append(torch.tensor(traj['celltype_label']).view(-1,1))\n",
    "        \n",
    "    if i_dir in test_inds:\n",
    "        test_x.append(torch.concat((xy_t, p_t), -1))\n",
    "        #test_y.append(torch.concat((v_t, w_t), -1))\n",
    "        test_ct.append(torch.tensor(traj['celltype_label']).view(-1,1))\n",
    "    \n",
    "train_dataset = myDataset(train_x, train_ct, t_yseq=T_pred)\n",
    "\n",
    "valid_dataset = myDataset(valid_x, valid_ct, t_yseq=T_pred)\n",
    "\n",
    "test_dataset = myDataset(test_x, test_ct, t_yseq=T_pred)\n",
    "\n",
    "\n",
    "train_data = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, pin_memory=True)\n",
    "valid_data = torch.utils.data.DataLoader(valid_dataset, batch_size=batch_size, pin_memory=True)\n",
    "test_data = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, pin_memory=True)\n",
    "\n",
    "del train_x, train_ct, train_dataset\n",
    "del valid_x, valid_ct, valid_dataset\n",
    "del test_x, test_ct, test_dataset\n",
    "gc.collect()\n",
    "\n",
    "#print(data)\n",
    "#print(data.num_graphs)\n",
    "#print(data.x)\n",
    "#print(data.y)\n",
    "#print(data.edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1c0a633b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-27T02:41:13.121382Z",
     "start_time": "2022-10-27T02:41:13.117414Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 7  6 10  8  5]\n"
     ]
    }
   ],
   "source": [
    "print(train_inds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4f35452f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-27T02:41:13.132496Z",
     "start_time": "2022-10-27T02:41:13.123132Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_DataLoader__initialized',\n",
       " '_DataLoader__multiprocessing_context',\n",
       " '_IterableDataset_len_called',\n",
       " '__annotations__',\n",
       " '__class__',\n",
       " '__class_getitem__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__iter__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__orig_bases__',\n",
       " '__parameters__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__slots__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_auto_collation',\n",
       " '_dataset_kind',\n",
       " '_get_iterator',\n",
       " '_get_shared_seed',\n",
       " '_index_sampler',\n",
       " '_is_protocol',\n",
       " '_iterator',\n",
       " 'batch_sampler',\n",
       " 'batch_size',\n",
       " 'check_worker_number_rationality',\n",
       " 'collate_fn',\n",
       " 'dataset',\n",
       " 'drop_last',\n",
       " 'generator',\n",
       " 'multiprocessing_context',\n",
       " 'num_workers',\n",
       " 'persistent_workers',\n",
       " 'pin_memory',\n",
       " 'pin_memory_device',\n",
       " 'prefetch_factor',\n",
       " 'sampler',\n",
       " 'timeout',\n",
       " 'worker_init_fn']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e5e6c153",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-27T02:41:13.140822Z",
     "start_time": "2022-10-27T02:41:13.134404Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "625"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ca11fa4f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-27T02:41:13.163103Z",
     "start_time": "2022-10-27T02:41:13.143048Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 999 1998 2997 3996 4995]\n"
     ]
    }
   ],
   "source": [
    "print(train_data.dataset.data_len_cumsum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f94ee7f4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-27T02:41:13.187095Z",
     "start_time": "2022-10-27T02:41:13.171464Z"
    }
   },
   "outputs": [],
   "source": [
    "def plotGraph(data):\n",
    "\n",
    "    # networkxのグラフに変換\n",
    "    nxg = dgl.to_networkx(data)\n",
    "\n",
    "    # 可視化のためのページランク計算\n",
    "    pr = nx.pagerank(nxg)\n",
    "    pr_max = np.array(list(pr.values())).max()\n",
    "\n",
    "    # 可視化する際のノード位置\n",
    "    draw_pos = nx.spring_layout(nxg, seed=0) \n",
    "\n",
    "    # ノードの色設定\n",
    "    cmap = plt.get_cmap('tab10')\n",
    "    labels = data.y.numpy()\n",
    "    colors = [cmap(l) for l in labels]\n",
    "\n",
    "    # 図のサイズ\n",
    "    fig0 = plt.figure(figsize=(10, 10))\n",
    "\n",
    "    # 描画\n",
    "    nx.draw_networkx_nodes(nxg, \n",
    "                          draw_pos,\n",
    "                          node_size=[v / pr_max * 1000 for v in pr.values()])#,\n",
    "                          #node_color=colors, alpha=0.5)\n",
    "    nx.draw_networkx_edges(nxg, draw_pos, arrowstyle='-', alpha=0.2)\n",
    "    nx.draw_networkx_labels(nxg, draw_pos, font_size=10)\n",
    "\n",
    "    #plt.title('KarateClub')\n",
    "    plt.show()\n",
    "\n",
    "    return fig0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "21e97796",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-27T02:41:13.249789Z",
     "start_time": "2022-10-27T02:41:13.194138Z"
    }
   },
   "outputs": [],
   "source": [
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, Nchannels, dropout=0, batchN=False, flgBias=False):\n",
    "        super(NeuralNet, self).__init__()\n",
    "\n",
    "        if dropout:\n",
    "            self.dropout = nn.Dropout(p=dropout)\n",
    "        else:\n",
    "            self.dropout = 0\n",
    "            \n",
    "        if batchN:\n",
    "            self.bNorm1 = nn.BatchNorm1d(Nchannels)\n",
    "            self.bNorm2 = nn.BatchNorm1d(Nchannels)\n",
    "            self.bNorm3 = nn.BatchNorm1d(Nchannels)\n",
    "            \n",
    "        self.batchN=batchN\n",
    "        \n",
    "        self.layer1 = nn.Linear(in_channels, Nchannels, bias=flgBias)\n",
    "        self.layer2 = nn.Linear(Nchannels, Nchannels, bias=flgBias)\n",
    "        self.layer3 = nn.Linear(Nchannels, Nchannels, bias=flgBias)\n",
    "        self.layer4 = nn.Linear(Nchannels, out_channels, bias=flgBias)\n",
    "\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.layer1.reset_parameters()\n",
    "        self.layer2.reset_parameters()\n",
    "        self.layer3.reset_parameters()\n",
    "        self.layer4.reset_parameters()\n",
    "        #nn.init.zeros_(self.layer1.weight)\n",
    "        #nn.init.zeros_(self.layer2.weight)\n",
    "        #nn.init.zeros_(self.layer3.weight)\n",
    "        #nn.init.zeros_(self.layer4.weight)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.activation(self.layer1(x))\n",
    "        if self.batchN:\n",
    "            out = self.bNorm1(out)\n",
    "        if self.dropout:\n",
    "            out = self.dropout(out)\n",
    "        \n",
    "        out = self.activation(self.layer2(out))\n",
    "        if self.batchN:\n",
    "            out = self.bNorm2(out)\n",
    "        if self.dropout:\n",
    "            out = self.dropout(out)\n",
    "        \n",
    "        out = self.activation(self.layer3(out))\n",
    "        if self.batchN:\n",
    "            out = self.bNorm3(out)\n",
    "        if self.dropout:\n",
    "            out = self.dropout(out)\n",
    "        \n",
    "        out = self.layer4(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "class ActiveNet(nn.Module):\n",
    "    def __init__(self, xy_dim, r, dropout=0, batchN=False, bias=False, Nchannels=128):\n",
    "        super().__init__()\n",
    "\n",
    "        self.interactNN = NeuralNet(xy_dim*2 + 2, xy_dim, Nchannels, dropout, batchN, bias)\n",
    "\n",
    "        self.thetaDotNN = NeuralNet(xy_dim*2 + 2, 1, Nchannels, dropout, batchN, bias)\n",
    "        \n",
    "        self.selfpropel = nn.Parameter(torch.tensor(0.0, requires_grad=True, device=device))\n",
    "\n",
    "        #self.Normalizer = nn.Softmax(dim=1)\n",
    "\n",
    "        self.xy_dim = xy_dim\n",
    "        \n",
    "        self.r = r\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.interactNN.reset_parameters()\n",
    "\n",
    "        self.thetaDotNN.reset_parameters()\n",
    "        \n",
    "        nn.init.uniform_(self.selfpropel)\n",
    "\n",
    "        #self.bias.data.zero_()\n",
    "        \n",
    "    def load_celltypes(self, celltype):\n",
    "        self.celltype = celltype\n",
    "\n",
    "    def calc_message(self, edges):\n",
    "        dx = calc_dr(edges.dst['x'], edges.src['x'])\n",
    "\n",
    "        costheta = torch.cos(edges.dst['theta'])\n",
    "        sintheta = torch.sin(edges.dst['theta'])\n",
    "\n",
    "        dx_para = costheta * dx[..., :1] + sintheta * dx[..., 1:]\n",
    "        dx_perp = costheta * dx[..., 1:] - sintheta * dx[..., :1]\n",
    "\n",
    "        p_para_src = torch.cos(edges.src['theta'] - edges.dst['theta'])\n",
    "        p_perp_src = torch.sin(edges.src['theta'] - edges.dst['theta'])\n",
    "\n",
    "        rot_m_v = self.interactNN(torch.concat((dx_para, dx_perp, \n",
    "                                                p_para_src, p_perp_src,\n",
    "                                                edges.dst['type'], edges.src['type']), -1))\n",
    "\n",
    "        m_v = torch.concat((costheta * rot_m_v[..., :1] - sintheta * rot_m_v[..., 1:],\n",
    "                            costheta * rot_m_v[..., 1:] + sintheta * rot_m_v[..., :1]), -1)\n",
    "\n",
    "        m_theta = self.thetaDotNN(torch.concat((dx_para, dx_perp, \n",
    "                                                p_para_src, p_perp_src, \n",
    "                                                edges.dst['type'], edges.src['type']), -1))\n",
    "        \n",
    "        return {'m': torch.concat((m_v, m_theta), -1)}\n",
    "        \n",
    "    def forward(self, xv):\n",
    "        r_g = makeGraph(xv[..., :self.xy_dim], self.r/2)\n",
    "        r_g.ndata['x'] = xv[..., :self.xy_dim]\n",
    "        r_g.ndata['theta'] = xv[..., self.xy_dim:(self.xy_dim+1)]\n",
    "        r_g.ndata['type'] = self.celltype\n",
    "        r_g.update_all(self.calc_message, fn.sum('m', 'a'))\n",
    "        r_g.ndata['a'][..., :self.xy_dim] = r_g.ndata['a'][..., :self.xy_dim] + self.selfpropel * torch.concat((torch.cos(r_g.ndata['theta']), torch.sin(r_g.ndata['theta'])), -1)\n",
    "        \n",
    "        return r_g.ndata['a']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b52c1e60",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-27T02:41:13.259783Z",
     "start_time": "2022-10-27T02:41:13.252563Z"
    }
   },
   "outputs": [],
   "source": [
    "def myLoss(out, target):\n",
    "    #dv = torch.sum(torch.square(out[..., :xy_dim] - target[..., :xy_dim]), dim=-1)\n",
    "    dv = torch.sum(torch.square(calc_dr(out[..., :xy_dim], target[..., :xy_dim])), dim=-1)\n",
    "    dcos = torch.cos(out[..., xy_dim] - target[..., xy_dim])\n",
    "    \n",
    "    wei_shape = np.ones([dv.dim()], dtype=int)\n",
    "    wei_shape[0] = T_pred\n",
    "    wei = torch.tensor(np.reshape(1/np.arange(1, T_pred+1), wei_shape)).to(dv.device)\n",
    "    wei = wei/wei.mean()\n",
    "    \n",
    "    return torch.mean(dv*wei), torch.mean((1-dcos)*wei)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6033283c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-27T06:52:02.435969Z",
     "start_time": "2022-10-27T05:40:21.846871Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 | train Loss: [38.1907, 1.0089] | valid Loss: [35.5517, 0.9983]\n",
      "Epoch 1 | train Loss: [34.9242, 0.9996] | valid Loss: [32.3162, 1.0055]\n",
      "Epoch 2 | train Loss: [32.4367, 0.9998] | valid Loss: [29.5289, 1.0125]\n",
      "Epoch 3 | train Loss: [30.8456, 0.9848] | valid Loss: [27.6122, 1.0141]\n",
      "Epoch 4 | train Loss: [29.2139, 1.0038] | valid Loss: [26.1854, 1.0126]\n",
      "Epoch 5 | train Loss: [27.6221, 1.0143] | valid Loss: [26.0551, 1.0125]\n",
      "Epoch 6 | train Loss: [27.0333, 0.9788] | valid Loss: [25.7563, 1.0120]\n",
      "Epoch 7 | train Loss: [26.2185, 0.9883] | valid Loss: [25.4604, 1.0096]\n",
      "Epoch 8 | train Loss: [25.3958, 1.0205] | valid Loss: [26.4179, 1.0062]\n",
      "Epoch 9 | train Loss: [24.3118, 1.0044] | valid Loss: [27.3081, 1.0027]\n",
      "Epoch 10 | train Loss: [24.1553, 0.9939] | valid Loss: [28.5734, 1.0010]\n",
      "Epoch 11 | train Loss: [23.3556, 1.0098] | valid Loss: [29.3111, 1.0032]\n",
      "Epoch 12 | train Loss: [23.3963, 0.9891] | valid Loss: [30.5753, 1.0054]\n",
      "Epoch 00014: reducing learning rate of group 0 to 1.5000e-07.\n",
      "Epoch 13 | train Loss: [22.8988, 0.9969] | valid Loss: [31.8419, 1.0087]\n",
      "Epoch 14 | train Loss: [23.1617, 1.0155] | valid Loss: [32.6140, 1.0098]\n",
      "Epoch 15 | train Loss: [23.0273, 1.0186] | valid Loss: [32.1353, 1.0101]\n",
      "Epoch 16 | train Loss: [22.9705, 1.0063] | valid Loss: [32.8251, 1.0095]\n",
      "Epoch 17 | train Loss: [23.0690, 0.9896] | valid Loss: [33.2985, 1.0090]\n",
      "Epoch 18 | train Loss: [22.8881, 0.9985] | valid Loss: [33.7068, 1.0091]\n",
      "Epoch 00020: reducing learning rate of group 0 to 7.5000e-08.\n",
      "Epoch 19 | train Loss: [22.9165, 1.0110] | valid Loss: [33.4961, 1.0096]\n",
      "Epoch 20 | train Loss: [22.8932, 1.0160] | valid Loss: [33.5600, 1.0100]\n",
      "Epoch 21 | train Loss: [23.1460, 1.0270] | valid Loss: [33.8234, 1.0097]\n",
      "Epoch 22 | train Loss: [23.2265, 1.0123] | valid Loss: [33.9916, 1.0099]\n",
      "Epoch 23 | train Loss: [22.9069, 1.0115] | valid Loss: [33.9568, 1.0099]\n",
      "Epoch 24 | train Loss: [23.1405, 1.0086] | valid Loss: [33.9225, 1.0096]\n",
      "Epoch 00026: reducing learning rate of group 0 to 3.7500e-08.\n",
      "Epoch 25 | train Loss: [23.6278, 0.9879] | valid Loss: [34.1202, 1.0088]\n",
      "Epoch 26 | train Loss: [23.7873, 0.9983] | valid Loss: [34.1824, 1.0092]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [56]\u001b[0m, in \u001b[0;36m<cell line: 28>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ib \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(batch_x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)):\n\u001b[1;32m     35\u001b[0m     model\u001b[38;5;241m.\u001b[39mload_celltypes(batch_ct[ib]\u001b[38;5;241m.\u001b[39mto(device))\n\u001b[0;32m---> 36\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mcalc_multiSteps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_x\u001b[49m\u001b[43m[\u001b[49m\u001b[43mib\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m     lv, ltheta \u001b[38;5;241m=\u001b[39m myLoss(out, batch_y[ib]\u001b[38;5;241m.\u001b[39mto(device))\n\u001b[1;32m     38\u001b[0m     lossv \u001b[38;5;241m=\u001b[39m lossv \u001b[38;5;241m+\u001b[39m lv\n",
      "Input \u001b[0;32mIn [56]\u001b[0m, in \u001b[0;36mcalc_multiSteps\u001b[0;34m(x0)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i_step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(T_pred):\n\u001b[1;32m     12\u001b[0m     x_i \u001b[38;5;241m=\u001b[39m x_i \u001b[38;5;241m+\u001b[39m model(x_i) \u001b[38;5;241m*\u001b[39m dt\n\u001b[0;32m---> 13\u001b[0m     outs\u001b[38;5;241m.\u001b[39mappend(\u001b[43mx_i\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mstack(outs, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# モデルのインスタンス生成\n",
    "xy_dim = 2\n",
    "\n",
    "model = ActiveNet(xy_dim, dr_thresh, dropout=0, batchN=True, bias=True, Nchannels=128).to(device)\n",
    "# input data\n",
    "#data = dataset[0]\n",
    "\n",
    "def calc_multiSteps(x0):\n",
    "    outs = []\n",
    "    x_i = x0\n",
    "    for i_step in range(T_pred):\n",
    "        x_i = x_i + model(x_i) * dt\n",
    "        outs.append(x_i.clone())\n",
    "    return torch.stack(outs, dim=0)\n",
    "\n",
    "# optimizer\n",
    "#optimizer = torch.optim.SGD(model.parameters(), lr=1e-4, momentum=0.9)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-7)#, weight_decay=5e-4)\n",
    "#optimizer = torch.optim.Adadelta(model.parameters())#, rho=0.95)#, lr=1e-1, momentum=0.9)\n",
    "\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min', factor=0.5, patience=5, verbose=True)\n",
    "\n",
    "val_loss_log = []\n",
    "\n",
    "val_loss_min = np.Inf\n",
    "\n",
    "# learnig loop\n",
    "for epoch in range(50):\n",
    "    model.train()\n",
    "    for batch_x, batch_y, batch_ct in train_data:\n",
    "        optimizer.zero_grad()\n",
    "        lossv = 0\n",
    "        losstheta = 0\n",
    "        for ib in range(batch_x.size(0)):\n",
    "            model.load_celltypes(batch_ct[ib].to(device))\n",
    "            out = calc_multiSteps(batch_x[ib].to(device))\n",
    "            lv, ltheta = myLoss(out, batch_y[ib].to(device))\n",
    "            lossv = lossv + lv\n",
    "            losstheta = losstheta + ltheta\n",
    "        lossv = lossv / batch_x.size(0)\n",
    "        losstheta = losstheta / batch_x.size(0)\n",
    "        (lossv+losstheta).backward()\n",
    "        optimizer.step()\n",
    "    model.eval()\n",
    "    val_lossv = 0\n",
    "    val_losstheta = 0\n",
    "    val_count = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_y, batch_ct in valid_data:\n",
    "            for ib in range(batch_x.size(0)):\n",
    "                model.load_celltypes(batch_ct[ib].to(device))\n",
    "                val_out = calc_multiSteps(batch_x[ib].to(device))\n",
    "                lv, ltheta = myLoss(val_out, batch_y[ib].to(device))\n",
    "                val_lossv = val_lossv + lv\n",
    "                val_losstheta = val_losstheta + ltheta\n",
    "            val_count = val_count + batch_x.size(0)\n",
    "    val_lossv = val_lossv/val_count\n",
    "    val_losstheta = val_losstheta/val_count\n",
    "    val_loss = val_lossv + val_losstheta\n",
    "    scheduler.step(val_loss)\n",
    "    print('Epoch %d | train Loss: [%.4f, %.4f] | valid Loss: [%.4f, %.4f]' % (epoch,\n",
    "                                                                              lossv.item(), \n",
    "                                                                              losstheta.item(),\n",
    "                                                                              val_lossv.item(), \n",
    "                                                                              val_losstheta.item()))\n",
    "    val_loss_log.append([val_lossv.cpu().item(), val_losstheta.cpu().item()])\n",
    "    if val_loss.item() < val_loss_min:\n",
    "        stored_model = model\n",
    "        val_loss_min = val_loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605ebb18",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-27T06:52:02.443552Z",
     "start_time": "2022-10-27T06:52:02.443536Z"
    }
   },
   "outputs": [],
   "source": [
    "print(batch_x.shape)\n",
    "print(batch_y.shape)\n",
    "print(batch_ct.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3cf1e91",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-27T06:52:02.445085Z",
     "start_time": "2022-10-27T06:52:02.445071Z"
    }
   },
   "outputs": [],
   "source": [
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44265599",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-27T06:52:02.446404Z",
     "start_time": "2022-10-27T06:52:02.446390Z"
    }
   },
   "outputs": [],
   "source": [
    "# モデルを評価モードに設定\n",
    "stored_model.eval()\n",
    "\n",
    "# 推論\n",
    "test_lossv = 0\n",
    "test_losstheta = 0\n",
    "test_count = 0\n",
    "with torch.no_grad():\n",
    "    for batch_x, batch_y, batch_ct in test_data:\n",
    "        for ib in range(batch_x.size(0)):\n",
    "            model.load_celltypes(batch_ct[ib].to(device))\n",
    "            test_out = calc_multiSteps(batch_x[ib].to(device))\n",
    "            lv, ltheta = myLoss(test_out, batch_y[ib].to(device))\n",
    "            test_lossv = test_lossv + lv\n",
    "            test_losstheta = test_losstheta + ltheta\n",
    "        test_count = test_count + batch_x.size(0)\n",
    "test_lossv = test_lossv/test_count\n",
    "test_losstheta = test_losstheta/test_count\n",
    "print('test Loss: [%.4f, %.4f]' % (test_lossv.item(), test_losstheta.item()))\n",
    "test_loss = [test_lossv.item(), test_losstheta.item()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0283033",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-27T06:52:02.447512Z",
     "start_time": "2022-10-27T06:52:02.447499Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import datetime\n",
    "\n",
    "now = datetime.datetime.now()\n",
    "nowstr = now.strftime('%Y%m%d_%H%M%S')\n",
    "\n",
    "os.makedirs(savedirName + nowstr + '/', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445915ef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-27T06:52:02.451067Z",
     "start_time": "2022-10-27T06:52:02.451051Z"
    }
   },
   "outputs": [],
   "source": [
    "stored_model.selfpropel.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62154d31",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-27T06:52:02.452342Z",
     "start_time": "2022-10-27T06:52:02.452327Z"
    }
   },
   "outputs": [],
   "source": [
    "stored_model = stored_model.to('cpu')\n",
    "\n",
    "filename1 = savedirName + nowstr + '/' + nowstr + '_Model.pkl'\n",
    "with open(filename1, \"wb\") as f:\n",
    "    pickle.dump(stored_model, f)\n",
    "\n",
    "filename1_2 = savedirName + nowstr + '/' + nowstr + '_Model.pt'\n",
    "torch.save(stored_model, filename1_2)\n",
    "\n",
    "filename2 = savedirName + nowstr + '/' + nowstr\n",
    "torch.save(stored_model.interactNN.state_dict(), filename2 + '_interactNN.pkl')\n",
    "torch.save(stored_model.thetaDotNN.state_dict(), filename2 + '_thetaDotNN.pkl')\n",
    "torch.save(stored_model.selfpropel.detach(), filename2 + '_selfpropel.pkl')\n",
    "\n",
    "filename3 = savedirName + nowstr + '/' + nowstr + '_Separation.npz'\n",
    "np.savez(filename3, dr_thresh=dr_thresh, T_pred=T_pred, batch_size=batch_size,\n",
    "         train_inds=train_inds, valid_inds=valid_inds, test_inds=test_inds, \n",
    "         val_loss_log=val_loss_log, test_loss=test_loss)\n",
    "\n",
    "filename4 = savedirName + nowstr + '/' + nowstr + '_optimizer.pt'\n",
    "torch.save(optimizer, filename4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0bd69a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 249.41666600000002,
   "position": {
    "height": "40px",
    "left": "554px",
    "right": "20px",
    "top": "120px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "none",
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
