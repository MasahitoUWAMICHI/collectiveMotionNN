{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f855b76a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-22T05:36:39.365907Z",
     "start_time": "2022-11-22T05:36:39.359541Z"
    }
   },
   "outputs": [],
   "source": [
    "#!pip install dgl-cu113 dglgo -f https://data.dgl.ai/wheels/repo.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7807020c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-22T05:36:39.375631Z",
     "start_time": "2022-11-22T05:36:39.371391Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['DGLBACKEND'] = 'pytorch'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d12568c6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-22T05:36:40.922066Z",
     "start_time": "2022-11-22T05:36:39.378774Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "import dgl\n",
    "import dgl.function as fn\n",
    "\n",
    "import networkx as nx\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import gc\n",
    "\n",
    "# デバイス設定\n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "61e1f3ee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-22T05:36:40.927498Z",
     "start_time": "2022-11-22T05:36:40.925006Z"
    }
   },
   "outputs": [],
   "source": [
    "def printNPZ(npz):\n",
    "    for kw in npz.files:\n",
    "        print(kw, npz[kw])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ba5ba52",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-22T05:36:40.940215Z",
     "start_time": "2022-11-22T05:36:40.929369Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v0 1.0\n",
      "r 1.0\n",
      "D 0.1\n",
      "A 0.0\n",
      "L 20\n",
      "rho 1.0\n",
      "beta 1.0\n",
      "A_CFs [0.9 0.1]\n",
      "A_CIL 0.0\n",
      "cellType_ratio [0.7 0.3]\n",
      "quiv_colors ['k' 'r']\n",
      "kappa 1.0\n",
      "A_Macdonalds [2. 2.]\n",
      "batch_size 400\n",
      "state_size 3\n",
      "brownian_size 1\n",
      "periodic True\n",
      "t_max 200\n",
      "methodSDE heun\n",
      "isIto False\n",
      "stepSDE 0.01\n"
     ]
    }
   ],
   "source": [
    "dirName = './HiraiwaModel_chem20221102_020319/'\n",
    "savedirName = dirName + 'ActiveNet_vp_rotsym_batchNorm/'\n",
    "os.makedirs(savedirName, exist_ok=True)\n",
    "\n",
    "params = np.load(dirName+'params.npz')\n",
    "#traj = np.load(dirName+'result.npz')\n",
    "\n",
    "printNPZ(params)\n",
    "#printNPZ(traj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c4e73591",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-22T05:36:40.947262Z",
     "start_time": "2022-11-22T05:36:40.941738Z"
    }
   },
   "outputs": [],
   "source": [
    "if params['periodic']:\n",
    "    L = torch.tensor(params['L'])\n",
    "    def calc_dr(r1, r2):\n",
    "        dr = torch.remainder((r1 - r2), L)\n",
    "        dr[dr > L/2] = dr[dr > L/2] - L\n",
    "        return dr\n",
    "else:\n",
    "    def calc_dr(r1, r2):\n",
    "        return r1 - r2\n",
    "    \n",
    "def makeGraph(x_data, r_thresh):\n",
    "        Ndata = x_data.size(0)\n",
    "        dx = calc_dr(torch.unsqueeze(x_data, 0), torch.unsqueeze(x_data, 1))\n",
    "        dx = torch.sum(dx**2, dim=2)\n",
    "        edges = torch.argwhere(dx < r_thresh/2)\n",
    "        return dgl.graph((edges[:,0], edges[:,1]), num_nodes=Ndata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e3e01d8a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-22T05:36:40.953584Z",
     "start_time": "2022-11-22T05:36:40.949109Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['./HiraiwaModel_chem20221102_020319/20221102_194956', './HiraiwaModel_chem20221102_020319/20221102_203311', './HiraiwaModel_chem20221102_020319/20221102_211612', './HiraiwaModel_chem20221102_020319/20221102_215953', './HiraiwaModel_chem20221102_020319/20221102_224239', './HiraiwaModel_chem20221102_020319/20221102_232519', './HiraiwaModel_chem20221102_020319/20221103_000843', './HiraiwaModel_chem20221102_020319/20221103_005036', './HiraiwaModel_chem20221102_020319/20221103_013303', './HiraiwaModel_chem20221102_020319/20221103_021518', './HiraiwaModel_chem20221102_020319/20221103_025758', './HiraiwaModel_chem20221102_020319/20221103_033955', './HiraiwaModel_chem20221102_020319/20221103_042225', './HiraiwaModel_chem20221102_020319/20221103_050424', './HiraiwaModel_chem20221102_020319/20221103_054634', './HiraiwaModel_chem20221102_020319/20221103_062842', './HiraiwaModel_chem20221102_020319/20221103_071105', './HiraiwaModel_chem20221102_020319/20221103_075340', './HiraiwaModel_chem20221102_020319/20221103_083543', './HiraiwaModel_chem20221102_020319/20221103_091743', './HiraiwaModel_chem20221102_020319/20221103_100013', './HiraiwaModel_chem20221102_020319/20221103_104313', './HiraiwaModel_chem20221102_020319/20221103_112705', './HiraiwaModel_chem20221102_020319/20221103_121018', './HiraiwaModel_chem20221102_020319/20221103_125339', './HiraiwaModel_chem20221102_020319/ActiveNet_vp_rotsym_batchNorm']\n",
      "['./HiraiwaModel_chem20221102_020319/20221102_194956', './HiraiwaModel_chem20221102_020319/20221102_203311', './HiraiwaModel_chem20221102_020319/20221102_211612', './HiraiwaModel_chem20221102_020319/20221102_215953', './HiraiwaModel_chem20221102_020319/20221102_224239', './HiraiwaModel_chem20221102_020319/20221102_232519', './HiraiwaModel_chem20221102_020319/20221103_000843', './HiraiwaModel_chem20221102_020319/20221103_005036', './HiraiwaModel_chem20221102_020319/20221103_013303', './HiraiwaModel_chem20221102_020319/20221103_021518', './HiraiwaModel_chem20221102_020319/20221103_025758', './HiraiwaModel_chem20221102_020319/20221103_033955', './HiraiwaModel_chem20221102_020319/20221103_042225', './HiraiwaModel_chem20221102_020319/20221103_050424', './HiraiwaModel_chem20221102_020319/20221103_054634', './HiraiwaModel_chem20221102_020319/20221103_062842', './HiraiwaModel_chem20221102_020319/20221103_071105', './HiraiwaModel_chem20221102_020319/20221103_075340', './HiraiwaModel_chem20221102_020319/20221103_083543', './HiraiwaModel_chem20221102_020319/20221103_091743', './HiraiwaModel_chem20221102_020319/20221103_100013', './HiraiwaModel_chem20221102_020319/20221103_104313', './HiraiwaModel_chem20221102_020319/20221103_112705', './HiraiwaModel_chem20221102_020319/20221103_121018', './HiraiwaModel_chem20221102_020319/20221103_125339']\n"
     ]
    }
   ],
   "source": [
    "subdir_list = [f.path for f in os.scandir(dirName) if f.is_dir()]\n",
    "\n",
    "print(subdir_list)\n",
    "\n",
    "datadir_list = [f for f in subdir_list if 'result.npz' in [ff.name for ff in os.scandir(f)]]\n",
    "\n",
    "print(datadir_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e9e2f01",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-22T05:36:41.494433Z",
     "start_time": "2022-11-22T05:36:40.955253Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "242"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dr_thresh = 4\n",
    "dt = 1\n",
    "batch_size = 8\n",
    "\n",
    "N_data = len(datadir_list)\n",
    "\n",
    "#TR_VA_rate = np.array([0.6, 0.2])\n",
    "\n",
    "TR_last = 20\n",
    "VA_last = 23\n",
    "\n",
    "shuffle_inds = np.arange(N_data, dtype=int)\n",
    "np.random.shuffle(shuffle_inds)\n",
    "\n",
    "train_inds = shuffle_inds[:TR_last]\n",
    "valid_inds = shuffle_inds[TR_last:VA_last]\n",
    "test_inds = shuffle_inds[VA_last:]\n",
    "\n",
    "celltype_lst = []\n",
    "\n",
    "train_x = []\n",
    "valid_x = []\n",
    "test_x = []\n",
    "\n",
    "train_y = []\n",
    "valid_y = []\n",
    "test_y = []\n",
    "\n",
    "train_i_dir = []\n",
    "valid_i_dir = []\n",
    "test_i_dir = []\n",
    "\n",
    "for i_dir, subdirName in enumerate(datadir_list):\n",
    "    \n",
    "    traj = np.load(subdirName+'/result.npz')\n",
    "    \n",
    "    celltype_lst.append(torch.tensor(traj['celltype_label']).view(-1,1))\n",
    "\n",
    "    xy_t = torch.tensor(traj['xy'][:-1,:,:])\n",
    "    v_t = calc_dr(torch.tensor(traj['xy'][1:,:,:]), torch.tensor(traj['xy'][:-1,:,:])) / dt\n",
    "    p_t = torch.unsqueeze(torch.tensor(traj['theta'][:-1,:]), dim=2)\n",
    "    w_t = torch.unsqueeze(torch.tensor((traj['theta'][1:,:]-traj['theta'][:-1,:])%(2*np.pi)/dt), dim=2)\n",
    "    \n",
    "    if i_dir in train_inds:\n",
    "        train_x.append(torch.concat((xy_t, p_t), -1))\n",
    "        train_y.append(torch.concat((v_t, w_t), -1))\n",
    "        train_i_dir.append(torch.ones([xy_t.size(0)])*i_dir)\n",
    "\n",
    "    if i_dir in valid_inds:\n",
    "        valid_x.append(torch.concat((xy_t, p_t), -1))\n",
    "        valid_y.append(torch.concat((v_t, w_t), -1))\n",
    "        valid_i_dir.append(torch.ones([xy_t.size(0)])*i_dir)\n",
    "        \n",
    "    if i_dir in test_inds:\n",
    "        test_x.append(torch.concat((xy_t, p_t), -1))\n",
    "        test_y.append(torch.concat((v_t, w_t), -1))\n",
    "        test_i_dir.append(torch.ones([xy_t.size(0)])*i_dir)\n",
    "    \n",
    "train_dataset = torch.utils.data.TensorDataset(\n",
    "    torch.concat(train_x, 0), \n",
    "    torch.concat(train_y, 0), \n",
    "    torch.concat(train_i_dir, 0))\n",
    "\n",
    "valid_dataset = torch.utils.data.TensorDataset(\n",
    "    torch.concat(valid_x, 0), \n",
    "    torch.concat(valid_y, 0), \n",
    "    torch.concat(valid_i_dir, 0))\n",
    "\n",
    "test_dataset = torch.utils.data.TensorDataset(\n",
    "    torch.concat(test_x, 0), \n",
    "    torch.concat(test_y, 0), \n",
    "    torch.concat(test_i_dir, 0))\n",
    "\n",
    "train_data = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, pin_memory=True)\n",
    "valid_data = torch.utils.data.DataLoader(valid_dataset, batch_size=batch_size, pin_memory=True)\n",
    "test_data = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, pin_memory=True)\n",
    "\n",
    "del train_x, train_y, train_i_dir, train_dataset\n",
    "del valid_x, valid_y, valid_i_dir, valid_dataset\n",
    "del test_x, test_y, test_i_dir, test_dataset\n",
    "gc.collect()\n",
    "\n",
    "#print(data)\n",
    "#print(data.num_graphs)\n",
    "#print(data.x)\n",
    "#print(data.y)\n",
    "#print(data.edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1c0a633b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-22T05:36:41.501727Z",
     "start_time": "2022-11-22T05:36:41.498254Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 4 10 19 12 22 20  6 16  0 23 11 24 15 13 21  8  9  2  5 17]\n"
     ]
    }
   ],
   "source": [
    "print(train_inds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4f35452f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-22T05:36:41.507653Z",
     "start_time": "2022-11-22T05:36:41.503541Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_DataLoader__initialized',\n",
       " '_DataLoader__multiprocessing_context',\n",
       " '_IterableDataset_len_called',\n",
       " '__annotations__',\n",
       " '__class__',\n",
       " '__class_getitem__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__iter__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__orig_bases__',\n",
       " '__parameters__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__slots__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_auto_collation',\n",
       " '_dataset_kind',\n",
       " '_get_iterator',\n",
       " '_get_shared_seed',\n",
       " '_index_sampler',\n",
       " '_is_protocol',\n",
       " '_iterator',\n",
       " 'batch_sampler',\n",
       " 'batch_size',\n",
       " 'check_worker_number_rationality',\n",
       " 'collate_fn',\n",
       " 'dataset',\n",
       " 'drop_last',\n",
       " 'generator',\n",
       " 'multiprocessing_context',\n",
       " 'num_workers',\n",
       " 'persistent_workers',\n",
       " 'pin_memory',\n",
       " 'pin_memory_device',\n",
       " 'prefetch_factor',\n",
       " 'sampler',\n",
       " 'timeout',\n",
       " 'worker_init_fn']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f94ee7f4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-22T05:36:41.514553Z",
     "start_time": "2022-11-22T05:36:41.509369Z"
    }
   },
   "outputs": [],
   "source": [
    "def plotGraph(data):\n",
    "\n",
    "    # networkxのグラフに変換\n",
    "    nxg = dgl.to_networkx(data)\n",
    "\n",
    "    # 可視化のためのページランク計算\n",
    "    pr = nx.pagerank(nxg)\n",
    "    pr_max = np.array(list(pr.values())).max()\n",
    "\n",
    "    # 可視化する際のノード位置\n",
    "    draw_pos = nx.spring_layout(nxg, seed=0) \n",
    "\n",
    "    # ノードの色設定\n",
    "    cmap = plt.get_cmap('tab10')\n",
    "    labels = data.y.numpy()\n",
    "    colors = [cmap(l) for l in labels]\n",
    "\n",
    "    # 図のサイズ\n",
    "    fig0 = plt.figure(figsize=(10, 10))\n",
    "\n",
    "    # 描画\n",
    "    nx.draw_networkx_nodes(nxg, \n",
    "                          draw_pos,\n",
    "                          node_size=[v / pr_max * 1000 for v in pr.values()])#,\n",
    "                          #node_color=colors, alpha=0.5)\n",
    "    nx.draw_networkx_edges(nxg, draw_pos, arrowstyle='-', alpha=0.2)\n",
    "    nx.draw_networkx_labels(nxg, draw_pos, font_size=10)\n",
    "\n",
    "    #plt.title('KarateClub')\n",
    "    plt.show()\n",
    "\n",
    "    return fig0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "21e97796",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-22T05:36:41.539816Z",
     "start_time": "2022-11-22T05:36:41.521702Z"
    }
   },
   "outputs": [],
   "source": [
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, Nchannels, dropout=0, batchN=False, flgBias=False):\n",
    "        super(NeuralNet, self).__init__()\n",
    "\n",
    "        if dropout:\n",
    "            self.dropout = nn.Dropout(p=dropout)\n",
    "        else:\n",
    "            self.dropout = 0\n",
    "            \n",
    "        if batchN:\n",
    "            self.bNorm1 = nn.BatchNorm1d(Nchannels)\n",
    "            self.bNorm2 = nn.BatchNorm1d(Nchannels)\n",
    "            self.bNorm3 = nn.BatchNorm1d(Nchannels)\n",
    "            \n",
    "        self.batchN=batchN\n",
    "        \n",
    "        self.layer1 = nn.Linear(in_channels, Nchannels, bias=flgBias)\n",
    "        self.layer2 = nn.Linear(Nchannels, Nchannels, bias=flgBias)\n",
    "        self.layer3 = nn.Linear(Nchannels, Nchannels, bias=flgBias)\n",
    "        self.layer4 = nn.Linear(Nchannels, out_channels, bias=flgBias)\n",
    "\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.layer1.reset_parameters()\n",
    "        self.layer2.reset_parameters()\n",
    "        self.layer3.reset_parameters()\n",
    "        self.layer4.reset_parameters()\n",
    "        #nn.init.zeros_(self.layer1.weight)\n",
    "        #nn.init.zeros_(self.layer2.weight)\n",
    "        #nn.init.zeros_(self.layer3.weight)\n",
    "        #nn.init.zeros_(self.layer4.weight)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.activation(self.layer1(x))\n",
    "        if self.batchN:\n",
    "            out = self.bNorm1(out)\n",
    "        if self.dropout:\n",
    "            out = self.dropout(out)\n",
    "        \n",
    "        out = self.activation(self.layer2(out))\n",
    "        if self.batchN:\n",
    "            out = self.bNorm2(out)\n",
    "        if self.dropout:\n",
    "            out = self.dropout(out)\n",
    "        \n",
    "        out = self.activation(self.layer3(out))\n",
    "        if self.batchN:\n",
    "            out = self.bNorm3(out)\n",
    "        if self.dropout:\n",
    "            out = self.dropout(out)\n",
    "        \n",
    "        out = self.layer4(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "class ActiveNet(nn.Module):\n",
    "    def __init__(self, xy_dim, r, dropout=0, batchN=False, bias=False, Nchannels=128):\n",
    "        super().__init__()\n",
    "\n",
    "        self.interactNN = NeuralNet(xy_dim*2 + 2, xy_dim, Nchannels, dropout, batchN, bias)\n",
    "\n",
    "        self.thetaDotNN = NeuralNet(xy_dim*2 + 2, 1, Nchannels, dropout, batchN, bias)\n",
    "        \n",
    "        self.selfpropel = nn.Parameter(torch.tensor(0.0, requires_grad=True, device=device))\n",
    "\n",
    "        #self.Normalizer = nn.Softmax(dim=1)\n",
    "\n",
    "        self.xy_dim = xy_dim\n",
    "        \n",
    "        self.r = r\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.interactNN.reset_parameters()\n",
    "\n",
    "        self.thetaDotNN.reset_parameters()\n",
    "        \n",
    "        nn.init.uniform_(self.selfpropel)\n",
    "\n",
    "        #self.bias.data.zero_()\n",
    "        \n",
    "    def load_celltypes(self, celltype):\n",
    "        self.celltype = celltype\n",
    "\n",
    "    def calc_message(self, edges):\n",
    "        dx = calc_dr(edges.dst['x'], edges.src['x'])\n",
    "\n",
    "        costheta = torch.cos(edges.dst['theta'])\n",
    "        sintheta = torch.sin(edges.dst['theta'])\n",
    "\n",
    "        dx_para = costheta * dx[..., :1] + sintheta * dx[..., 1:]\n",
    "        dx_perp = costheta * dx[..., 1:] - sintheta * dx[..., :1]\n",
    "\n",
    "        p_para_src = torch.cos(edges.src['theta'] - edges.dst['theta'])\n",
    "        p_perp_src = torch.sin(edges.src['theta'] - edges.dst['theta'])\n",
    "\n",
    "        rot_m_v = self.interactNN(torch.concat((dx_para, dx_perp, \n",
    "                                                p_para_src, p_perp_src,\n",
    "                                                edges.dst['type'], edges.src['type']), -1))\n",
    "\n",
    "        m_v = torch.concat((costheta * rot_m_v[..., :1] - sintheta * rot_m_v[..., 1:],\n",
    "                            costheta * rot_m_v[..., 1:] + sintheta * rot_m_v[..., :1]), -1)\n",
    "\n",
    "        m_theta = self.thetaDotNN(torch.concat((dx_para, dx_perp, \n",
    "                                                p_para_src, p_perp_src, \n",
    "                                                edges.dst['type'], edges.src['type']), -1))\n",
    "        \n",
    "        return {'m': torch.concat((m_v, m_theta), -1)}\n",
    "        \n",
    "    def forward(self, xv):\n",
    "        r_g = makeGraph(xv[..., :self.xy_dim], self.r/2)\n",
    "        r_g.ndata['x'] = xv[..., :self.xy_dim]\n",
    "        r_g.ndata['theta'] = xv[..., self.xy_dim:(self.xy_dim+1)]\n",
    "        r_g.ndata['type'] = self.celltype\n",
    "        r_g.update_all(self.calc_message, fn.sum('m', 'a'))\n",
    "        r_g.ndata['a'][..., :self.xy_dim] = r_g.ndata['a'][..., :self.xy_dim] + self.selfpropel * torch.concat((torch.cos(r_g.ndata['theta']), torch.sin(r_g.ndata['theta'])), -1)\n",
    "        \n",
    "        return r_g.ndata['a']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b52c1e60",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-22T05:36:41.544900Z",
     "start_time": "2022-11-22T05:36:41.541548Z"
    }
   },
   "outputs": [],
   "source": [
    "def myLoss(out, target):\n",
    "    dv = torch.sum(torch.square(out[..., :xy_dim] - target[..., :xy_dim]), dim=-1)\n",
    "    dcos = torch.cos(out[..., xy_dim] - target[..., xy_dim])\n",
    "    return torch.mean(dv), 1 - torch.mean(dcos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6033283c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-22T08:28:09.909374Z",
     "start_time": "2022-11-22T05:36:41.551720Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 | train Loss: [0.0884, 0.0648] | valid Loss: [0.0998, 0.0771]\n",
      "Epoch 1 | train Loss: [0.0790, 0.0637] | valid Loss: [0.0897, 0.0751]\n",
      "Epoch 2 | train Loss: [0.0759, 0.0632] | valid Loss: [0.0875, 0.0742]\n",
      "Epoch 3 | train Loss: [0.0730, 0.0628] | valid Loss: [0.0845, 0.0737]\n",
      "Epoch 4 | train Loss: [0.0694, 0.0625] | valid Loss: [0.0805, 0.0732]\n",
      "Epoch 5 | train Loss: [0.0671, 0.0622] | valid Loss: [0.0782, 0.0728]\n",
      "Epoch 6 | train Loss: [0.0645, 0.0620] | valid Loss: [0.0752, 0.0725]\n",
      "Epoch 7 | train Loss: [0.0631, 0.0618] | valid Loss: [0.0740, 0.0722]\n",
      "Epoch 8 | train Loss: [0.0614, 0.0615] | valid Loss: [0.0723, 0.0720]\n",
      "Epoch 9 | train Loss: [0.0599, 0.0614] | valid Loss: [0.0706, 0.0718]\n",
      "Epoch 10 | train Loss: [0.0593, 0.0612] | valid Loss: [0.0702, 0.0716]\n",
      "Epoch 11 | train Loss: [0.0581, 0.0610] | valid Loss: [0.0689, 0.0713]\n",
      "Epoch 12 | train Loss: [0.0574, 0.0609] | valid Loss: [0.0684, 0.0711]\n",
      "Epoch 13 | train Loss: [0.0560, 0.0607] | valid Loss: [0.0667, 0.0709]\n",
      "Epoch 14 | train Loss: [0.0560, 0.0606] | valid Loss: [0.0668, 0.0707]\n",
      "Epoch 15 | train Loss: [0.0562, 0.0604] | valid Loss: [0.0669, 0.0705]\n",
      "Epoch 16 | train Loss: [0.0566, 0.0603] | valid Loss: [0.0681, 0.0703]\n",
      "Epoch 17 | train Loss: [0.0550, 0.0602] | valid Loss: [0.0657, 0.0701]\n",
      "Epoch 18 | train Loss: [0.0561, 0.0601] | valid Loss: [0.0682, 0.0701]\n",
      "Epoch 19 | train Loss: [0.0528, 0.0601] | valid Loss: [0.0630, 0.0699]\n",
      "Epoch 20 | train Loss: [0.0546, 0.0600] | valid Loss: [0.0655, 0.0698]\n",
      "Epoch 21 | train Loss: [0.0525, 0.0599] | valid Loss: [0.0625, 0.0697]\n",
      "Epoch 22 | train Loss: [0.0533, 0.0599] | valid Loss: [0.0631, 0.0695]\n",
      "Epoch 23 | train Loss: [0.0523, 0.0598] | valid Loss: [0.0619, 0.0694]\n",
      "Epoch 24 | train Loss: [0.0523, 0.0597] | valid Loss: [0.0615, 0.0693]\n",
      "Epoch 25 | train Loss: [0.0517, 0.0597] | valid Loss: [0.0611, 0.0692]\n",
      "Epoch 26 | train Loss: [0.0526, 0.0596] | valid Loss: [0.0625, 0.0691]\n",
      "Epoch 27 | train Loss: [0.0591, 0.0596] | valid Loss: [0.0721, 0.0690]\n",
      "Epoch 28 | train Loss: [0.0514, 0.0595] | valid Loss: [0.0611, 0.0689]\n",
      "Epoch 29 | train Loss: [0.0530, 0.0595] | valid Loss: [0.0653, 0.0688]\n",
      "Epoch 30 | train Loss: [0.0536, 0.0595] | valid Loss: [0.0633, 0.0687]\n",
      "Epoch 31 | train Loss: [0.0527, 0.0594] | valid Loss: [0.0621, 0.0686]\n",
      "Epoch 32 | train Loss: [0.0515, 0.0594] | valid Loss: [0.0618, 0.0686]\n",
      "Epoch 33 | train Loss: [0.0500, 0.0593] | valid Loss: [0.0603, 0.0685]\n",
      "Epoch 34 | train Loss: [0.0512, 0.0593] | valid Loss: [0.0623, 0.0685]\n",
      "Epoch 35 | train Loss: [0.0517, 0.0593] | valid Loss: [0.0628, 0.0684]\n",
      "Epoch 36 | train Loss: [0.0492, 0.0592] | valid Loss: [0.0590, 0.0683]\n",
      "Epoch 37 | train Loss: [0.0496, 0.0592] | valid Loss: [0.0596, 0.0683]\n",
      "Epoch 38 | train Loss: [0.0489, 0.0592] | valid Loss: [0.0594, 0.0682]\n",
      "Epoch 39 | train Loss: [0.0503, 0.0591] | valid Loss: [0.0611, 0.0682]\n",
      "Epoch 40 | train Loss: [0.0476, 0.0591] | valid Loss: [0.0575, 0.0681]\n",
      "Epoch 41 | train Loss: [0.0493, 0.0591] | valid Loss: [0.0594, 0.0681]\n",
      "Epoch 42 | train Loss: [0.0484, 0.0590] | valid Loss: [0.0587, 0.0680]\n",
      "Epoch 43 | train Loss: [0.0490, 0.0590] | valid Loss: [0.0596, 0.0680]\n",
      "Epoch 44 | train Loss: [0.0481, 0.0590] | valid Loss: [0.0583, 0.0680]\n",
      "Epoch 45 | train Loss: [0.0476, 0.0590] | valid Loss: [0.0579, 0.0679]\n",
      "Epoch 00047: reducing learning rate of group 0 to 5.0000e-01.\n",
      "Epoch 46 | train Loss: [0.0750, 0.0590] | valid Loss: [0.0865, 0.0679]\n",
      "Epoch 47 | train Loss: [0.0452, 0.0584] | valid Loss: [0.0554, 0.0671]\n",
      "Epoch 48 | train Loss: [0.0447, 0.0582] | valid Loss: [0.0548, 0.0669]\n",
      "Epoch 49 | train Loss: [0.0448, 0.0581] | valid Loss: [0.0550, 0.0668]\n",
      "Epoch 50 | train Loss: [0.0442, 0.0580] | valid Loss: [0.0544, 0.0667]\n",
      "Epoch 51 | train Loss: [0.0456, 0.0580] | valid Loss: [0.0561, 0.0667]\n",
      "Epoch 52 | train Loss: [0.0436, 0.0580] | valid Loss: [0.0539, 0.0666]\n",
      "Epoch 53 | train Loss: [0.0439, 0.0579] | valid Loss: [0.0544, 0.0665]\n",
      "Epoch 54 | train Loss: [0.0437, 0.0579] | valid Loss: [0.0542, 0.0665]\n",
      "Epoch 55 | train Loss: [0.0439, 0.0578] | valid Loss: [0.0545, 0.0665]\n",
      "Epoch 56 | train Loss: [0.0435, 0.0578] | valid Loss: [0.0540, 0.0664]\n",
      "Epoch 57 | train Loss: [0.0437, 0.0578] | valid Loss: [0.0543, 0.0664]\n",
      "Epoch 58 | train Loss: [0.0434, 0.0578] | valid Loss: [0.0539, 0.0664]\n",
      "Epoch 59 | train Loss: [0.0434, 0.0578] | valid Loss: [0.0540, 0.0663]\n",
      "Epoch 60 | train Loss: [0.0433, 0.0578] | valid Loss: [0.0538, 0.0663]\n",
      "Epoch 61 | train Loss: [0.0440, 0.0578] | valid Loss: [0.0549, 0.0663]\n",
      "Epoch 62 | train Loss: [0.0438, 0.0578] | valid Loss: [0.0546, 0.0663]\n",
      "Epoch 63 | train Loss: [0.0429, 0.0578] | valid Loss: [0.0534, 0.0662]\n",
      "Epoch 64 | train Loss: [0.0440, 0.0577] | valid Loss: [0.0549, 0.0662]\n",
      "Epoch 65 | train Loss: [0.0437, 0.0577] | valid Loss: [0.0546, 0.0662]\n",
      "Epoch 66 | train Loss: [0.0428, 0.0577] | valid Loss: [0.0534, 0.0662]\n",
      "Epoch 67 | train Loss: [0.0431, 0.0577] | valid Loss: [0.0538, 0.0662]\n",
      "Epoch 68 | train Loss: [0.0431, 0.0577] | valid Loss: [0.0538, 0.0661]\n",
      "Epoch 69 | train Loss: [0.0428, 0.0577] | valid Loss: [0.0535, 0.0661]\n",
      "Epoch 70 | train Loss: [0.0426, 0.0577] | valid Loss: [0.0532, 0.0661]\n",
      "Epoch 71 | train Loss: [0.0426, 0.0577] | valid Loss: [0.0532, 0.0661]\n",
      "Epoch 72 | train Loss: [0.0423, 0.0577] | valid Loss: [0.0528, 0.0661]\n",
      "Epoch 73 | train Loss: [0.0424, 0.0577] | valid Loss: [0.0530, 0.0661]\n",
      "Epoch 74 | train Loss: [0.0425, 0.0576] | valid Loss: [0.0531, 0.0661]\n",
      "Epoch 75 | train Loss: [0.0422, 0.0576] | valid Loss: [0.0528, 0.0660]\n",
      "Epoch 76 | train Loss: [0.0422, 0.0577] | valid Loss: [0.0527, 0.0660]\n",
      "Epoch 77 | train Loss: [0.0421, 0.0577] | valid Loss: [0.0526, 0.0660]\n",
      "Epoch 78 | train Loss: [0.0421, 0.0577] | valid Loss: [0.0527, 0.0660]\n",
      "Epoch 79 | train Loss: [0.0420, 0.0577] | valid Loss: [0.0526, 0.0660]\n",
      "Epoch 80 | train Loss: [0.0420, 0.0577] | valid Loss: [0.0526, 0.0660]\n",
      "Epoch 81 | train Loss: [0.0419, 0.0576] | valid Loss: [0.0524, 0.0660]\n",
      "Epoch 82 | train Loss: [0.0419, 0.0576] | valid Loss: [0.0525, 0.0660]\n",
      "Epoch 83 | train Loss: [0.0418, 0.0576] | valid Loss: [0.0524, 0.0659]\n",
      "Epoch 84 | train Loss: [0.0419, 0.0576] | valid Loss: [0.0525, 0.0659]\n",
      "Epoch 85 | train Loss: [0.0418, 0.0576] | valid Loss: [0.0524, 0.0659]\n",
      "Epoch 86 | train Loss: [0.0417, 0.0576] | valid Loss: [0.0523, 0.0659]\n",
      "Epoch 87 | train Loss: [0.0417, 0.0576] | valid Loss: [0.0522, 0.0659]\n",
      "Epoch 88 | train Loss: [0.0417, 0.0576] | valid Loss: [0.0523, 0.0659]\n",
      "Epoch 89 | train Loss: [0.0416, 0.0576] | valid Loss: [0.0521, 0.0659]\n",
      "Epoch 90 | train Loss: [0.0416, 0.0576] | valid Loss: [0.0521, 0.0659]\n",
      "Epoch 91 | train Loss: [0.0415, 0.0576] | valid Loss: [0.0521, 0.0659]\n",
      "Epoch 92 | train Loss: [0.0415, 0.0575] | valid Loss: [0.0521, 0.0658]\n",
      "Epoch 93 | train Loss: [0.0415, 0.0575] | valid Loss: [0.0520, 0.0658]\n",
      "Epoch 94 | train Loss: [0.0415, 0.0575] | valid Loss: [0.0521, 0.0658]\n",
      "Epoch 95 | train Loss: [0.0415, 0.0575] | valid Loss: [0.0520, 0.0658]\n",
      "Epoch 96 | train Loss: [0.0414, 0.0575] | valid Loss: [0.0519, 0.0658]\n",
      "Epoch 97 | train Loss: [0.0414, 0.0575] | valid Loss: [0.0519, 0.0658]\n",
      "Epoch 98 | train Loss: [0.0413, 0.0575] | valid Loss: [0.0519, 0.0658]\n",
      "Epoch 99 | train Loss: [0.0413, 0.0575] | valid Loss: [0.0519, 0.0658]\n",
      "Epoch 100 | train Loss: [0.0414, 0.0575] | valid Loss: [0.0520, 0.0658]\n",
      "Epoch 101 | train Loss: [0.0414, 0.0575] | valid Loss: [0.0520, 0.0658]\n",
      "Epoch 102 | train Loss: [0.0414, 0.0574] | valid Loss: [0.0520, 0.0658]\n",
      "Epoch 103 | train Loss: [0.0414, 0.0574] | valid Loss: [0.0520, 0.0658]\n",
      "Epoch 00105: reducing learning rate of group 0 to 2.5000e-01.\n",
      "Epoch 104 | train Loss: [0.0414, 0.0574] | valid Loss: [0.0521, 0.0658]\n",
      "Epoch 105 | train Loss: [0.0407, 0.0573] | valid Loss: [0.0513, 0.0654]\n",
      "Epoch 106 | train Loss: [0.0406, 0.0572] | valid Loss: [0.0512, 0.0654]\n",
      "Epoch 107 | train Loss: [0.0405, 0.0572] | valid Loss: [0.0512, 0.0654]\n",
      "Epoch 108 | train Loss: [0.0405, 0.0572] | valid Loss: [0.0511, 0.0653]\n",
      "Epoch 109 | train Loss: [0.0404, 0.0572] | valid Loss: [0.0511, 0.0653]\n",
      "Epoch 110 | train Loss: [0.0404, 0.0571] | valid Loss: [0.0511, 0.0653]\n",
      "Epoch 111 | train Loss: [0.0403, 0.0571] | valid Loss: [0.0511, 0.0653]\n",
      "Epoch 112 | train Loss: [0.0403, 0.0571] | valid Loss: [0.0510, 0.0653]\n",
      "Epoch 113 | train Loss: [0.0403, 0.0571] | valid Loss: [0.0510, 0.0653]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 114 | train Loss: [0.0403, 0.0571] | valid Loss: [0.0510, 0.0652]\n",
      "Epoch 115 | train Loss: [0.0403, 0.0571] | valid Loss: [0.0510, 0.0652]\n",
      "Epoch 116 | train Loss: [0.0402, 0.0571] | valid Loss: [0.0510, 0.0652]\n",
      "Epoch 117 | train Loss: [0.0402, 0.0571] | valid Loss: [0.0510, 0.0652]\n",
      "Epoch 118 | train Loss: [0.0402, 0.0570] | valid Loss: [0.0509, 0.0652]\n",
      "Epoch 119 | train Loss: [0.0402, 0.0570] | valid Loss: [0.0509, 0.0652]\n",
      "Epoch 120 | train Loss: [0.0402, 0.0570] | valid Loss: [0.0509, 0.0652]\n",
      "Epoch 121 | train Loss: [0.0402, 0.0570] | valid Loss: [0.0509, 0.0652]\n",
      "Epoch 122 | train Loss: [0.0401, 0.0570] | valid Loss: [0.0509, 0.0652]\n",
      "Epoch 123 | train Loss: [0.0401, 0.0570] | valid Loss: [0.0509, 0.0652]\n",
      "Epoch 124 | train Loss: [0.0401, 0.0570] | valid Loss: [0.0509, 0.0652]\n",
      "Epoch 125 | train Loss: [0.0401, 0.0570] | valid Loss: [0.0509, 0.0652]\n",
      "Epoch 126 | train Loss: [0.0401, 0.0570] | valid Loss: [0.0509, 0.0652]\n",
      "Epoch 127 | train Loss: [0.0401, 0.0570] | valid Loss: [0.0509, 0.0652]\n",
      "Epoch 128 | train Loss: [0.0401, 0.0570] | valid Loss: [0.0509, 0.0651]\n",
      "Epoch 129 | train Loss: [0.0401, 0.0570] | valid Loss: [0.0509, 0.0651]\n",
      "Epoch 130 | train Loss: [0.0401, 0.0570] | valid Loss: [0.0509, 0.0651]\n",
      "Epoch 00132: reducing learning rate of group 0 to 1.2500e-01.\n",
      "Epoch 131 | train Loss: [0.0402, 0.0570] | valid Loss: [0.0509, 0.0651]\n",
      "Epoch 132 | train Loss: [0.0398, 0.0569] | valid Loss: [0.0505, 0.0650]\n",
      "Epoch 133 | train Loss: [0.0397, 0.0569] | valid Loss: [0.0505, 0.0650]\n",
      "Epoch 134 | train Loss: [0.0397, 0.0569] | valid Loss: [0.0505, 0.0649]\n",
      "Epoch 135 | train Loss: [0.0397, 0.0569] | valid Loss: [0.0505, 0.0649]\n",
      "Epoch 136 | train Loss: [0.0397, 0.0569] | valid Loss: [0.0505, 0.0649]\n",
      "Epoch 137 | train Loss: [0.0397, 0.0569] | valid Loss: [0.0505, 0.0649]\n",
      "Epoch 138 | train Loss: [0.0396, 0.0569] | valid Loss: [0.0505, 0.0649]\n",
      "Epoch 139 | train Loss: [0.0396, 0.0569] | valid Loss: [0.0504, 0.0649]\n",
      "Epoch 140 | train Loss: [0.0396, 0.0569] | valid Loss: [0.0504, 0.0649]\n",
      "Epoch 141 | train Loss: [0.0396, 0.0569] | valid Loss: [0.0504, 0.0649]\n",
      "Epoch 142 | train Loss: [0.0396, 0.0569] | valid Loss: [0.0504, 0.0649]\n",
      "Epoch 143 | train Loss: [0.0396, 0.0569] | valid Loss: [0.0504, 0.0649]\n",
      "Epoch 144 | train Loss: [0.0396, 0.0569] | valid Loss: [0.0504, 0.0649]\n",
      "Epoch 145 | train Loss: [0.0396, 0.0569] | valid Loss: [0.0504, 0.0649]\n",
      "Epoch 146 | train Loss: [0.0395, 0.0568] | valid Loss: [0.0504, 0.0649]\n",
      "Epoch 147 | train Loss: [0.0395, 0.0568] | valid Loss: [0.0504, 0.0649]\n",
      "Epoch 148 | train Loss: [0.0395, 0.0568] | valid Loss: [0.0504, 0.0649]\n",
      "Epoch 149 | train Loss: [0.0395, 0.0568] | valid Loss: [0.0504, 0.0649]\n",
      "Epoch 150 | train Loss: [0.0395, 0.0568] | valid Loss: [0.0504, 0.0649]\n",
      "Epoch 151 | train Loss: [0.0395, 0.0568] | valid Loss: [0.0503, 0.0649]\n",
      "Epoch 152 | train Loss: [0.0395, 0.0568] | valid Loss: [0.0503, 0.0649]\n",
      "Epoch 153 | train Loss: [0.0395, 0.0568] | valid Loss: [0.0503, 0.0649]\n",
      "Epoch 154 | train Loss: [0.0395, 0.0568] | valid Loss: [0.0503, 0.0649]\n",
      "Epoch 155 | train Loss: [0.0395, 0.0568] | valid Loss: [0.0503, 0.0649]\n",
      "Epoch 156 | train Loss: [0.0395, 0.0568] | valid Loss: [0.0503, 0.0649]\n",
      "Epoch 157 | train Loss: [0.0395, 0.0568] | valid Loss: [0.0503, 0.0649]\n",
      "Epoch 158 | train Loss: [0.0395, 0.0568] | valid Loss: [0.0503, 0.0649]\n",
      "Epoch 159 | train Loss: [0.0395, 0.0568] | valid Loss: [0.0503, 0.0649]\n",
      "Epoch 160 | train Loss: [0.0395, 0.0568] | valid Loss: [0.0503, 0.0648]\n",
      "Epoch 161 | train Loss: [0.0395, 0.0568] | valid Loss: [0.0503, 0.0648]\n",
      "Epoch 162 | train Loss: [0.0394, 0.0568] | valid Loss: [0.0503, 0.0648]\n",
      "Epoch 163 | train Loss: [0.0394, 0.0568] | valid Loss: [0.0503, 0.0648]\n",
      "Epoch 164 | train Loss: [0.0394, 0.0568] | valid Loss: [0.0503, 0.0648]\n",
      "Epoch 165 | train Loss: [0.0394, 0.0568] | valid Loss: [0.0503, 0.0648]\n",
      "Epoch 166 | train Loss: [0.0394, 0.0567] | valid Loss: [0.0503, 0.0648]\n",
      "Epoch 167 | train Loss: [0.0394, 0.0567] | valid Loss: [0.0503, 0.0648]\n",
      "Epoch 168 | train Loss: [0.0394, 0.0567] | valid Loss: [0.0502, 0.0648]\n",
      "Epoch 169 | train Loss: [0.0394, 0.0567] | valid Loss: [0.0502, 0.0648]\n",
      "Epoch 170 | train Loss: [0.0394, 0.0567] | valid Loss: [0.0502, 0.0648]\n",
      "Epoch 171 | train Loss: [0.0394, 0.0567] | valid Loss: [0.0502, 0.0648]\n",
      "Epoch 172 | train Loss: [0.0394, 0.0567] | valid Loss: [0.0502, 0.0648]\n",
      "Epoch 173 | train Loss: [0.0394, 0.0567] | valid Loss: [0.0502, 0.0648]\n",
      "Epoch 174 | train Loss: [0.0394, 0.0567] | valid Loss: [0.0502, 0.0648]\n",
      "Epoch 175 | train Loss: [0.0394, 0.0567] | valid Loss: [0.0502, 0.0648]\n",
      "Epoch 176 | train Loss: [0.0394, 0.0567] | valid Loss: [0.0502, 0.0648]\n",
      "Epoch 177 | train Loss: [0.0394, 0.0567] | valid Loss: [0.0502, 0.0648]\n",
      "Epoch 178 | train Loss: [0.0394, 0.0567] | valid Loss: [0.0502, 0.0648]\n",
      "Epoch 179 | train Loss: [0.0394, 0.0567] | valid Loss: [0.0502, 0.0648]\n",
      "Epoch 180 | train Loss: [0.0394, 0.0567] | valid Loss: [0.0502, 0.0648]\n",
      "Epoch 181 | train Loss: [0.0394, 0.0567] | valid Loss: [0.0502, 0.0648]\n",
      "Epoch 182 | train Loss: [0.0394, 0.0567] | valid Loss: [0.0502, 0.0648]\n",
      "Epoch 183 | train Loss: [0.0394, 0.0567] | valid Loss: [0.0502, 0.0648]\n",
      "Epoch 184 | train Loss: [0.0394, 0.0567] | valid Loss: [0.0502, 0.0648]\n",
      "Epoch 185 | train Loss: [0.0394, 0.0567] | valid Loss: [0.0502, 0.0648]\n",
      "Epoch 186 | train Loss: [0.0393, 0.0567] | valid Loss: [0.0502, 0.0648]\n",
      "Epoch 187 | train Loss: [0.0393, 0.0567] | valid Loss: [0.0502, 0.0648]\n",
      "Epoch 188 | train Loss: [0.0393, 0.0567] | valid Loss: [0.0502, 0.0648]\n",
      "Epoch 189 | train Loss: [0.0393, 0.0567] | valid Loss: [0.0502, 0.0648]\n",
      "Epoch 190 | train Loss: [0.0393, 0.0566] | valid Loss: [0.0502, 0.0648]\n",
      "Epoch 191 | train Loss: [0.0393, 0.0566] | valid Loss: [0.0501, 0.0648]\n",
      "Epoch 192 | train Loss: [0.0393, 0.0566] | valid Loss: [0.0501, 0.0648]\n",
      "Epoch 193 | train Loss: [0.0393, 0.0566] | valid Loss: [0.0501, 0.0648]\n",
      "Epoch 194 | train Loss: [0.0393, 0.0566] | valid Loss: [0.0501, 0.0648]\n",
      "Epoch 195 | train Loss: [0.0393, 0.0566] | valid Loss: [0.0501, 0.0648]\n",
      "Epoch 196 | train Loss: [0.0393, 0.0566] | valid Loss: [0.0501, 0.0648]\n",
      "Epoch 197 | train Loss: [0.0393, 0.0566] | valid Loss: [0.0501, 0.0648]\n",
      "Epoch 198 | train Loss: [0.0393, 0.0566] | valid Loss: [0.0501, 0.0648]\n",
      "Epoch 199 | train Loss: [0.0393, 0.0566] | valid Loss: [0.0501, 0.0648]\n",
      "Epoch 200 | train Loss: [0.0393, 0.0566] | valid Loss: [0.0501, 0.0648]\n",
      "Epoch 201 | train Loss: [0.0393, 0.0566] | valid Loss: [0.0501, 0.0648]\n",
      "Epoch 202 | train Loss: [0.0393, 0.0566] | valid Loss: [0.0501, 0.0648]\n",
      "Epoch 203 | train Loss: [0.0393, 0.0566] | valid Loss: [0.0501, 0.0648]\n",
      "Epoch 204 | train Loss: [0.0393, 0.0566] | valid Loss: [0.0501, 0.0648]\n",
      "Epoch 205 | train Loss: [0.0393, 0.0566] | valid Loss: [0.0501, 0.0648]\n",
      "Epoch 206 | train Loss: [0.0393, 0.0566] | valid Loss: [0.0501, 0.0648]\n",
      "Epoch 207 | train Loss: [0.0393, 0.0566] | valid Loss: [0.0501, 0.0648]\n",
      "Epoch 208 | train Loss: [0.0393, 0.0566] | valid Loss: [0.0501, 0.0648]\n",
      "Epoch 209 | train Loss: [0.0393, 0.0566] | valid Loss: [0.0501, 0.0648]\n",
      "Epoch 210 | train Loss: [0.0393, 0.0566] | valid Loss: [0.0501, 0.0648]\n",
      "Epoch 211 | train Loss: [0.0393, 0.0566] | valid Loss: [0.0501, 0.0648]\n",
      "Epoch 212 | train Loss: [0.0393, 0.0566] | valid Loss: [0.0501, 0.0648]\n",
      "Epoch 213 | train Loss: [0.0393, 0.0566] | valid Loss: [0.0501, 0.0648]\n",
      "Epoch 214 | train Loss: [0.0393, 0.0566] | valid Loss: [0.0501, 0.0648]\n",
      "Epoch 215 | train Loss: [0.0393, 0.0566] | valid Loss: [0.0501, 0.0648]\n",
      "Epoch 216 | train Loss: [0.0393, 0.0566] | valid Loss: [0.0501, 0.0648]\n",
      "Epoch 217 | train Loss: [0.0393, 0.0566] | valid Loss: [0.0501, 0.0648]\n",
      "Epoch 218 | train Loss: [0.0393, 0.0566] | valid Loss: [0.0501, 0.0648]\n",
      "Epoch 219 | train Loss: [0.0393, 0.0566] | valid Loss: [0.0501, 0.0648]\n",
      "Epoch 220 | train Loss: [0.0393, 0.0566] | valid Loss: [0.0501, 0.0648]\n",
      "Epoch 221 | train Loss: [0.0393, 0.0566] | valid Loss: [0.0501, 0.0648]\n",
      "Epoch 222 | train Loss: [0.0393, 0.0566] | valid Loss: [0.0501, 0.0648]\n",
      "Epoch 223 | train Loss: [0.0393, 0.0566] | valid Loss: [0.0501, 0.0648]\n",
      "Epoch 224 | train Loss: [0.0393, 0.0566] | valid Loss: [0.0501, 0.0648]\n",
      "Epoch 225 | train Loss: [0.0393, 0.0566] | valid Loss: [0.0501, 0.0648]\n",
      "Epoch 226 | train Loss: [0.0392, 0.0566] | valid Loss: [0.0501, 0.0648]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 227 | train Loss: [0.0392, 0.0566] | valid Loss: [0.0501, 0.0647]\n",
      "Epoch 228 | train Loss: [0.0392, 0.0566] | valid Loss: [0.0500, 0.0647]\n",
      "Epoch 229 | train Loss: [0.0392, 0.0566] | valid Loss: [0.0500, 0.0647]\n",
      "Epoch 230 | train Loss: [0.0393, 0.0566] | valid Loss: [0.0500, 0.0647]\n",
      "Epoch 231 | train Loss: [0.0392, 0.0566] | valid Loss: [0.0500, 0.0647]\n",
      "Epoch 232 | train Loss: [0.0392, 0.0565] | valid Loss: [0.0500, 0.0647]\n",
      "Epoch 233 | train Loss: [0.0393, 0.0565] | valid Loss: [0.0500, 0.0647]\n",
      "Epoch 234 | train Loss: [0.0392, 0.0565] | valid Loss: [0.0500, 0.0647]\n",
      "Epoch 235 | train Loss: [0.0392, 0.0565] | valid Loss: [0.0500, 0.0647]\n",
      "Epoch 236 | train Loss: [0.0392, 0.0565] | valid Loss: [0.0500, 0.0647]\n",
      "Epoch 237 | train Loss: [0.0392, 0.0565] | valid Loss: [0.0500, 0.0647]\n",
      "Epoch 238 | train Loss: [0.0392, 0.0565] | valid Loss: [0.0500, 0.0647]\n",
      "Epoch 239 | train Loss: [0.0392, 0.0565] | valid Loss: [0.0500, 0.0647]\n",
      "Epoch 240 | train Loss: [0.0392, 0.0565] | valid Loss: [0.0500, 0.0647]\n",
      "Epoch 241 | train Loss: [0.0392, 0.0565] | valid Loss: [0.0500, 0.0647]\n",
      "Epoch 242 | train Loss: [0.0392, 0.0565] | valid Loss: [0.0500, 0.0647]\n",
      "Epoch 243 | train Loss: [0.0392, 0.0565] | valid Loss: [0.0500, 0.0647]\n",
      "Epoch 244 | train Loss: [0.0392, 0.0565] | valid Loss: [0.0500, 0.0647]\n",
      "Epoch 245 | train Loss: [0.0392, 0.0565] | valid Loss: [0.0500, 0.0647]\n",
      "Epoch 246 | train Loss: [0.0392, 0.0565] | valid Loss: [0.0500, 0.0647]\n",
      "Epoch 247 | train Loss: [0.0393, 0.0565] | valid Loss: [0.0500, 0.0647]\n",
      "Epoch 248 | train Loss: [0.0392, 0.0565] | valid Loss: [0.0500, 0.0647]\n",
      "Epoch 249 | train Loss: [0.0393, 0.0565] | valid Loss: [0.0500, 0.0647]\n",
      "Epoch 250 | train Loss: [0.0392, 0.0565] | valid Loss: [0.0500, 0.0647]\n",
      "Epoch 251 | train Loss: [0.0392, 0.0565] | valid Loss: [0.0500, 0.0647]\n",
      "Epoch 252 | train Loss: [0.0392, 0.0565] | valid Loss: [0.0500, 0.0647]\n",
      "Epoch 253 | train Loss: [0.0392, 0.0565] | valid Loss: [0.0500, 0.0647]\n",
      "Epoch 254 | train Loss: [0.0393, 0.0565] | valid Loss: [0.0500, 0.0647]\n",
      "Epoch 255 | train Loss: [0.0393, 0.0565] | valid Loss: [0.0500, 0.0647]\n",
      "Epoch 256 | train Loss: [0.0393, 0.0565] | valid Loss: [0.0500, 0.0647]\n",
      "Epoch 257 | train Loss: [0.0392, 0.0565] | valid Loss: [0.0500, 0.0647]\n",
      "Epoch 258 | train Loss: [0.0393, 0.0565] | valid Loss: [0.0500, 0.0647]\n",
      "Epoch 259 | train Loss: [0.0393, 0.0565] | valid Loss: [0.0500, 0.0647]\n",
      "Epoch 00261: reducing learning rate of group 0 to 6.2500e-02.\n",
      "Epoch 260 | train Loss: [0.0393, 0.0565] | valid Loss: [0.0500, 0.0647]\n",
      "Epoch 261 | train Loss: [0.0392, 0.0565] | valid Loss: [0.0500, 0.0647]\n",
      "Epoch 262 | train Loss: [0.0392, 0.0565] | valid Loss: [0.0500, 0.0647]\n",
      "Epoch 263 | train Loss: [0.0392, 0.0565] | valid Loss: [0.0500, 0.0647]\n",
      "Epoch 264 | train Loss: [0.0392, 0.0565] | valid Loss: [0.0500, 0.0647]\n",
      "Epoch 265 | train Loss: [0.0392, 0.0565] | valid Loss: [0.0500, 0.0647]\n",
      "Epoch 266 | train Loss: [0.0392, 0.0565] | valid Loss: [0.0500, 0.0647]\n",
      "Epoch 267 | train Loss: [0.0392, 0.0565] | valid Loss: [0.0500, 0.0647]\n",
      "Epoch 268 | train Loss: [0.0392, 0.0565] | valid Loss: [0.0500, 0.0647]\n",
      "Epoch 269 | train Loss: [0.0392, 0.0564] | valid Loss: [0.0500, 0.0647]\n",
      "Epoch 270 | train Loss: [0.0392, 0.0564] | valid Loss: [0.0500, 0.0647]\n",
      "Epoch 271 | train Loss: [0.0392, 0.0564] | valid Loss: [0.0500, 0.0647]\n",
      "Epoch 00273: reducing learning rate of group 0 to 3.1250e-02.\n",
      "Epoch 272 | train Loss: [0.0392, 0.0564] | valid Loss: [0.0500, 0.0647]\n",
      "Epoch 273 | train Loss: [0.0391, 0.0565] | valid Loss: [0.0500, 0.0646]\n",
      "Epoch 274 | train Loss: [0.0391, 0.0565] | valid Loss: [0.0500, 0.0646]\n",
      "Epoch 275 | train Loss: [0.0391, 0.0565] | valid Loss: [0.0500, 0.0646]\n",
      "Epoch 276 | train Loss: [0.0391, 0.0565] | valid Loss: [0.0500, 0.0646]\n",
      "Epoch 277 | train Loss: [0.0391, 0.0565] | valid Loss: [0.0500, 0.0646]\n",
      "Epoch 278 | train Loss: [0.0391, 0.0565] | valid Loss: [0.0500, 0.0646]\n",
      "Epoch 00280: reducing learning rate of group 0 to 1.5625e-02.\n",
      "Epoch 279 | train Loss: [0.0391, 0.0565] | valid Loss: [0.0500, 0.0646]\n",
      "Epoch 280 | train Loss: [0.0391, 0.0566] | valid Loss: [0.0500, 0.0646]\n",
      "Epoch 281 | train Loss: [0.0391, 0.0566] | valid Loss: [0.0500, 0.0646]\n",
      "Epoch 282 | train Loss: [0.0391, 0.0566] | valid Loss: [0.0500, 0.0646]\n",
      "Epoch 283 | train Loss: [0.0391, 0.0566] | valid Loss: [0.0500, 0.0646]\n",
      "Epoch 284 | train Loss: [0.0391, 0.0566] | valid Loss: [0.0500, 0.0646]\n",
      "Epoch 285 | train Loss: [0.0391, 0.0566] | valid Loss: [0.0500, 0.0646]\n",
      "Epoch 00287: reducing learning rate of group 0 to 7.8125e-03.\n",
      "Epoch 286 | train Loss: [0.0391, 0.0566] | valid Loss: [0.0500, 0.0646]\n",
      "Epoch 287 | train Loss: [0.0391, 0.0566] | valid Loss: [0.0499, 0.0646]\n",
      "Epoch 288 | train Loss: [0.0391, 0.0566] | valid Loss: [0.0499, 0.0646]\n",
      "Epoch 289 | train Loss: [0.0391, 0.0566] | valid Loss: [0.0499, 0.0646]\n",
      "Epoch 290 | train Loss: [0.0391, 0.0566] | valid Loss: [0.0499, 0.0646]\n",
      "Epoch 291 | train Loss: [0.0391, 0.0566] | valid Loss: [0.0499, 0.0646]\n",
      "Epoch 292 | train Loss: [0.0391, 0.0566] | valid Loss: [0.0499, 0.0646]\n",
      "Epoch 293 | train Loss: [0.0391, 0.0566] | valid Loss: [0.0499, 0.0646]\n",
      "Epoch 00295: reducing learning rate of group 0 to 3.9062e-03.\n",
      "Epoch 294 | train Loss: [0.0391, 0.0566] | valid Loss: [0.0499, 0.0646]\n",
      "Epoch 295 | train Loss: [0.0391, 0.0567] | valid Loss: [0.0499, 0.0646]\n",
      "Epoch 296 | train Loss: [0.0391, 0.0567] | valid Loss: [0.0499, 0.0646]\n",
      "Epoch 297 | train Loss: [0.0391, 0.0567] | valid Loss: [0.0499, 0.0646]\n",
      "Epoch 298 | train Loss: [0.0391, 0.0567] | valid Loss: [0.0499, 0.0646]\n",
      "Epoch 299 | train Loss: [0.0391, 0.0567] | valid Loss: [0.0499, 0.0646]\n"
     ]
    }
   ],
   "source": [
    "# モデルのインスタンス生成\n",
    "xy_dim = 2\n",
    "\n",
    "model = ActiveNet(xy_dim, dr_thresh, dropout=0, batchN=False, bias=True, Nchannels=128).to(device)\n",
    "# input data\n",
    "#data = dataset[0]\n",
    "\n",
    "# optimizer\n",
    "#optimizer = torch.optim.SGD(model.parameters(), lr=1e-4, momentum=0.9)\n",
    "#optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)#, weight_decay=5e-4)\n",
    "optimizer = torch.optim.Adadelta(model.parameters())#, rho=0.95)#, lr=1e-1, momentum=0.9)\n",
    "\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min', factor=0.5, patience=5, verbose=True)\n",
    "\n",
    "val_loss_log = []\n",
    "\n",
    "val_loss_min = np.Inf\n",
    "\n",
    "# learnig loop\n",
    "for epoch in range(300):\n",
    "    model.train()\n",
    "    for batch_x, batch_y, batch_i_dir in train_data:\n",
    "        optimizer.zero_grad()\n",
    "        lossv = 0\n",
    "        losstheta = 0\n",
    "        for ib in range(batch_x.size(0)):\n",
    "            model.load_celltypes(celltype_lst[int(batch_i_dir[ib])].to(device))\n",
    "            out = model(batch_x[ib].to(device))\n",
    "            lv, ltheta = myLoss(out, batch_y[ib].to(device))\n",
    "            lossv = lossv + lv\n",
    "            losstheta = losstheta + ltheta\n",
    "        lossv = lossv / batch_x.size(0)\n",
    "        losstheta = losstheta / batch_x.size(0)\n",
    "        (lossv+losstheta).backward()\n",
    "        optimizer.step()\n",
    "    model.eval()\n",
    "    val_lossv = 0\n",
    "    val_losstheta = 0\n",
    "    val_count = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_y, batch_i_dir in valid_data:\n",
    "            for ib in range(batch_x.size(0)):\n",
    "                model.load_celltypes(celltype_lst[int(batch_i_dir[ib])].to(device))\n",
    "                val_out = model(batch_x[ib].to(device))\n",
    "                lv, ltheta = myLoss(val_out, batch_y[ib].to(device))\n",
    "                val_lossv = val_lossv + lv\n",
    "                val_losstheta = val_losstheta + ltheta\n",
    "            val_count = val_count + batch_x.size(0)\n",
    "    val_lossv = val_lossv/val_count\n",
    "    val_losstheta = val_losstheta/val_count\n",
    "    val_loss = val_lossv + val_losstheta\n",
    "    scheduler.step(val_loss)\n",
    "    print('Epoch %d | train Loss: [%.4f, %.4f] | valid Loss: [%.4f, %.4f]' % (epoch,\n",
    "                                                                              lossv.item(), \n",
    "                                                                              losstheta.item(),\n",
    "                                                                              val_lossv.item(), \n",
    "                                                                              val_losstheta.item()))\n",
    "    val_loss_log.append([val_lossv.cpu().item(), val_losstheta.cpu().item()])\n",
    "    if val_loss.item() < val_loss_min:\n",
    "        stored_model = model\n",
    "        val_loss_min = val_loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b3cf1e91",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-22T08:28:10.030507Z",
     "start_time": "2022-11-22T08:28:09.912606Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('selfpropel', tensor(0.9674, device='cuda:1')),\n",
       "             ('interactNN.layer1.weight',\n",
       "              tensor([[-1.5789e-01, -1.8519e-01,  1.4166e-01, -4.5040e-02,  1.4097e-01,\n",
       "                       -1.2141e-01],\n",
       "                      [-6.7769e-01,  8.1622e-01,  2.5423e-01, -2.3726e-02, -5.1402e-02,\n",
       "                       -1.5248e-02],\n",
       "                      [-1.4998e-01, -1.1237e-02, -8.8741e-02, -2.6726e-01, -3.6581e-01,\n",
       "                       -9.9633e-03],\n",
       "                      [-1.8666e-01, -2.4193e-01,  3.5165e-01,  1.4185e-01,  1.4809e-01,\n",
       "                        1.5322e-01],\n",
       "                      [ 1.6536e-01, -3.4551e-01, -3.2431e-01,  9.5790e-02,  3.4689e-01,\n",
       "                        4.8828e-02],\n",
       "                      [ 1.4116e-01,  4.9433e-01, -1.6227e-02,  1.4102e-02,  1.4293e-01,\n",
       "                        1.8939e-01],\n",
       "                      [ 1.7721e-01, -1.1081e+00, -2.6408e-01,  2.6522e-01,  5.3711e-02,\n",
       "                        6.9439e-02],\n",
       "                      [ 5.8838e-02, -1.1318e-01,  3.2413e-01,  2.4427e-01, -2.3609e-01,\n",
       "                       -2.5406e-01],\n",
       "                      [-8.2445e-01,  6.6213e-01,  5.1267e-02, -2.2211e-01, -7.7745e-02,\n",
       "                       -3.6832e-02],\n",
       "                      [ 1.7080e-01, -5.7209e-01, -8.7961e-02,  8.4794e-02, -1.4046e-01,\n",
       "                       -2.0105e-02],\n",
       "                      [ 1.1428e+00,  1.6626e-01, -2.6142e-01, -1.6602e-02,  2.1054e-02,\n",
       "                        3.5129e-02],\n",
       "                      [ 1.8431e-01, -2.8820e-01, -2.7539e-01,  1.0541e-01, -5.1938e-01,\n",
       "                       -1.9178e-01],\n",
       "                      [ 2.9764e-02,  7.0189e-02,  2.1355e-01,  1.0924e-01,  2.9149e-01,\n",
       "                        2.2630e-01],\n",
       "                      [ 1.4164e-01, -3.7885e-01,  8.5692e-04, -3.7606e-02,  2.4969e-01,\n",
       "                       -1.2863e-02],\n",
       "                      [-5.2698e-01, -3.8255e-01,  2.1854e-01,  1.4995e-01, -3.5832e-01,\n",
       "                        1.4183e-01],\n",
       "                      [-2.5472e-01, -3.0014e-01,  6.6289e-02,  1.8731e-01, -4.0089e-01,\n",
       "                       -3.0792e-02],\n",
       "                      [ 2.9659e-01, -3.0183e-01,  3.4544e-02, -1.2226e-02,  6.9370e-03,\n",
       "                       -8.8740e-02],\n",
       "                      [ 2.4905e-01,  3.7991e-02, -1.0153e-01, -1.8117e-01, -7.5487e-02,\n",
       "                       -3.1758e-01],\n",
       "                      [-9.1314e-01, -4.8169e-01, -3.3636e-01,  8.9180e-02, -7.0585e-02,\n",
       "                        3.6179e-02],\n",
       "                      [ 1.3092e-01,  4.3149e-01,  2.3018e-01, -9.5810e-02, -4.7287e-02,\n",
       "                        3.1526e-01],\n",
       "                      [ 1.9473e-01, -4.0446e-01, -4.0200e-01,  1.4764e-01, -2.7297e-01,\n",
       "                        2.9049e-01],\n",
       "                      [ 1.3377e-01,  1.2137e+00, -2.8706e-01,  2.9719e-02,  2.8918e-02,\n",
       "                        2.7561e-02],\n",
       "                      [ 6.3554e-01, -3.3422e-01,  7.2506e-02,  2.8296e-01, -2.6844e-01,\n",
       "                        8.5005e-02],\n",
       "                      [ 2.5392e-01, -1.4677e-01,  1.4278e-01,  2.4800e-01,  3.1357e-01,\n",
       "                       -3.1286e-01],\n",
       "                      [ 7.9004e-01,  4.4825e-01, -1.2267e-01,  1.7319e-01, -1.3934e-02,\n",
       "                       -2.2559e-02],\n",
       "                      [-6.0422e-01,  5.1448e-01, -4.6040e-01, -2.6657e-02, -5.8094e-01,\n",
       "                        1.3521e-01],\n",
       "                      [ 1.2509e-01,  6.2122e-01,  1.5054e-01, -7.9953e-02, -2.7186e-01,\n",
       "                       -5.9249e-02],\n",
       "                      [ 5.5287e-02,  2.5015e-01, -2.2354e-02, -3.3303e-03,  5.4841e-02,\n",
       "                       -1.1184e-01],\n",
       "                      [-1.4458e-01, -1.3649e-02, -8.6786e-02,  5.4443e-02, -1.9312e-01,\n",
       "                       -3.0543e-01],\n",
       "                      [ 8.3905e-01, -4.9502e-01, -1.4503e-01, -5.6790e-02,  1.1903e-01,\n",
       "                        1.0807e-01],\n",
       "                      [-1.8233e-02, -4.5479e-02, -1.7662e-02,  3.4865e-01,  4.2339e-02,\n",
       "                       -3.4648e-02],\n",
       "                      [ 1.9115e-01, -2.7128e-01, -5.4367e-03,  3.8981e-01,  2.1242e-01,\n",
       "                        4.1365e-02],\n",
       "                      [ 1.2991e-02, -6.8597e-01, -1.3152e-02,  4.0413e-01,  8.5083e-02,\n",
       "                        3.7487e-02],\n",
       "                      [ 1.6935e-01, -3.0728e-01, -3.9514e-01,  2.3677e-01,  1.5593e-02,\n",
       "                        8.3387e-02],\n",
       "                      [ 8.3074e-02, -7.6618e-02, -9.7726e-02,  4.6186e-01, -3.3419e-01,\n",
       "                       -3.5860e-01],\n",
       "                      [-4.9731e-02,  3.6557e-01,  3.2034e-01, -3.0452e-01, -5.4991e-01,\n",
       "                       -9.4022e-02],\n",
       "                      [ 1.8175e-01,  1.0029e-02, -3.6396e-01, -6.6348e-02, -1.1420e-01,\n",
       "                       -4.5287e-01],\n",
       "                      [ 1.9508e-01,  4.8638e-01, -2.6955e-01, -2.6573e-01,  8.3171e-02,\n",
       "                        3.5697e-01],\n",
       "                      [-1.0457e-01,  2.2245e-01,  1.6825e-01,  3.9505e-01,  4.9622e-03,\n",
       "                       -6.3761e-02],\n",
       "                      [ 5.5390e-02, -2.2435e-01,  6.2464e-02,  2.3679e-02, -8.0839e-02,\n",
       "                       -1.5500e-01],\n",
       "                      [-1.1073e-02, -5.5337e-01,  1.6352e-01,  3.2771e-01,  1.8143e-02,\n",
       "                        6.2038e-02],\n",
       "                      [-2.0287e-01, -2.1019e-01,  2.4981e-02,  4.1407e-02, -2.8438e-01,\n",
       "                        7.6326e-02],\n",
       "                      [ 3.2349e-01,  5.2709e-01, -4.9310e-01, -2.1026e-02,  2.7192e-01,\n",
       "                       -1.7833e-01],\n",
       "                      [ 1.3762e-01,  2.1444e-01,  1.6088e-02, -3.9642e-01,  2.8496e-02,\n",
       "                       -1.4505e-01],\n",
       "                      [-7.4623e-02, -2.5874e-01,  9.6292e-02, -2.4340e-02, -1.7906e-01,\n",
       "                        4.2485e-03],\n",
       "                      [ 8.1155e-02, -1.5034e-01, -2.0570e-01,  3.7974e-01,  1.6663e-01,\n",
       "                       -1.7476e-01],\n",
       "                      [-6.2738e-01, -5.6138e-01,  1.7731e-01, -1.2807e-01, -3.5558e-02,\n",
       "                        2.3685e-02],\n",
       "                      [ 7.7212e-02,  4.0891e-01, -1.3031e-01,  1.1068e-01,  1.5576e-01,\n",
       "                        1.7552e-02],\n",
       "                      [-6.3534e-01, -6.6869e-02,  9.5204e-02,  2.5637e-03, -2.7528e-02,\n",
       "                       -1.3996e-01],\n",
       "                      [-2.3758e-02,  8.1152e-02,  1.0117e-01,  7.7172e-02, -3.8963e-01,\n",
       "                        2.8440e-01],\n",
       "                      [-2.2288e-01,  3.0648e-01,  1.8522e-02, -1.0105e-01, -3.9682e-02,\n",
       "                        1.9091e-02],\n",
       "                      [-4.6702e-01, -8.2130e-02, -3.0213e-01,  4.9934e-01, -4.1293e-01,\n",
       "                        4.4123e-02],\n",
       "                      [ 5.1039e-01,  1.2944e-01, -1.9760e-01, -3.1762e-02, -1.6993e-01,\n",
       "                        3.8813e-03],\n",
       "                      [-3.2761e-01, -9.6815e-01,  2.6708e-01, -7.4334e-02, -2.4467e-02,\n",
       "                       -3.4784e-01],\n",
       "                      [ 5.9727e-01,  3.8406e-01, -1.4759e-01, -6.7483e-02,  2.6480e-01,\n",
       "                        1.2960e-01],\n",
       "                      [-5.1665e-01,  6.5230e-02, -4.2968e-02, -6.0465e-01, -4.6514e-03,\n",
       "                       -4.0431e-02],\n",
       "                      [ 1.6642e-01, -1.9577e-01,  4.3513e-01,  1.8787e-01, -7.9207e-02,\n",
       "                       -3.0641e-01],\n",
       "                      [ 6.5563e-02,  1.2124e-02,  3.5146e-01, -1.8684e-01, -3.9794e-03,\n",
       "                        1.4832e-01],\n",
       "                      [-3.4886e-01, -4.9288e-01, -4.3534e-02,  4.0823e-01, -5.7960e-01,\n",
       "                        2.1110e-03],\n",
       "                      [ 3.8141e-01,  6.4764e-02, -8.3595e-02, -1.1832e-01,  2.0918e-01,\n",
       "                       -1.7587e-01],\n",
       "                      [-3.5726e-03, -3.7971e-01,  8.6275e-02,  1.5952e-01,  3.2157e-01,\n",
       "                       -8.2157e-03],\n",
       "                      [ 1.4293e-01, -2.1481e-01,  2.2303e-02, -3.8425e-01, -2.7580e-02,\n",
       "                       -1.7818e-01],\n",
       "                      [ 2.1371e-01,  3.6345e-01,  1.6451e-02,  1.4679e-01, -1.8811e-01,\n",
       "                        2.1490e-01],\n",
       "                      [ 4.2511e-02, -6.7066e-01, -3.3185e-01,  6.3085e-02, -2.8413e-01,\n",
       "                       -2.7467e-01],\n",
       "                      [ 1.2347e-01,  2.6818e-01,  7.3317e-02, -2.3677e-01, -4.5462e-03,\n",
       "                       -1.8737e-02],\n",
       "                      [-8.1994e-03,  1.5089e-01,  3.9925e-01, -1.4070e-01, -1.4113e-01,\n",
       "                       -3.9537e-01],\n",
       "                      [ 3.6024e-02, -2.0394e-02, -2.8439e-02, -2.5547e-01,  3.2639e-01,\n",
       "                        4.2251e-01],\n",
       "                      [ 1.3382e-01, -2.2846e-01,  3.0758e-01,  3.6821e-02, -1.5783e-01,\n",
       "                        2.4711e-01],\n",
       "                      [-4.1363e-01, -7.1976e-01, -2.1738e-01,  5.9835e-02, -3.3370e-01,\n",
       "                        1.3475e-01],\n",
       "                      [ 1.3631e-01, -8.2265e-01, -2.9117e-01,  1.0761e-02, -3.3233e-02,\n",
       "                       -1.4991e-02],\n",
       "                      [ 4.4623e-01,  8.8137e-02, -2.0046e-01,  1.0740e-01,  1.1571e-01,\n",
       "                        3.4283e-02],\n",
       "                      [ 2.8217e-01,  6.4369e-01, -1.4392e-01,  9.8204e-02, -3.1610e-01,\n",
       "                       -2.3044e-01],\n",
       "                      [ 2.3859e-01,  1.0899e-01, -2.3924e-01, -4.1782e-01, -1.7067e-01,\n",
       "                        3.6534e-01],\n",
       "                      [-7.9824e-02, -2.4325e-01, -1.9848e-01,  2.8568e-01,  4.3259e-01,\n",
       "                       -1.4819e-01],\n",
       "                      [ 6.6905e-01,  8.2857e-02, -4.4151e-01,  1.8582e-01,  1.8647e-02,\n",
       "                       -3.9259e-01],\n",
       "                      [-1.0522e-02,  4.1577e-01, -3.3002e-01, -3.7647e-01, -6.0567e-01,\n",
       "                       -7.1996e-02],\n",
       "                      [ 3.5610e-01, -1.5106e-01,  2.9027e-01,  1.2344e-01, -1.5894e-01,\n",
       "                       -1.4486e-01],\n",
       "                      [-1.9832e-01,  8.1664e-01,  3.5640e-02,  1.0579e-01,  6.5454e-02,\n",
       "                       -3.1337e-02],\n",
       "                      [ 4.2882e-01, -2.7736e-02, -4.1238e-01, -1.1919e-01, -1.2106e-02,\n",
       "                        2.4355e-01],\n",
       "                      [ 1.3760e-01,  7.6162e-02, -9.1454e-02, -6.6187e-02, -1.0705e-01,\n",
       "                       -9.0709e-02],\n",
       "                      [ 3.6715e-01,  1.0859e-01,  9.0037e-02,  5.2485e-02, -4.4049e-01,\n",
       "                       -4.8160e-01],\n",
       "                      [ 7.1824e-02,  5.3557e-02,  1.9014e-02, -1.5931e-01, -1.7857e-01,\n",
       "                       -1.6881e-01],\n",
       "                      [-1.8587e-01, -1.1608e-01, -9.0556e-02, -1.9798e-01,  1.3486e-01,\n",
       "                        1.4455e-02],\n",
       "                      [ 8.9048e-01,  8.7861e-01, -4.9269e-01, -3.3870e-02, -6.7674e-02,\n",
       "                        1.5326e-01],\n",
       "                      [ 2.4873e-02,  2.8153e-01,  6.2285e-02, -1.3152e-01, -2.3793e-01,\n",
       "                       -1.3994e-03],\n",
       "                      [-2.2554e-02,  1.2593e+00, -1.8286e-01, -6.2991e-02, -8.9523e-03,\n",
       "                       -1.6483e-02],\n",
       "                      [ 1.0224e+00,  2.2238e-01,  1.2482e-01, -1.7581e-01,  1.0843e-02,\n",
       "                       -9.4269e-02],\n",
       "                      [ 4.3706e-01,  7.5145e-01, -1.5728e-01, -1.1556e-01, -3.3496e-02,\n",
       "                       -4.2087e-01],\n",
       "                      [-2.5098e-01, -3.5052e-01,  3.9003e-01,  1.4319e-02,  3.8173e-02,\n",
       "                       -2.3569e-02],\n",
       "                      [-9.8976e-01,  5.0209e-02,  1.7065e-01,  3.1141e-01, -2.2008e-01,\n",
       "                       -2.6199e-02],\n",
       "                      [ 3.8429e-03,  3.3898e-01,  3.9638e-01, -7.2978e-02,  2.5109e-01,\n",
       "                       -2.6598e-01],\n",
       "                      [-5.8547e-01, -2.2417e-01,  1.9510e-01, -2.7219e-01, -2.6723e-01,\n",
       "                        7.1553e-02],\n",
       "                      [-3.4912e-01,  3.0470e-01, -3.9591e-01, -2.4336e-03,  2.0396e-01,\n",
       "                        5.9344e-02],\n",
       "                      [ 8.9630e-02,  1.7612e-02, -5.3610e-02, -1.1422e-01,  3.1007e-01,\n",
       "                       -2.9826e-01],\n",
       "                      [-6.5044e-01,  5.8448e-01, -1.4114e-01, -1.2597e-01,  1.9681e-01,\n",
       "                        1.2525e-01],\n",
       "                      [-8.8978e-02,  6.9019e-02,  1.5793e-01, -4.1189e-01, -3.2693e-01,\n",
       "                       -3.4167e-01],\n",
       "                      [ 2.9116e-01, -4.7612e-02,  1.4489e-01,  1.7115e-01, -1.2445e-02,\n",
       "                        1.0861e-03],\n",
       "                      [-2.8934e-01, -5.6465e-01, -3.3183e-02,  3.0998e-01, -3.9948e-02,\n",
       "                        9.3773e-03],\n",
       "                      [ 4.2585e-01,  1.5835e-01, -6.3613e-02,  1.9335e-01,  7.1076e-02,\n",
       "                       -5.3705e-02],\n",
       "                      [ 2.5153e-01, -2.7958e-01, -2.1739e-01,  9.9729e-02, -8.0303e-02,\n",
       "                        3.6601e-01],\n",
       "                      [ 7.6187e-01, -1.5506e-01,  1.1511e-01,  9.8623e-02,  1.3598e-01,\n",
       "                       -1.0546e-01],\n",
       "                      [-8.0373e-01, -5.5427e-01, -1.1485e-01, -7.0584e-02,  3.5548e-01,\n",
       "                        2.3023e-01],\n",
       "                      [ 8.9219e-02,  2.3492e-01, -5.7804e-02, -1.4765e-01,  1.7059e-02,\n",
       "                       -4.2372e-01],\n",
       "                      [ 3.6927e-01, -3.0313e-01, -3.2898e-01,  6.8284e-02,  8.1833e-04,\n",
       "                       -3.1491e-01],\n",
       "                      [ 4.2371e-02, -2.2524e-01,  4.4762e-02, -2.1325e-01, -3.1222e-01,\n",
       "                        4.5839e-01],\n",
       "                      [ 2.0998e-01, -5.0685e-03,  1.1665e-01, -3.2250e-02,  3.2270e-01,\n",
       "                       -1.3541e-01],\n",
       "                      [-4.0036e-01, -1.6865e-01, -2.6223e-01,  6.8696e-02,  9.2171e-02,\n",
       "                       -3.3982e-01],\n",
       "                      [ 9.8562e-02, -8.7568e-01,  4.0950e-02,  1.6748e-01,  1.9566e-01,\n",
       "                       -2.2631e-02],\n",
       "                      [ 5.9608e-01,  9.8404e-02,  1.9897e-01,  3.3072e-02, -8.2502e-02,\n",
       "                        5.2872e-02],\n",
       "                      [ 5.0338e-02, -1.5834e-01,  1.6633e-01,  2.1894e-01, -3.4147e-01,\n",
       "                       -4.2847e-01],\n",
       "                      [-2.5086e-01,  4.9355e-01, -7.5533e-02,  2.8257e-01, -7.7604e-02,\n",
       "                       -1.6615e-01],\n",
       "                      [ 3.4371e-01, -1.6975e-01, -6.4130e-02,  2.4304e-01,  5.0020e-02,\n",
       "                       -6.2859e-02],\n",
       "                      [-8.5108e-01, -6.8873e-01,  8.0557e-02,  3.1483e-01,  2.0082e-01,\n",
       "                       -3.2232e-02],\n",
       "                      [ 4.2674e-02, -2.9603e-01,  2.4034e-01, -2.7659e-01,  3.4584e-01,\n",
       "                       -1.1494e-01],\n",
       "                      [-4.5483e-01, -2.3443e-01,  2.5490e-01, -3.6365e-02, -1.6652e-01,\n",
       "                       -3.3525e-01],\n",
       "                      [-3.4519e-01,  5.8624e-01, -3.4160e-01,  1.1686e-01, -2.0906e-02,\n",
       "                       -8.1965e-02],\n",
       "                      [ 8.5310e-02, -3.4349e-01,  3.6580e-01, -1.3704e-01, -3.3623e-02,\n",
       "                       -3.3551e-01],\n",
       "                      [-2.5544e-01, -5.7394e-02,  1.3019e-01,  1.6986e-01, -9.3667e-03,\n",
       "                       -4.2922e-02],\n",
       "                      [ 9.3794e-02,  3.5201e-01,  1.6103e-02,  1.3729e-01,  3.4560e-02,\n",
       "                       -3.1877e-02],\n",
       "                      [ 1.0591e-01, -2.2670e-01, -4.6822e-01,  3.5114e-02, -5.7773e-01,\n",
       "                       -2.6373e-01],\n",
       "                      [ 3.2845e-01, -3.5781e-01,  4.8890e-02, -1.4646e-01, -2.6146e-01,\n",
       "                       -1.1535e-01],\n",
       "                      [ 8.3194e-01, -5.9937e-01, -7.6721e-02, -1.2637e-01,  1.1000e-01,\n",
       "                        6.2636e-02],\n",
       "                      [ 6.7978e-01, -7.8765e-01, -8.8825e-02, -4.5540e-02, -8.3736e-02,\n",
       "                        1.0174e-01],\n",
       "                      [-7.6373e-01, -2.1863e-02, -1.4027e-02,  1.4888e-01,  1.0219e-01,\n",
       "                        3.3211e-02],\n",
       "                      [-2.2901e-01,  6.5913e-02, -1.5580e-01,  1.6431e-01,  3.0555e-01,\n",
       "                       -9.6984e-03],\n",
       "                      [ 1.7529e-01, -3.6502e-01, -1.8103e-01,  1.4364e-01, -1.6705e-01,\n",
       "                       -9.8283e-02],\n",
       "                      [ 7.2690e-02, -1.1223e+00, -3.9903e-01, -1.7894e-01, -9.9954e-02,\n",
       "                       -5.2147e-03],\n",
       "                      [-5.4194e-01,  4.1903e-01,  2.7948e-01, -6.5974e-02,  3.8696e-02,\n",
       "                        1.4658e-01]], device='cuda:1')),\n",
       "             ('interactNN.layer1.bias',\n",
       "              tensor([-0.0267, -0.3058,  0.0195, -0.3943, -0.3479, -0.1071, -0.0397, -0.2737,\n",
       "                      -0.4664,  0.0533, -0.3151,  0.2463, -0.2645, -0.0779, -0.4311,  0.0026,\n",
       "                       0.1595,  0.0935, -0.3957, -0.2131, -0.0712, -0.3213, -0.1266, -0.4447,\n",
       "                      -0.2065, -0.1105, -0.3279,  0.0793,  0.0546,  0.1446,  0.0121, -0.3903,\n",
       "                       0.2726,  0.3137, -0.0199, -0.2738,  0.0520, -0.0908, -0.4798,  0.0257,\n",
       "                      -0.1344,  0.0459, -0.1033, -0.1719,  0.0101, -0.3002, -0.3031,  0.0016,\n",
       "                      -0.1710, -0.2233, -0.0267, -0.3530,  0.1750, -0.5543, -0.0894, -0.4247,\n",
       "                      -0.3723, -0.3453, -0.2586,  0.0741, -0.2870, -0.2664, -0.0238, -0.0960,\n",
       "                      -0.0623, -0.2473, -0.4054, -0.1894, -0.2588, -0.1249,  0.1701, -0.2996,\n",
       "                      -0.1930, -0.3379,  0.0437, -0.0376, -0.0903, -0.2690,  0.2122,  0.1532,\n",
       "                      -0.0905, -0.2446, -0.0594, -0.2388,  0.0345, -0.1859, -0.4555, -0.0578,\n",
       "                      -0.2450, -0.4324, -0.3898, -0.3487, -0.1547, -0.0121, -0.1081, -0.1577,\n",
       "                       0.1287, -0.0876,  0.0649, -0.0460, -0.1153, -0.4796,  0.0894, -0.0833,\n",
       "                      -0.3497, -0.1746, -0.0873, -0.1817, -0.1387, -0.0720, -0.4153,  0.0800,\n",
       "                      -0.2678, -0.4789, -0.2678, -0.3461, -0.3629, -0.0555,  0.0521,  0.2558,\n",
       "                      -0.0500, -0.1598, -0.1834, -0.0699, -0.1856,  0.3071,  0.0564, -0.2642],\n",
       "                     device='cuda:1')),\n",
       "             ('interactNN.layer2.weight',\n",
       "              tensor([[-0.1422,  0.0535, -0.0371,  ..., -0.1134, -0.0141,  0.0309],\n",
       "                      [-0.0283,  0.0754, -0.1444,  ..., -0.0326, -0.0370,  0.1540],\n",
       "                      [-0.1007,  0.0198,  0.0378,  ...,  0.0751, -0.0607,  0.1558],\n",
       "                      ...,\n",
       "                      [ 0.2639, -0.0584, -0.0210,  ..., -0.0358,  0.2589,  0.0960],\n",
       "                      [-0.2044, -0.3003, -0.0407,  ...,  0.0345, -0.1121, -0.2159],\n",
       "                      [-0.1757, -0.3248, -0.1682,  ..., -0.0145, -0.1096, -0.2792]],\n",
       "                     device='cuda:1')),\n",
       "             ('interactNN.layer2.bias',\n",
       "              tensor([-0.5729, -0.2678, -0.3903, -0.4144, -0.5002,  0.2451, -0.6687, -0.2398,\n",
       "                      -0.0680, -0.3262,  0.1671, -0.2943, -0.2412, -0.0567, -0.0711, -0.6460,\n",
       "                      -0.0690, -0.3576,  0.3753, -0.3299, -0.2750, -0.4489,  0.3391,  0.3604,\n",
       "                      -0.1456, -0.1838,  0.4939, -0.0254, -0.4093,  0.2948, -0.2534,  0.0760,\n",
       "                      -0.0721,  0.6577,  0.1545, -0.0446,  0.4102,  0.0373, -1.7937,  0.2906,\n",
       "                      -0.0567, -0.0702, -0.0470,  0.2198,  0.0154, -0.5088,  0.1180, -0.0276,\n",
       "                      -0.6311, -0.0500,  0.0294, -0.4196, -0.4882, -0.0475,  0.2338, -0.0714,\n",
       "                      -0.6104, -0.1435,  0.7137, -0.0633, -0.0022, -0.1059,  0.0497, -0.0958,\n",
       "                      -0.1059, -0.4932,  0.5636, -0.2202, -0.4429, -0.6586, -0.1171, -0.9655,\n",
       "                       0.1998, -0.3947, -0.1271, -0.0483, -0.0393, -0.2509, -0.0586,  0.1751,\n",
       "                       0.0721, -0.0458,  0.2672, -0.1532, -0.1079,  0.0540, -0.5764,  0.2923,\n",
       "                      -0.3620, -0.0251, -0.0514,  0.1189, -0.5309, -0.0462, -0.1198,  0.0471,\n",
       "                      -0.2348, -0.2263, -0.0810,  0.2340, -0.2556,  0.1494, -0.0722, -0.3395,\n",
       "                      -0.0507,  0.2917, -0.0415, -0.1778, -0.3191, -0.6904,  0.1061, -0.0814,\n",
       "                      -0.8127, -0.3896, -0.3265, -0.2238, -0.3220, -0.1484, -0.1685,  0.0052,\n",
       "                      -0.3268, -0.3952, -0.1633, -0.4160, -0.4006, -0.6139, -0.1682,  0.5033],\n",
       "                     device='cuda:1')),\n",
       "             ('interactNN.layer3.weight',\n",
       "              tensor([[-0.0863, -0.0653,  0.0063,  ..., -0.1262,  0.0679, -0.0357],\n",
       "                      [ 0.1741, -0.1052, -0.0323,  ..., -0.0586, -0.0663,  0.1063],\n",
       "                      [-0.0447,  0.0783, -0.0742,  ...,  0.0503, -0.0212,  0.0892],\n",
       "                      ...,\n",
       "                      [-0.1007, -0.0715, -0.0594,  ...,  0.0411,  0.0703,  0.1467],\n",
       "                      [-0.0843, -0.0636, -0.0266,  ..., -0.0718, -0.2059, -0.0839],\n",
       "                      [-0.0615, -0.0477, -0.0156,  ...,  0.0617, -0.0648, -0.1092]],\n",
       "                     device='cuda:1')),\n",
       "             ('interactNN.layer3.bias',\n",
       "              tensor([-0.0041, -0.0788, -0.0407,  0.3022, -0.4486,  0.2279, -0.0973, -0.1027,\n",
       "                      -0.0964,  0.3391, -0.3589, -0.0939, -0.4010, -0.3160, -0.1917,  0.0222,\n",
       "                      -0.2062, -0.3802,  0.2551, -0.0356, -0.2510, -0.0062, -0.4035,  0.3457,\n",
       "                      -0.0801,  0.3080, -0.1210, -0.1559, -0.0811,  0.3874, -0.1576,  0.2116,\n",
       "                       0.0632,  0.1540, -0.3229, -0.0539, -0.1273,  0.3059,  0.3004, -0.0805,\n",
       "                       0.3159, -0.3175,  0.2569, -0.0778, -0.0167,  0.3494,  0.3664,  0.1969,\n",
       "                       0.0902,  0.0297,  0.2231, -0.0006, -0.1025, -0.3653, -0.2060, -0.1507,\n",
       "                      -0.1452, -0.0360,  0.4007, -0.0089, -0.4934, -0.1533, -0.0636, -0.0508,\n",
       "                      -0.0291, -0.2760, -0.1335, -0.1703,  0.2707,  0.1762, -0.1194, -0.3604,\n",
       "                      -0.2371,  0.1388, -0.2040,  0.2726,  0.0647, -0.1466, -0.0236, -0.3172,\n",
       "                       0.1304, -0.1398,  0.1500, -0.0825,  0.0314, -0.4025,  0.0617,  0.3200,\n",
       "                       0.0694,  0.3226, -0.3157,  0.3254, -0.1791,  0.2822,  0.3054,  0.2446,\n",
       "                       0.3151,  0.2536, -0.1074, -0.0028, -0.2361,  0.2505, -0.1878, -0.0885,\n",
       "                       0.3600, -0.1000, -0.2482,  0.0361, -0.3209,  0.0051, -0.0899,  0.0503,\n",
       "                       0.0340, -0.1947,  0.1049, -0.0297, -0.0436,  0.3080, -0.1716,  0.4096,\n",
       "                       0.0093, -0.1343,  0.0089, -0.0484, -0.0739, -0.2688,  0.1919, -0.0471],\n",
       "                     device='cuda:1')),\n",
       "             ('interactNN.layer4.weight',\n",
       "              tensor([[-4.2938e-02,  5.8062e-02,  6.4283e-02, -5.1266e-01, -8.7036e-02,\n",
       "                       -9.0579e-02, -6.4805e-02, -1.0057e-01,  2.1887e-01, -2.5154e-01,\n",
       "                        2.8083e-01, -2.4055e-02, -1.6712e-01,  2.5115e-01, -2.4889e-01,\n",
       "                        3.5476e-02,  1.4436e-01, -2.2113e-01,  1.4863e-01,  3.9714e-02,\n",
       "                        1.0521e-01,  1.9580e-01, -3.8247e-01, -2.0544e-01,  1.6048e-01,\n",
       "                        7.5749e-02,  2.2576e-01, -6.2571e-02, -5.1113e-02,  2.5539e-01,\n",
       "                        1.4121e-01,  3.7946e-01, -4.5628e-02,  1.7196e-03, -2.2566e-01,\n",
       "                       -6.2261e-03, -9.0164e-02, -2.7966e-01,  2.6609e-01,  1.2047e-01,\n",
       "                       -2.8621e-01, -1.6729e-02,  2.3980e-01,  1.9910e-01,  9.5949e-02,\n",
       "                        3.0230e-02, -3.8343e-01, -1.2407e-04, -1.1607e-01,  4.7332e-02,\n",
       "                       -2.9175e-01,  1.8038e-01,  2.1512e-02, -4.1438e-02, -5.6376e-02,\n",
       "                        3.5961e-01,  1.7983e-01, -4.7305e-02, -2.7449e-01,  1.1139e-01,\n",
       "                        3.9439e-02, -9.1805e-03, -9.4278e-03,  2.3787e-01,  9.4897e-02,\n",
       "                        1.2390e-02,  2.2366e-01, -1.4886e-01,  3.1029e-01, -2.1097e-01,\n",
       "                        3.9079e-02,  2.8978e-01,  9.2609e-02, -2.2865e-01,  2.6774e-01,\n",
       "                       -4.8183e-01,  4.5422e-02,  1.0399e-01, -8.8687e-02, -1.2527e-01,\n",
       "                        4.3452e-02,  2.1071e-01, -1.3003e-01, -2.3972e-02, -7.2120e-02,\n",
       "                       -9.8542e-02, -3.5976e-02, -1.4495e-01, -3.7972e-02, -4.0974e-01,\n",
       "                        2.4125e-01,  4.8454e-01,  1.9003e-01,  3.3917e-01, -7.5044e-02,\n",
       "                        1.5204e-01, -2.5107e-01, -9.8977e-02, -1.0852e-02,  8.6135e-02,\n",
       "                       -1.5708e-02, -2.9098e-02,  1.1294e-01,  6.9975e-02,  2.1352e-01,\n",
       "                        1.4244e-01,  7.4819e-02,  4.6190e-01, -1.3010e-01,  6.4902e-02,\n",
       "                        1.8277e-01,  1.2890e-01, -6.4143e-02, -3.3444e-01, -3.4383e-02,\n",
       "                       -3.8959e-02,  3.6811e-02, -1.4445e-01, -1.5502e-01,  1.5158e-01,\n",
       "                       -3.4822e-02,  7.4751e-02, -2.6018e-02, -2.5146e-02,  8.1514e-02,\n",
       "                       -5.6323e-02,  1.9144e-01,  6.2652e-03],\n",
       "                      [ 3.1347e-01, -1.0809e-01,  1.3986e-01, -1.6205e-01,  2.1529e-01,\n",
       "                        3.1155e-01, -1.1764e-01,  1.5413e-01, -3.6153e-02,  2.0062e-02,\n",
       "                        2.2589e-02,  5.8148e-02,  3.9589e-01,  4.8736e-02,  1.0255e-01,\n",
       "                        9.4472e-02, -5.3478e-03, -3.0509e-01, -2.0317e-01, -2.6483e-02,\n",
       "                       -6.9764e-02,  2.0223e-01, -1.7722e-01,  1.1362e-01,  1.4631e-01,\n",
       "                        1.1940e-01, -2.5202e-01,  8.9240e-02, -1.7729e-01, -1.0609e-01,\n",
       "                        5.8956e-02, -3.0674e-01, -4.4497e-02, -3.1278e-01,  1.6583e-01,\n",
       "                       -1.7208e-01,  5.6664e-02,  1.9402e-01,  1.9014e-01, -5.1658e-02,\n",
       "                        1.0139e-01,  1.1321e-01, -3.6300e-01, -9.1463e-02,  1.7437e-01,\n",
       "                        3.8139e-01,  5.3148e-04,  1.1134e-01, -1.1317e-02,  8.4403e-02,\n",
       "                       -1.9250e-01, -5.5248e-02,  2.0327e-01, -4.3276e-01,  1.4980e-01,\n",
       "                       -9.8364e-02, -2.4719e-01,  2.0282e-01, -7.5160e-02, -3.7893e-02,\n",
       "                       -5.6187e-01, -1.2694e-01, -2.8115e-01,  1.5146e-02, -1.8314e-01,\n",
       "                       -1.9906e-01,  5.6379e-02, -1.0966e-01, -8.2092e-02, -1.0168e-02,\n",
       "                        1.4604e-03,  2.0121e-01, -2.2373e-01,  1.6758e-01,  1.8371e-03,\n",
       "                       -2.9392e-02, -1.2038e-01,  2.0252e-01,  2.5140e-01, -5.6142e-02,\n",
       "                       -1.4264e-01,  3.6799e-03, -1.0709e-01,  6.3726e-02,  5.1653e-02,\n",
       "                       -1.1625e-01, -1.2097e-01, -3.1157e-01,  1.6419e-01,  1.4563e-01,\n",
       "                        5.8765e-02,  5.1761e-02,  1.1727e-01, -2.0549e-01, -3.1548e-01,\n",
       "                        1.3576e-01, -2.4889e-01,  8.6324e-02,  7.0483e-02,  1.9248e-01,\n",
       "                        9.6042e-02,  3.1178e-01,  4.3041e-02, -2.7324e-01,  1.4901e-01,\n",
       "                        1.2747e-01, -1.7199e-01,  7.2609e-02,  2.0471e-01, -2.1160e-02,\n",
       "                       -2.0438e-02, -3.9906e-02,  2.4856e-02, -1.1147e-01, -1.3362e-01,\n",
       "                        1.2537e-01, -7.5881e-02, -7.2558e-02,  1.4488e-02, -3.4043e-01,\n",
       "                        1.4453e-01,  2.0532e-01, -3.2797e-01, -2.9518e-01, -9.6399e-02,\n",
       "                       -2.4283e-01, -1.5052e-01, -1.0491e-01]], device='cuda:1')),\n",
       "             ('interactNN.layer4.bias',\n",
       "              tensor([-0.0083,  0.0122], device='cuda:1')),\n",
       "             ('thetaDotNN.layer1.weight',\n",
       "              tensor([[ 9.8317e-01, -1.7588e-01, -1.1020e-01,  3.1775e-01, -2.2885e-01,\n",
       "                        4.5582e-02],\n",
       "                      [-3.6183e-01, -1.4516e-02,  7.6313e-02,  8.3019e-02,  3.6142e-01,\n",
       "                        2.9881e-01],\n",
       "                      [-1.2060e-01,  1.5303e-01, -2.0734e-01,  4.1629e-01, -1.4992e-01,\n",
       "                        1.5443e-01],\n",
       "                      [ 4.7786e-01, -1.6094e-01, -3.1612e-01,  2.1712e-01, -4.2747e-01,\n",
       "                       -5.6344e-02],\n",
       "                      [ 1.0495e-01,  2.7671e-01, -2.1899e-01, -1.0282e-01,  6.7371e-02,\n",
       "                        5.3935e-02],\n",
       "                      [ 2.2811e-01, -7.0584e-02,  1.2862e-01,  2.0878e-02, -3.4997e-01,\n",
       "                        1.0856e-01],\n",
       "                      [-1.8245e-01,  1.8208e-02, -2.4216e-02, -2.2820e-01,  4.2581e-01,\n",
       "                       -7.6078e-02],\n",
       "                      [-4.5446e-01,  3.3693e-01, -1.4551e-01, -1.4496e-01, -2.7551e-02,\n",
       "                       -1.6630e-01],\n",
       "                      [ 2.8440e-01,  2.6496e-01, -1.0329e-01,  6.3484e-02, -2.9976e-02,\n",
       "                        7.9116e-03],\n",
       "                      [-3.8558e-02,  2.6345e-01, -1.4079e-01, -2.8564e-01, -2.5558e-01,\n",
       "                        2.6034e-01],\n",
       "                      [-1.5329e-01, -1.1798e-01, -2.1378e-01,  6.1301e-02,  1.8098e-01,\n",
       "                        1.5942e-01],\n",
       "                      [-1.5048e-01, -1.2201e-01,  4.9678e-02,  1.1007e-02, -4.1698e-01,\n",
       "                       -3.9955e-01],\n",
       "                      [-4.3250e-02, -7.8681e-02,  1.9142e-01,  1.9621e-01, -3.9345e-02,\n",
       "                       -3.9104e-01],\n",
       "                      [ 2.6262e-01, -2.7159e-01,  2.3323e-01, -2.2274e-01, -4.9643e-01,\n",
       "                        5.5551e-02],\n",
       "                      [ 2.4895e-01,  8.2641e-02, -2.0832e-01, -1.0358e-01,  3.9240e-01,\n",
       "                       -3.3065e-01],\n",
       "                      [ 5.8461e-01,  1.0105e-01, -2.0618e-01, -6.0752e-01, -4.3860e-01,\n",
       "                       -3.5796e-01],\n",
       "                      [-2.6775e-01, -8.0582e-01, -2.3912e-01,  1.9583e-01, -6.2113e-01,\n",
       "                       -1.7158e-01],\n",
       "                      [ 1.1719e-01,  3.4566e-01,  1.2761e-01,  1.4677e-01, -4.2366e-01,\n",
       "                        2.4238e-01],\n",
       "                      [-4.5859e-01,  5.5668e-01,  1.3315e-01,  3.1810e-01,  4.2286e-01,\n",
       "                       -3.9185e-01],\n",
       "                      [-2.3884e-01,  3.5521e-01, -1.4548e-02, -7.6307e-01, -5.3245e-01,\n",
       "                       -9.0863e-02],\n",
       "                      [-7.4892e-02, -7.3995e-02,  1.0667e-01,  1.5514e-01,  2.1189e-01,\n",
       "                       -3.3421e-01],\n",
       "                      [ 5.2678e-01,  1.1374e-01,  3.1475e-01, -2.4829e-01, -6.4876e-03,\n",
       "                       -1.8110e-01],\n",
       "                      [ 4.2908e-02, -2.9134e-01,  2.2071e-01,  2.5816e-01, -4.0495e-01,\n",
       "                       -9.0451e-02],\n",
       "                      [-3.0775e-01,  6.2138e-02, -4.8349e-01,  1.1517e-01, -1.6501e-01,\n",
       "                        1.3005e-01],\n",
       "                      [ 7.9251e-01, -3.1359e-01,  3.9902e-01,  1.6498e-01, -2.4814e-01,\n",
       "                        8.5943e-04],\n",
       "                      [ 7.7050e-01,  4.3484e-01,  7.8563e-02, -1.7772e-01, -3.7002e-01,\n",
       "                        9.0388e-02],\n",
       "                      [-3.3213e-01,  5.0491e-01, -2.2689e-01,  2.1566e-01, -8.7957e-02,\n",
       "                       -2.0746e-01],\n",
       "                      [ 6.8445e-01, -3.7947e-01, -9.6052e-02,  1.5106e-01,  7.6116e-02,\n",
       "                       -1.0681e-01],\n",
       "                      [-1.5894e-01,  4.9198e-01, -6.1703e-02,  1.3058e-01, -1.7382e-02,\n",
       "                        3.0588e-02],\n",
       "                      [ 6.7039e-01, -1.7457e-01,  3.1847e-02, -5.2513e-01,  2.8203e-01,\n",
       "                        1.0178e-01],\n",
       "                      [ 2.4079e-01,  4.8861e-01, -1.3936e-01, -3.4737e-01, -1.4261e-01,\n",
       "                        7.2675e-02],\n",
       "                      [-3.1676e-01,  3.8609e-01,  1.0056e-01,  2.9373e-01, -3.1414e-01,\n",
       "                        2.5980e-03],\n",
       "                      [-2.7253e-01, -1.4546e-01, -4.0465e-01,  2.8670e-01, -3.5639e-01,\n",
       "                       -3.2148e-01],\n",
       "                      [ 3.9808e-01, -3.6145e-02,  1.3724e-02,  5.1774e-02, -1.9145e-01,\n",
       "                       -1.7753e-01],\n",
       "                      [ 1.0117e+00, -2.8515e-01,  1.0326e-02, -1.1856e-01,  8.2651e-02,\n",
       "                        1.6856e-02],\n",
       "                      [-5.4384e-02, -7.9673e-02,  7.8405e-02,  3.2368e-01, -1.1240e-01,\n",
       "                       -4.2554e-01],\n",
       "                      [-1.6950e-01, -1.9302e-01, -2.0444e-01,  3.4632e-01, -1.5062e-02,\n",
       "                        3.3877e-01],\n",
       "                      [-7.2644e-01, -1.0674e-01,  3.7613e-01, -3.1959e-02, -2.1754e-02,\n",
       "                       -4.2528e-02],\n",
       "                      [ 1.2104e-01,  2.8111e-01,  1.9916e-01, -2.2523e-02, -2.6700e-01,\n",
       "                       -3.4734e-01],\n",
       "                      [ 6.0893e-01, -3.7503e-01, -5.4483e-01,  3.0809e-01,  2.0685e-01,\n",
       "                        1.4613e-01],\n",
       "                      [ 6.0380e-02, -1.5067e-01,  2.4884e-01,  1.9432e-01, -1.0089e-01,\n",
       "                        7.0284e-02],\n",
       "                      [-2.3287e-01,  3.1981e-01,  2.7599e-02,  1.7395e-01, -4.2260e-02,\n",
       "                       -4.0460e-02],\n",
       "                      [ 5.3651e-01, -5.8667e-01, -6.9600e-02, -6.4240e-02,  5.0707e-01,\n",
       "                        4.5039e-01],\n",
       "                      [ 7.4419e-01, -4.5810e-01, -2.4353e-01,  1.1558e-01,  2.7459e-01,\n",
       "                        8.5572e-02],\n",
       "                      [-7.4897e-02, -2.1644e-01, -4.2773e-01, -2.3099e-01, -3.6587e-01,\n",
       "                       -6.2026e-02],\n",
       "                      [ 4.5253e-01, -2.1051e-01,  8.7150e-02, -2.2286e-03,  3.5722e-01,\n",
       "                       -2.3286e-01],\n",
       "                      [-2.7249e-01,  3.3894e-01, -2.2029e-02,  2.5829e-01,  1.5794e-01,\n",
       "                        2.7451e-01],\n",
       "                      [-2.6129e-01, -3.2574e-01,  1.8021e-02, -3.0992e-01, -5.4516e-01,\n",
       "                        1.3259e-01],\n",
       "                      [ 2.5809e-01,  7.5753e-01, -6.8246e-02,  3.5676e-01,  3.3056e-01,\n",
       "                        1.9144e-01],\n",
       "                      [-7.9891e-02,  1.4526e-03,  3.4254e-01, -1.5539e-02, -2.8613e-02,\n",
       "                       -2.5839e-01],\n",
       "                      [ 6.6149e-01, -1.4518e-01,  3.2785e-01,  3.7611e-01, -1.6611e-01,\n",
       "                       -9.1693e-02],\n",
       "                      [-3.9506e-02,  4.8983e-01,  4.1919e-01,  3.2177e-02, -2.1688e-01,\n",
       "                        5.7357e-02],\n",
       "                      [ 1.2211e-01, -2.3162e-01, -1.9351e-01, -3.9964e-01, -3.7352e-01,\n",
       "                        4.4044e-01],\n",
       "                      [-4.9000e-01, -1.3500e-01, -6.4572e-01,  1.8121e-01,  1.0731e-01,\n",
       "                        2.0881e-01],\n",
       "                      [ 9.2419e-01,  4.8731e-01,  3.6731e-02,  2.7707e-01, -2.2751e-01,\n",
       "                       -1.1319e-01],\n",
       "                      [-1.2873e-01, -7.0637e-01, -1.7596e-01, -3.8951e-01,  2.4304e-02,\n",
       "                        7.0247e-02],\n",
       "                      [ 6.2314e-01,  4.4876e-01, -2.7587e-01, -1.1025e-01, -3.1351e-01,\n",
       "                       -1.6325e-02],\n",
       "                      [ 3.0309e-01, -1.4726e-01,  5.6228e-02,  1.8602e-01,  4.4518e-01,\n",
       "                        2.9733e-01],\n",
       "                      [ 4.2431e-01, -1.3916e-01, -3.7962e-01,  3.3755e-01,  2.8744e-01,\n",
       "                       -4.3597e-01],\n",
       "                      [-3.8595e-01,  7.6942e-01, -1.8362e-02,  1.0173e-01, -4.0461e-01,\n",
       "                       -7.1193e-02],\n",
       "                      [-3.5737e-01, -3.7135e-01,  2.2174e-01, -1.0458e-01,  4.7368e-01,\n",
       "                       -1.1750e-01],\n",
       "                      [ 4.6862e-01,  1.8371e-01,  2.3329e-01, -4.0088e-01, -3.9383e-03,\n",
       "                        2.9793e-02],\n",
       "                      [-2.7374e-01, -3.2287e-01, -9.6546e-02, -3.2163e-01,  1.4916e-01,\n",
       "                       -3.0309e-01],\n",
       "                      [ 1.8071e-01,  2.0648e-01,  3.6652e-02, -3.0343e-01,  3.8582e-01,\n",
       "                        3.5681e-01],\n",
       "                      [-3.9887e-01,  1.5470e-01, -8.6570e-02, -1.1487e-01, -1.4541e-01,\n",
       "                       -3.1045e-01],\n",
       "                      [-2.3804e-01, -2.8915e-02, -2.8002e-01,  5.7525e-03, -3.9599e-01,\n",
       "                       -8.5739e-02],\n",
       "                      [-1.0524e-01, -8.7096e-01, -1.1747e-02, -1.1664e-01, -4.2085e-02,\n",
       "                        3.3898e-02],\n",
       "                      [ 6.3945e-01, -6.4427e-01, -1.4002e-01,  3.2685e-01, -5.2748e-01,\n",
       "                       -1.3927e-01],\n",
       "                      [-1.2414e-01,  1.6234e-01, -3.1069e-01, -3.3341e-01, -3.4700e-01,\n",
       "                       -4.8096e-02],\n",
       "                      [ 7.1658e-01, -1.0417e-01,  5.3064e-01, -1.8804e-01,  1.9065e-01,\n",
       "                        3.5091e-01],\n",
       "                      [ 2.7142e-01, -2.7675e-01,  1.2301e-01,  1.7975e-01,  1.0396e-01,\n",
       "                       -2.6215e-01],\n",
       "                      [-1.7243e-01,  1.3932e-02,  7.0826e-02,  1.5810e-01, -1.8705e-01,\n",
       "                        2.1628e-02],\n",
       "                      [-6.7855e-01, -1.9596e-01,  1.3060e-01, -3.5287e-01,  1.5070e-01,\n",
       "                        3.7059e-02],\n",
       "                      [-6.0968e-01, -8.8481e-02,  1.6865e-01, -8.4599e-02,  4.2693e-01,\n",
       "                       -4.7804e-02],\n",
       "                      [ 3.4937e-01, -2.3739e-01,  2.7217e-01,  2.3634e-01,  2.4319e-01,\n",
       "                       -8.7087e-02],\n",
       "                      [ 8.8914e-02,  1.5416e-01, -3.9057e-01,  3.8501e-01,  4.5719e-01,\n",
       "                        4.4721e-02],\n",
       "                      [ 1.1798e-01, -4.9445e-01, -1.8286e-01, -1.7611e-01,  4.9031e-01,\n",
       "                        2.9603e-03],\n",
       "                      [ 4.3547e-01, -1.4019e-01, -2.5885e-02,  2.3069e-01,  2.3030e-02,\n",
       "                        2.3495e-01],\n",
       "                      [ 8.6866e-02,  3.0725e-01, -2.4740e-01,  9.6401e-02, -1.0119e-01,\n",
       "                        3.9582e-01],\n",
       "                      [-7.2480e-02,  4.3030e-02,  2.9162e-02,  3.2993e-02, -4.1865e-01,\n",
       "                       -3.9311e-01],\n",
       "                      [ 3.1019e-01, -5.3248e-01,  2.6689e-01,  5.1505e-02, -4.2165e-01,\n",
       "                       -4.3822e-01],\n",
       "                      [ 4.9507e-01,  2.4957e-01,  2.1951e-01, -9.5024e-02, -2.1216e-01,\n",
       "                       -3.0103e-01],\n",
       "                      [-4.8955e-01, -1.0674e-01,  1.1266e-01,  4.3060e-02,  3.6479e-01,\n",
       "                       -2.1940e-01],\n",
       "                      [ 5.6955e-01,  8.5853e-01, -4.5836e-01,  2.8374e-01, -1.2855e-01,\n",
       "                        4.1182e-02],\n",
       "                      [ 4.4748e-01, -2.6284e-02,  4.5637e-02, -9.2726e-02, -1.4133e-01,\n",
       "                       -5.1116e-01],\n",
       "                      [-2.9129e-01,  4.3159e-01, -1.2947e-01,  6.2557e-02, -1.2586e-01,\n",
       "                       -3.8574e-01],\n",
       "                      [ 1.8411e-01, -2.6839e-01, -8.0166e-02,  4.0117e-01, -2.7001e-01,\n",
       "                       -2.4013e-01],\n",
       "                      [-1.9736e-01, -7.5815e-01,  2.1846e-01, -3.9196e-02, -1.0880e-01,\n",
       "                        1.6763e-01],\n",
       "                      [-2.0777e-01,  1.3961e-01, -2.7321e-01,  2.8820e-01,  2.0272e-02,\n",
       "                        3.6927e-02],\n",
       "                      [-3.6743e-01, -6.9184e-02,  2.4039e-01, -6.7920e-03, -3.8493e-01,\n",
       "                       -8.9402e-02],\n",
       "                      [ 1.2344e-01,  8.5590e-01,  2.6201e-02, -1.5580e-01,  1.4963e-01,\n",
       "                       -1.3807e-02],\n",
       "                      [ 1.8720e-01,  5.7286e-01,  3.3316e-02, -2.3466e-02,  1.6164e-01,\n",
       "                       -6.1529e-02],\n",
       "                      [-7.5716e-02,  6.0908e-02, -1.0551e-01,  1.0932e-01,  1.9048e-01,\n",
       "                       -3.2833e-01],\n",
       "                      [ 4.0667e-01, -5.1987e-02, -1.4658e-01,  3.0476e-01, -3.0730e-01,\n",
       "                       -1.9737e-01],\n",
       "                      [-2.1650e-01, -5.6541e-01, -1.1610e-01,  9.7753e-02,  6.6925e-01,\n",
       "                       -3.1751e-01],\n",
       "                      [-6.4254e-02,  1.9385e-01, -6.8484e-02, -3.3863e-01,  1.8286e-02,\n",
       "                       -1.8493e-01],\n",
       "                      [-5.9819e-02, -4.7031e-01,  1.2752e-01,  3.6658e-02, -3.2100e-02,\n",
       "                        3.7626e-01],\n",
       "                      [ 3.8813e-01, -3.6040e-01, -2.4868e-01, -2.1764e-01, -3.6201e-01,\n",
       "                        4.5866e-01],\n",
       "                      [ 4.3217e-01,  6.8064e-02, -1.3108e-01,  2.2226e-01,  2.8570e-01,\n",
       "                        4.7637e-02],\n",
       "                      [-1.6501e-01, -2.1268e-02, -2.5120e-01,  2.5525e-01, -3.2417e-01,\n",
       "                       -3.6166e-02],\n",
       "                      [-3.4543e-01,  1.6106e-01, -4.9576e-02, -1.3725e-01,  1.8994e-01,\n",
       "                        2.0614e-01],\n",
       "                      [-1.2434e-01,  8.0636e-01, -3.2004e-01,  2.3585e-01,  1.9046e-01,\n",
       "                       -3.8792e-03],\n",
       "                      [-8.2521e-01,  3.5534e-02,  1.6478e-01,  1.6207e-01, -3.6068e-01,\n",
       "                       -1.9227e-01],\n",
       "                      [ 3.4593e-01,  2.3593e-01, -3.9756e-01, -4.2734e-01, -3.5263e-01,\n",
       "                       -3.9642e-02],\n",
       "                      [-2.9587e-01, -3.4992e-01,  1.9182e-01,  2.8741e-02,  2.4060e-01,\n",
       "                       -4.1944e-02],\n",
       "                      [ 9.3149e-02, -5.8124e-01, -2.0233e-01,  3.3958e-01,  4.7461e-02,\n",
       "                       -5.9639e-02],\n",
       "                      [-1.9681e-01, -3.9697e-01, -3.2410e-01,  1.3791e-02, -3.0982e-01,\n",
       "                       -2.3503e-01],\n",
       "                      [-1.7431e-01, -3.5223e-02,  3.1049e-02,  3.2980e-01,  5.8918e-02,\n",
       "                        2.8784e-01],\n",
       "                      [-2.0055e-01, -5.5370e-01, -2.7730e-01, -2.3976e-01, -1.7660e-01,\n",
       "                        9.5864e-02],\n",
       "                      [ 6.0247e-01, -1.3435e-01, -7.8718e-02,  1.3688e-02,  4.5751e-01,\n",
       "                        1.5198e-01],\n",
       "                      [ 5.1663e-01, -1.8861e-02, -3.7699e-01,  4.6366e-01, -1.4875e-01,\n",
       "                       -1.6840e-01],\n",
       "                      [ 2.7331e-01, -3.0285e-01,  1.4142e-01,  4.9219e-01, -5.6429e-01,\n",
       "                       -3.5391e-02],\n",
       "                      [ 4.5073e-01, -1.8838e-01,  1.8124e-01, -8.3472e-02,  9.9799e-03,\n",
       "                        2.4015e-01],\n",
       "                      [-2.1450e-01,  4.9402e-01,  4.5797e-02,  1.4069e-01, -3.9796e-01,\n",
       "                       -1.8019e-01],\n",
       "                      [-4.6702e-01, -1.0310e-01,  8.1261e-02, -3.3856e-01, -2.0072e-01,\n",
       "                       -2.4108e-01],\n",
       "                      [-1.5020e+00, -2.8690e-01, -7.4286e-03, -1.0605e-01,  3.7959e-01,\n",
       "                        2.7005e-01],\n",
       "                      [ 6.4983e-01,  2.4088e-02,  1.9125e-01,  3.0231e-01,  9.9796e-02,\n",
       "                        1.1020e-01],\n",
       "                      [-1.6580e-01, -5.5977e-01,  1.2969e-02, -6.2703e-02, -5.5292e-02,\n",
       "                       -1.9993e-01],\n",
       "                      [-4.5360e-01,  2.1486e-01,  7.4543e-02, -4.9008e-03,  2.1279e-01,\n",
       "                       -1.6851e-02],\n",
       "                      [ 6.7433e-01,  3.2732e-01, -3.4143e-01, -1.6845e-02,  3.7192e-01,\n",
       "                       -2.4057e-03],\n",
       "                      [ 3.3277e-01, -2.9490e-01, -2.5742e-01, -3.2882e-01, -2.5804e-02,\n",
       "                        1.9774e-01],\n",
       "                      [ 4.7434e-01, -1.6271e-01,  6.1269e-02, -3.0309e-03, -6.7997e-02,\n",
       "                        1.9499e-02],\n",
       "                      [ 3.9146e-01, -2.2200e-02, -6.6564e-02,  1.5114e-01, -3.6252e-01,\n",
       "                       -3.2528e-01],\n",
       "                      [-2.0062e-01,  3.1234e-01, -3.9088e-01, -1.3963e-02, -2.7545e-01,\n",
       "                        2.7204e-01],\n",
       "                      [ 1.9726e-01,  5.9374e-01, -1.1684e-01, -1.6231e-01, -1.9386e-01,\n",
       "                        3.8498e-02],\n",
       "                      [ 4.8392e-02,  5.5968e-01, -2.4821e-02,  1.4659e-01, -2.4714e-01,\n",
       "                       -1.9815e-01],\n",
       "                      [ 7.3711e-01,  6.0068e-01,  2.4534e-01, -4.9617e-01, -1.7232e-02,\n",
       "                        8.0675e-03],\n",
       "                      [-9.8742e-02, -2.2862e-01, -3.9181e-01,  5.7429e-02,  1.8133e-01,\n",
       "                        1.4334e-02]], device='cuda:1')),\n",
       "             ('thetaDotNN.layer1.bias',\n",
       "              tensor([-1.0358e-01, -2.9595e-01, -2.6259e-01,  3.3812e-01, -3.2226e-02,\n",
       "                      -5.6174e-02, -3.3394e-01, -1.3380e-01,  6.0576e-02, -3.9401e-01,\n",
       "                       1.4337e-01, -2.0113e-02, -1.0812e-01,  1.5219e-01, -3.2946e-01,\n",
       "                       2.6211e-01,  1.5094e-01, -3.8167e-02, -5.2553e-01,  2.5881e-01,\n",
       "                       6.2809e-03,  2.0198e-03, -3.0450e-01,  2.7470e-02, -3.0887e-01,\n",
       "                      -1.7815e-02, -1.7561e-01, -1.2775e-01,  5.6544e-02, -3.8254e-01,\n",
       "                       2.4203e-01, -1.0069e-01,  2.1677e-01,  2.7316e-01, -1.2833e-01,\n",
       "                      -3.5189e-02, -2.9005e-01, -1.2809e-01, -1.0453e-01,  8.8052e-02,\n",
       "                      -2.1369e-01, -2.9177e-02, -2.5503e-01,  4.2027e-02,  1.1008e-01,\n",
       "                       1.6142e-02, -3.6413e-01,  4.5697e-02, -3.8414e-01, -3.1403e-01,\n",
       "                       1.2319e-01, -3.4289e-01, -2.6308e-01,  1.6130e-01, -3.3969e-01,\n",
       "                      -4.2407e-01,  8.9899e-02,  6.0073e-02,  2.4035e-01, -2.8084e-01,\n",
       "                      -2.2082e-01,  1.1907e-02, -1.4730e-01, -3.1827e-01, -2.2401e-01,\n",
       "                       2.1218e-01,  2.0358e-02,  2.7286e-01, -3.5758e-01, -3.4549e-01,\n",
       "                       2.0612e-01, -1.0229e-02, -5.8451e-01, -1.9588e-01, -1.4075e-01,\n",
       "                      -1.6611e-01, -9.5647e-02,  2.5950e-02, -3.6877e-01,  1.1742e-02,\n",
       "                      -1.7288e-01, -6.5645e-02,  4.3544e-02, -1.9187e-01,  2.0571e-01,\n",
       "                       7.8093e-02, -3.0193e-01, -2.1812e-01, -2.5359e-01,  6.9375e-02,\n",
       "                      -3.2049e-03, -2.6411e-02,  7.9725e-02, -3.0813e-01,  1.2242e-01,\n",
       "                      -4.2639e-01, -3.7418e-01, -1.1616e-01,  4.4433e-02, -3.1928e-01,\n",
       "                      -1.2284e-01,  4.2181e-01, -3.2676e-01, -2.3991e-03, -1.9163e-01,\n",
       "                       1.8863e-01,  1.0013e-01, -1.4187e-01,  2.7772e-01,  5.7366e-02,\n",
       "                      -1.4144e-01, -6.1782e-02,  1.6806e-02, -8.4982e-02,  3.2991e-02,\n",
       "                      -2.3538e-01, -3.9157e-01, -7.2207e-02, -2.9854e-01,  4.9528e-02,\n",
       "                       1.4167e-04,  3.3171e-01,  1.1846e-01,  6.9203e-02,  1.3114e-03,\n",
       "                       2.4770e-02, -3.1577e-01, -2.3236e-01], device='cuda:1')),\n",
       "             ('thetaDotNN.layer2.weight',\n",
       "              tensor([[ 0.0215,  0.0191, -0.0214,  ...,  0.0447,  0.0231, -0.0244],\n",
       "                      [-0.1785, -0.0318,  0.0403,  ...,  0.1592,  0.2467,  0.0452],\n",
       "                      [-0.0409, -0.0464, -0.0544,  ..., -0.0937, -0.0451, -0.0506],\n",
       "                      ...,\n",
       "                      [-0.0874, -0.0169, -0.0279,  ..., -0.0672,  0.0602, -0.0101],\n",
       "                      [-0.0630,  0.0281, -0.0809,  ..., -0.0990, -0.0822, -0.0020],\n",
       "                      [ 0.0438,  0.0310, -0.0961,  ...,  0.0812,  0.1793, -0.0192]],\n",
       "                     device='cuda:1')),\n",
       "             ('thetaDotNN.layer2.bias',\n",
       "              tensor([-1.5096e-01, -2.4681e-01,  7.2383e-02,  3.8126e-02, -4.3131e-02,\n",
       "                       2.2164e-02, -1.0123e-01, -6.5842e-02, -1.1868e-01,  5.3602e-01,\n",
       "                      -2.1813e-01, -3.6503e-03, -1.3356e-01, -1.3557e-01, -1.6706e-02,\n",
       "                       3.5430e-01,  1.6337e-01, -5.5509e-01, -2.1706e-01, -6.1552e-02,\n",
       "                       7.4463e-02, -2.1262e-01,  1.9918e-02,  1.0854e-01, -3.9920e-01,\n",
       "                      -5.1908e-01,  5.5089e-04,  2.3184e-01,  1.2231e-01, -9.7119e-03,\n",
       "                      -3.0711e-03, -1.3961e-01,  9.7035e-03, -7.3213e-02,  4.9320e-02,\n",
       "                      -3.7246e-02, -2.7924e-02, -3.1165e-01, -3.6138e-02, -7.3852e-01,\n",
       "                       1.6408e-02, -1.0236e-01, -2.9106e-02, -5.4927e-01,  4.8241e-02,\n",
       "                      -4.2528e-02,  1.0131e-01, -2.9081e-01, -2.0030e-01, -4.0749e-02,\n",
       "                      -3.7979e-02,  1.2691e-01, -1.5860e-01, -8.9763e-02,  9.5551e-02,\n",
       "                       1.5544e-01,  1.5364e-01, -1.6898e-01, -2.3688e-02, -1.2903e-01,\n",
       "                      -4.7060e-01, -3.0083e-01,  9.2460e-02,  1.3623e-02,  2.6522e-02,\n",
       "                      -4.7065e-02,  1.4598e-01, -3.6318e-01, -1.1709e-01, -1.4618e-01,\n",
       "                       3.5154e-01, -9.5043e-02, -8.6811e-02,  2.6616e-01,  8.2055e-02,\n",
       "                       5.2672e-02, -8.6493e-02, -2.5506e-01, -2.7359e-01, -3.1286e-01,\n",
       "                      -5.8534e-01,  3.7986e-01, -3.1101e-02, -1.3165e-01, -6.0840e-02,\n",
       "                      -2.0772e-01, -3.6466e-01, -9.1964e-01,  1.5640e-02, -5.8688e-02,\n",
       "                      -2.2058e-01, -9.8260e-02, -3.7325e-01, -3.2912e-02, -6.8405e-02,\n",
       "                       2.9526e-02, -2.3454e-01, -1.2907e-01,  3.1251e-02, -1.5223e-01,\n",
       "                      -1.4089e-01, -1.4678e-01,  1.8367e-01, -1.1614e-01,  1.4483e-01,\n",
       "                       7.3479e-03, -6.6401e-02, -8.3061e-02, -6.3941e-02,  1.6810e-04,\n",
       "                      -2.6387e-02, -1.0929e-01, -8.6398e-02, -2.4254e-02,  5.8124e-02,\n",
       "                       1.0687e-01,  4.8040e-02, -7.8217e-03, -5.5178e-02, -5.6243e-01,\n",
       "                       7.2753e-02,  3.2250e-02, -3.4498e-01,  3.1105e-02,  4.2111e-01,\n",
       "                      -1.2000e-01,  1.2246e-01, -1.1117e-01], device='cuda:1')),\n",
       "             ('thetaDotNN.layer3.weight',\n",
       "              tensor([[ 5.8291e-02,  5.7994e-02, -1.3183e-02,  ..., -2.9725e-02,\n",
       "                        6.2078e-02, -5.1592e-03],\n",
       "                      [ 3.1619e-02, -4.4862e-02,  3.4001e-02,  ..., -9.1516e-02,\n",
       "                        7.8170e-02,  4.2469e-02],\n",
       "                      [ 7.7420e-03,  6.1470e-02, -3.6696e-02,  ...,  1.2344e-02,\n",
       "                        5.3171e-02, -8.9225e-03],\n",
       "                      ...,\n",
       "                      [-3.1274e-02, -2.5709e-02, -3.5353e-02,  ...,  2.8716e-05,\n",
       "                       -9.1093e-02,  1.6945e-02],\n",
       "                      [-5.8116e-02, -1.9915e-02, -6.4221e-02,  ...,  8.2076e-02,\n",
       "                        4.7438e-02,  5.8730e-02],\n",
       "                      [ 4.3297e-02,  4.8894e-02, -6.1773e-02,  ...,  6.6436e-03,\n",
       "                        1.2134e-03, -8.1973e-02]], device='cuda:1')),\n",
       "             ('thetaDotNN.layer3.bias',\n",
       "              tensor([-0.0512, -0.0513, -0.1133, -0.0072, -0.1951, -0.0028, -0.4166, -0.2153,\n",
       "                      -0.0442, -0.5394, -0.1743, -0.3444, -0.1305,  0.2075, -0.0885, -0.0574,\n",
       "                      -0.1473, -0.1298, -0.2862, -0.1467,  0.0511,  0.4479, -0.0588, -0.0795,\n",
       "                      -0.3419, -0.1917, -0.0901, -0.0900, -0.1373, -0.0732, -0.1117, -0.1604,\n",
       "                      -0.0811, -0.0612, -0.6342, -0.0641, -0.0203, -0.2909,  0.0176, -0.0690,\n",
       "                      -0.2433, -0.4027, -0.4816,  0.0474, -0.0688, -0.2607, -0.0733, -0.0371,\n",
       "                      -0.0574, -0.0617, -0.0233, -0.0295, -0.2803, -0.0651, -0.1702, -0.0255,\n",
       "                      -0.1435, -0.1667, -0.0502,  0.3107, -0.0494, -0.0514, -0.1038, -0.0350,\n",
       "                       0.0886, -0.1871, -0.1725, -0.0555, -0.2024,  0.4048, -0.1307, -0.1546,\n",
       "                      -0.1523, -0.5000, -0.0300, -0.6349, -0.0271, -0.2394, -0.1344, -0.0530,\n",
       "                       0.0065, -0.5902, -0.3742, -0.1040, -0.3664, -0.0988, -0.0960, -0.2751,\n",
       "                      -0.1376, -0.1802, -0.1706, -0.1106, -0.0951, -0.0366, -0.1353, -0.1792,\n",
       "                       0.1149, -0.0105,  0.4019, -0.1810, -0.2057, -0.4661,  0.2756, -0.3127,\n",
       "                      -0.1170,  0.0307, -0.0833, -0.0500, -0.0256, -0.0460, -0.1195, -0.0644,\n",
       "                      -0.1324,  0.0204, -0.1622, -0.4720,  0.0613, -0.1584, -0.0563, -0.1823,\n",
       "                      -0.1684, -0.1255,  0.4172, -0.2036, -0.0386, -0.0641, -0.1102, -0.1094],\n",
       "                     device='cuda:1')),\n",
       "             ('thetaDotNN.layer4.weight',\n",
       "              tensor([[-0.0093,  0.0908,  0.1320,  0.0936, -0.0779,  0.1342, -0.1696, -0.1182,\n",
       "                        0.2675, -0.3337,  0.1347, -0.1196,  0.1920, -0.2254,  0.1297, -0.0244,\n",
       "                       -0.0988,  0.2071, -0.1716, -0.0909,  0.1726, -0.3668, -0.0071, -0.2603,\n",
       "                        0.2007, -0.2265, -0.1619,  0.1621, -0.2093, -0.0067,  0.0043,  0.1823,\n",
       "                        0.0877,  0.0889, -0.1709, -0.0160,  0.1413,  0.2224,  0.4329, -0.1069,\n",
       "                        0.2213, -0.1514, -0.1166,  0.0942,  0.0655,  0.1084,  0.0993, -0.0238,\n",
       "                       -0.0374, -0.0141, -0.1154,  0.0885,  0.2149,  0.1283,  0.1839,  0.1888,\n",
       "                        0.1667,  0.2191,  0.1246, -0.4292,  0.0015, -0.1064,  0.1397,  0.2568,\n",
       "                        0.1231, -0.2110, -0.1371, -0.1175,  0.1515, -0.2612,  0.2501, -0.1490,\n",
       "                        0.1867, -0.2476,  0.0023, -0.0959,  0.1311,  0.1190, -0.1123,  0.0603,\n",
       "                       -0.0858, -0.2931, -0.2061, -0.0136,  0.2656, -0.1365,  0.2844, -0.1788,\n",
       "                        0.1229,  0.1319,  0.1060,  0.3944,  0.2512,  0.0550, -0.0584,  0.2897,\n",
       "                       -0.1860,  0.0294, -0.3076, -0.2067,  0.1428,  0.2472, -0.1942, -0.1706,\n",
       "                        0.1839, -0.2902,  0.0451,  0.1033, -0.1115, -0.0689, -0.1886, -0.0839,\n",
       "                        0.0032,  0.2796,  0.1281, -0.2847, -0.1752, -0.0901,  0.0242, -0.1381,\n",
       "                       -0.0970,  0.2180,  0.4599,  0.1663, -0.0247, -0.0137, -0.0065,  0.1681]],\n",
       "                     device='cuda:1')),\n",
       "             ('thetaDotNN.layer4.bias', tensor([0.1095], device='cuda:1'))])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "44265599",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-22T08:28:11.223418Z",
     "start_time": "2022-11-22T08:28:10.032947Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss: [0.0493, 0.0642]\n"
     ]
    }
   ],
   "source": [
    "# モデルを評価モードに設定\n",
    "stored_model.eval()\n",
    "\n",
    "# 推論\n",
    "test_lossv = 0\n",
    "test_losstheta = 0\n",
    "test_count = 0\n",
    "with torch.no_grad():\n",
    "    for batch_x, batch_y, batch_i_dir in test_data:\n",
    "        for ib in range(batch_x.size(0)):\n",
    "            model.load_celltypes(celltype_lst[int(batch_i_dir[ib])].to(device))\n",
    "            test_out = model(batch_x[ib].to(device))\n",
    "            lv, ltheta = myLoss(test_out, batch_y[ib].to(device))\n",
    "            test_lossv = test_lossv + lv\n",
    "            test_losstheta = test_losstheta + ltheta\n",
    "        test_count = test_count + batch_x.size(0)\n",
    "test_lossv = test_lossv/test_count\n",
    "test_losstheta = test_losstheta/test_count\n",
    "print('test Loss: [%.4f, %.4f]' % (test_lossv.item(), test_losstheta.item()))\n",
    "test_loss = [test_lossv.item(), test_losstheta.item()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d0283033",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-22T08:28:11.232061Z",
     "start_time": "2022-11-22T08:28:11.227231Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import datetime\n",
    "\n",
    "now = datetime.datetime.now()\n",
    "nowstr = now.strftime('%Y%m%d_%H%M%S')\n",
    "\n",
    "os.makedirs(savedirName + nowstr + '/', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "445915ef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-22T08:28:11.244790Z",
     "start_time": "2022-11-22T08:28:11.235251Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9674, device='cuda:1')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stored_model.selfpropel.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "62154d31",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-22T08:28:11.281283Z",
     "start_time": "2022-11-22T08:28:11.246717Z"
    }
   },
   "outputs": [],
   "source": [
    "stored_model = stored_model.to('cpu')\n",
    "\n",
    "filename1 = savedirName + nowstr + '/' + nowstr + '_Model.pkl'\n",
    "with open(filename1, \"wb\") as f:\n",
    "    pickle.dump(stored_model, f)\n",
    "\n",
    "filename1_2 = savedirName + nowstr + '/' + nowstr + '_Model.pt'\n",
    "torch.save(stored_model, filename1_2)\n",
    "\n",
    "filename2 = savedirName + nowstr + '/' + nowstr\n",
    "torch.save(stored_model.interactNN.state_dict(), filename2 + '_interactNN.pkl')\n",
    "torch.save(stored_model.thetaDotNN.state_dict(), filename2 + '_thetaDotNN.pkl')\n",
    "torch.save(stored_model.selfpropel.detach(), filename2 + '_selfpropel.pkl')\n",
    "\n",
    "filename3 = savedirName + nowstr + '/' + nowstr + '_Separation.npz'\n",
    "np.savez(filename3, dr_thresh=dr_thresh, batch_size=batch_size,\n",
    "         train_inds=train_inds, valid_inds=valid_inds, test_inds=test_inds, \n",
    "         val_loss_log=val_loss_log, test_loss=test_loss)\n",
    "\n",
    "filename4 = savedirName + nowstr + '/' + nowstr + '_optimizer.pt'\n",
    "torch.save(optimizer, filename4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0bd69a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "270.85px",
    "left": "494px",
    "right": "20px",
    "top": "120px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
