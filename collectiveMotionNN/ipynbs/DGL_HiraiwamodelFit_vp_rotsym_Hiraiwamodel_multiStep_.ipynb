{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f855b76a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-18T08:15:45.993959Z",
     "start_time": "2023-01-18T08:15:45.988552Z"
    }
   },
   "outputs": [],
   "source": [
    "#!pip install dgl-cu113 dglgo -f https://data.dgl.ai/wheels/repo.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7807020c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-18T08:15:46.007341Z",
     "start_time": "2023-01-18T08:15:46.002984Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['DGLBACKEND'] = 'pytorch'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d12568c6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-18T08:15:47.584018Z",
     "start_time": "2023-01-18T08:15:46.012502Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import dgl\n",
    "import dgl.function as fn\n",
    "\n",
    "import networkx as nx\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import gc\n",
    "\n",
    "# デバイス設定\n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eea59a8d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-18T08:18:10.280595Z",
     "start_time": "2023-01-18T08:18:10.259695Z"
    }
   },
   "outputs": [],
   "source": [
    "from scipy import special\n",
    "from torch.autograd import Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "61e1f3ee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-18T08:18:10.290539Z",
     "start_time": "2023-01-18T08:18:10.284973Z"
    }
   },
   "outputs": [],
   "source": [
    "def printNPZ(npz):\n",
    "    for kw in npz.files:\n",
    "        print(kw, npz[kw])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0ba5ba52",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-18T08:18:10.321523Z",
     "start_time": "2023-01-18T08:18:10.294678Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v0 1.0\n",
      "r 1.0\n",
      "D 0.1\n",
      "A 0.0\n",
      "L 20\n",
      "rho 1.0\n",
      "beta 1.0\n",
      "A_CFs [0.1 0.9]\n",
      "A_CIL 0.0\n",
      "cellType_ratio [0.5 0.5]\n",
      "quiv_colors ['k' 'r']\n",
      "kappa 0.5\n",
      "A_Macdonalds [2.  0.2]\n",
      "batch_size 400\n",
      "state_size 3\n",
      "brownian_size 1\n",
      "periodic True\n",
      "t_max 1000\n",
      "methodSDE heun\n",
      "isIto False\n",
      "stepSDE 0.01\n"
     ]
    }
   ],
   "source": [
    "dirName = '/home/uwamichi/jupyter/HiraiwaModel_chem20220916_150816/'\n",
    "savedirName = dirName + 'HiraiwaModelFit_multiStep/'\n",
    "os.makedirs(savedirName, exist_ok=True)\n",
    "\n",
    "params = np.load(dirName+'params.npz')\n",
    "#traj = np.load(dirName+'result.npz')\n",
    "\n",
    "printNPZ(params)\n",
    "#printNPZ(traj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c4e73591",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-18T08:18:10.336050Z",
     "start_time": "2023-01-18T08:18:10.326289Z"
    }
   },
   "outputs": [],
   "source": [
    "if params['periodic']:\n",
    "    L = torch.tensor(params['L'])\n",
    "    def calc_dr(r1, r2):\n",
    "        dr = torch.remainder((r1 - r2), L)\n",
    "        dr[dr > L/2] = dr[dr > L/2] - L\n",
    "        return dr\n",
    "else:\n",
    "    def calc_dr(r1, r2):\n",
    "        return r1 - r2\n",
    "    \n",
    "def makeGraph(x_data, r_thresh):\n",
    "        Ndata = x_data.size(0)\n",
    "        dx = calc_dr(torch.unsqueeze(x_data, 0), torch.unsqueeze(x_data, 1))\n",
    "        dx = torch.sum(dx**2, dim=2)\n",
    "        edges = torch.argwhere(dx < r_thresh/2)\n",
    "        return dgl.graph((edges[:,0], edges[:,1]), num_nodes=Ndata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e3e01d8a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-18T08:18:10.348931Z",
     "start_time": "2023-01-18T08:18:10.340389Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/uwamichi/jupyter/HiraiwaModel_chem20220916_150816/20221013_130509', '/home/uwamichi/jupyter/HiraiwaModel_chem20220916_150816/20221013_222508', '/home/uwamichi/jupyter/HiraiwaModel_chem20220916_150816/20221014_020927', '/home/uwamichi/jupyter/HiraiwaModel_chem20220916_150816/20221014_055334', '/home/uwamichi/jupyter/HiraiwaModel_chem20220916_150816/20221014_093729', '/home/uwamichi/jupyter/HiraiwaModel_chem20220916_150816/ActiveNet_celltypes', '/home/uwamichi/jupyter/HiraiwaModel_chem20220916_150816/ActiveNet_vp_rotsym', '/home/uwamichi/jupyter/HiraiwaModel_chem20220916_150816/20221022_000642', '/home/uwamichi/jupyter/HiraiwaModel_chem20220916_150816/ActiveNet_vp_rotsym_dropout', '/home/uwamichi/jupyter/HiraiwaModel_chem20220916_150816/ActiveNet_vp_rotsym_batchNorm', '/home/uwamichi/jupyter/HiraiwaModel_chem20220916_150816/20221024_041213', '/home/uwamichi/jupyter/HiraiwaModel_chem20220916_150816/20221024_080435', '/home/uwamichi/jupyter/HiraiwaModel_chem20220916_150816/20221024_115625', '/home/uwamichi/jupyter/HiraiwaModel_chem20220916_150816/20221024_155544', '/home/uwamichi/jupyter/HiraiwaModel_chem20220916_150816/20221024_195452', '/home/uwamichi/jupyter/HiraiwaModel_chem20220916_150816/ActiveNet_vp_rotsym_multiStep_batchNorm', '/home/uwamichi/jupyter/HiraiwaModel_chem20220916_150816/ActiveNet_vp_rotsym_multiStep_transfer_batchNorm', '/home/uwamichi/jupyter/HiraiwaModel_chem20220916_150816/HiraiwaModelFit_multiStep']\n",
      "['/home/uwamichi/jupyter/HiraiwaModel_chem20220916_150816/20221013_130509', '/home/uwamichi/jupyter/HiraiwaModel_chem20220916_150816/20221013_222508', '/home/uwamichi/jupyter/HiraiwaModel_chem20220916_150816/20221014_020927', '/home/uwamichi/jupyter/HiraiwaModel_chem20220916_150816/20221014_055334', '/home/uwamichi/jupyter/HiraiwaModel_chem20220916_150816/20221014_093729', '/home/uwamichi/jupyter/HiraiwaModel_chem20220916_150816/20221022_000642', '/home/uwamichi/jupyter/HiraiwaModel_chem20220916_150816/20221024_041213', '/home/uwamichi/jupyter/HiraiwaModel_chem20220916_150816/20221024_080435', '/home/uwamichi/jupyter/HiraiwaModel_chem20220916_150816/20221024_115625', '/home/uwamichi/jupyter/HiraiwaModel_chem20220916_150816/20221024_155544', '/home/uwamichi/jupyter/HiraiwaModel_chem20220916_150816/20221024_195452']\n"
     ]
    }
   ],
   "source": [
    "subdir_list = [f.path for f in os.scandir(dirName) if f.is_dir()]\n",
    "\n",
    "print(subdir_list)\n",
    "\n",
    "datadir_list = [f for f in subdir_list if 'result.npz' in [ff.name for ff in os.scandir(f)]]\n",
    "\n",
    "print(datadir_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "19f22447",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-18T08:18:10.360724Z",
     "start_time": "2023-01-18T08:18:10.351947Z"
    }
   },
   "outputs": [],
   "source": [
    "class myDataset(Dataset):\n",
    "    def __init__(self, data_x, celltype_List, t_yseq=1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.data_x = data_x # List of tensors\n",
    "        #self.data_y = data_y\n",
    "        self.celltype_List = celltype_List\n",
    "        \n",
    "        self.data_len = np.array([xx.size(0) for xx in self.data_x])\n",
    "        self.t_yseq = t_yseq\n",
    "        \n",
    "        self.data_len_cumsum = np.cumsum(self.data_len - (self.t_yseq - 1))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return (self.data_len - (self.t_yseq - 1)).sum()\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        id_List = np.argwhere(index<self.data_len_cumsum)[0,0]\n",
    "        \n",
    "        if id_List:\n",
    "            id_tensor = index - self.data_len_cumsum[id_List-1]\n",
    "        else:\n",
    "            id_tensor = index\n",
    "        \n",
    "        return self.data_x[id_List][id_tensor], self.data_x[id_List][id_tensor:(id_tensor+self.t_yseq)], self.celltype_List[id_List]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3e9e2f01",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-18T08:18:10.739836Z",
     "start_time": "2023-01-18T08:18:10.364495Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "366"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dr_thresh = 7\n",
    "dt = 1\n",
    "batch_size = 8\n",
    "\n",
    "T_pred = 3\n",
    "\n",
    "N_data = len(datadir_list)\n",
    "\n",
    "#TR_VA_rate = np.array([0.6, 0.2])\n",
    "\n",
    "TR_last = 5\n",
    "VA_last = 7\n",
    "\n",
    "shuffle_inds = np.arange(N_data, dtype=int)\n",
    "np.random.shuffle(shuffle_inds)\n",
    "\n",
    "train_inds = shuffle_inds[:TR_last]\n",
    "valid_inds = shuffle_inds[TR_last:VA_last]\n",
    "test_inds = shuffle_inds[VA_last:]\n",
    "\n",
    "celltype_lst = []\n",
    "\n",
    "train_x = []\n",
    "valid_x = []\n",
    "test_x = []\n",
    "\n",
    "train_y = []\n",
    "valid_y = []\n",
    "test_y = []\n",
    "\n",
    "train_ct = []\n",
    "valid_ct = []\n",
    "test_ct = []\n",
    "\n",
    "for i_dir, subdirName in enumerate(datadir_list):\n",
    "    \n",
    "    traj = np.load(subdirName+'/result.npz')\n",
    "    \n",
    "    xy_t = torch.tensor(traj['xy'])#[:-1,:,:])\n",
    "    #v_t = calc_dr(torch.tensor(traj['xy'][1:,:,:]), torch.tensor(traj['xy'][:-1,:,:])) / dt\n",
    "    p_t = torch.unsqueeze(torch.tensor(traj['theta']), dim=2)#[:-1,:]), dim=2)\n",
    "    #w_t = torch.unsqueeze(torch.tensor((traj['theta'][1:,:]-traj['theta'][:-1,:])%(2*np.pi)/dt), dim=2)\n",
    "    \n",
    "    if i_dir in train_inds:\n",
    "        train_x.append(torch.concat((xy_t, p_t), -1))\n",
    "        #train_y.append(torch.concat((v_t, w_t), -1))\n",
    "        train_ct.append(torch.tensor(traj['celltype_label']).view(-1,1))\n",
    "\n",
    "    if i_dir in valid_inds:\n",
    "        valid_x.append(torch.concat((xy_t, p_t), -1))\n",
    "        #valid_y.append(torch.concat((v_t, w_t), -1))\n",
    "        valid_ct.append(torch.tensor(traj['celltype_label']).view(-1,1))\n",
    "        \n",
    "    if i_dir in test_inds:\n",
    "        test_x.append(torch.concat((xy_t, p_t), -1))\n",
    "        #test_y.append(torch.concat((v_t, w_t), -1))\n",
    "        test_ct.append(torch.tensor(traj['celltype_label']).view(-1,1))\n",
    "    \n",
    "train_dataset = myDataset(train_x, train_ct, t_yseq=T_pred)\n",
    "\n",
    "valid_dataset = myDataset(valid_x, valid_ct, t_yseq=T_pred)\n",
    "\n",
    "test_dataset = myDataset(test_x, test_ct, t_yseq=T_pred)\n",
    "\n",
    "\n",
    "train_data = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, pin_memory=True)\n",
    "valid_data = torch.utils.data.DataLoader(valid_dataset, batch_size=batch_size, pin_memory=True)\n",
    "test_data = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, pin_memory=True)\n",
    "\n",
    "del train_x, train_ct, train_dataset\n",
    "del valid_x, valid_ct, valid_dataset\n",
    "del test_x, test_ct, test_dataset\n",
    "gc.collect()\n",
    "\n",
    "#print(data)\n",
    "#print(data.num_graphs)\n",
    "#print(data.x)\n",
    "#print(data.y)\n",
    "#print(data.edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1c0a633b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-18T08:18:10.745974Z",
     "start_time": "2023-01-18T08:18:10.741321Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1  8  0 10  9]\n"
     ]
    }
   ],
   "source": [
    "print(train_inds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4f35452f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-18T08:18:10.755149Z",
     "start_time": "2023-01-18T08:18:10.749953Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_DataLoader__initialized',\n",
       " '_DataLoader__multiprocessing_context',\n",
       " '_IterableDataset_len_called',\n",
       " '__annotations__',\n",
       " '__class__',\n",
       " '__class_getitem__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__iter__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__orig_bases__',\n",
       " '__parameters__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__slots__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_auto_collation',\n",
       " '_dataset_kind',\n",
       " '_get_iterator',\n",
       " '_get_shared_seed',\n",
       " '_index_sampler',\n",
       " '_is_protocol',\n",
       " '_iterator',\n",
       " 'batch_sampler',\n",
       " 'batch_size',\n",
       " 'check_worker_number_rationality',\n",
       " 'collate_fn',\n",
       " 'dataset',\n",
       " 'drop_last',\n",
       " 'generator',\n",
       " 'multiprocessing_context',\n",
       " 'num_workers',\n",
       " 'persistent_workers',\n",
       " 'pin_memory',\n",
       " 'pin_memory_device',\n",
       " 'prefetch_factor',\n",
       " 'sampler',\n",
       " 'timeout',\n",
       " 'worker_init_fn']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e5e6c153",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-18T08:18:10.766376Z",
     "start_time": "2023-01-18T08:18:10.758494Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "625"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ca11fa4f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-18T08:18:10.772799Z",
     "start_time": "2023-01-18T08:18:10.769546Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 999 1998 2997 3996 4995]\n"
     ]
    }
   ],
   "source": [
    "print(train_data.dataset.data_len_cumsum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "5d3d9149",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-18T08:48:49.105961Z",
     "start_time": "2023-01-18T08:48:49.072118Z"
    }
   },
   "outputs": [],
   "source": [
    "class torch_knFunction(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input, n):\n",
    "        ctx.save_for_backward(input)\n",
    "        ctx.n = n\n",
    "        numpy_input = input.cpu().detach().numpy()\n",
    "        result = special.kn(n, numpy_input)\n",
    "        return torch.as_tensor(result, dtype=input.dtype, device=device)\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        numpy_go = grad_output.cpu().detach().numpy()\n",
    "        input, = ctx.saved_tensors\n",
    "        n = ctx.n\n",
    "        numpy_input = input.cpu().detach().numpy()\n",
    "        if n==0:\n",
    "            grad_kn = -special.kn(1, numpy_input)\n",
    "        else:\n",
    "            grad_kn = -(special.kn(n-1, numpy_input) + special.kn(n+1, numpy_input))/2\n",
    "        result = numpy_go * grad_kn\n",
    "        return torch.as_tensor(result, dtype=input.dtype, device=device), None\n",
    "\n",
    "class torch_kn(nn.Module):\n",
    "    def __init__(self, n):\n",
    "        super(torch_kn, self).__init__()\n",
    "        self.n = n\n",
    "        \n",
    "    def forward(self, input):\n",
    "        return torch_knFunction.apply(input, self.n)\n",
    "    \n",
    "torch_scipy_k0 = torch_kn(0)\n",
    "torch_scipy_k1 = torch_kn(1)\n",
    "\n",
    "cutoff = torch.tensor([3.5], device = device)\n",
    "k1_cutoff = torch_scipy_k1(cutoff)\n",
    "\n",
    "func_cutoff = nn.ReLU()\n",
    "\n",
    "def torch_scipy_k1_cutoff(x):\n",
    "    return func_cutoff(torch_scipy_k1(x) - k1_cutoff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "86047d7a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-18T08:48:49.116112Z",
     "start_time": "2023-01-18T08:48:49.110082Z"
    }
   },
   "outputs": [],
   "source": [
    "def J_chemMacdonald(xy, d): # cutoff! --> shift the values\n",
    "    chemMag = torch_scipy_k1_cutoff(params['kappa'].item() * d) *(params['kappa'].item()/(2*np.pi))\n",
    "    xy = chemMag * xy\n",
    "    return torch.nansum(xy, dim=0, keepdim=False)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "1292f02c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-18T14:49:27.078213Z",
     "start_time": "2023-01-18T14:49:27.061580Z"
    }
   },
   "outputs": [],
   "source": [
    "if params['periodic']:\n",
    "    def xy2distance(xy, L):\n",
    "        dx = xy[:, :1] - xy[:, :1].T   # neighbor - target\n",
    "        dy = xy[:, 1:] - xy[:, 1:].T\n",
    "        dx = torch.remainder(dx, L)\n",
    "        dy = torch.remainder(dy, L)\n",
    "        dx[dx > L/2] = dx[dx > L/2] - L\n",
    "        dy[dy > L/2] = dy[dy > L/2] - L\n",
    "        return [dx, dy]\n",
    "else:\n",
    "    def xy2distance(xy, L):\n",
    "        dx = xy[:, :1] - xy[:, :1].T\n",
    "        dy = xy[:, 1:] - xy[:, 1:].T\n",
    "        return [dx, dy]    \n",
    "\n",
    "def J_CF(xy, dr, q):\n",
    "    c = torch.unsqueeze(q[0], 1)   # cos, sin of neighbor\n",
    "    s = torch.unsqueeze(q[1], 1)\n",
    "    xy = (1 + xy[:,:,:1] * c + xy[:,:,1:] * s) * xy / 2\n",
    "    return torch.nansum(xy, dim=0, keepdim=False)\n",
    "\n",
    "def J_CIL(xy, d, r):\n",
    "    xy = ((r/d) - 1) * xy\n",
    "    xy[...,:1][d==0] = 0\n",
    "    xy[...,1:][d==0] = 0\n",
    "    return torch.nansum(xy, dim=0, keepdim=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "21e97796",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-18T14:49:27.103684Z",
     "start_time": "2023-01-18T14:49:27.083384Z"
    }
   },
   "outputs": [],
   "source": [
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, L, periodic, v0, beta, A_CF, A_CIL, r, A, A_Macdonald):\n",
    "        super().__init__()\n",
    "        self.L = torch.tensor(L, device=device)\n",
    "        self.periodic = periodic\n",
    "        self.v0 = nn.Parameter(torch.tensor([[v0]], requires_grad=True, device=device))\n",
    "        self.beta = nn.Parameter(torch.tensor([[beta]], requires_grad=True, device=device))\n",
    "        self.A_CF = nn.Parameter(torch.unsqueeze(torch.tensor(A_CF, requires_grad=True, device=device), 1))\n",
    "        self.A_CIL = A_CIL\n",
    "        self.r = torch.tensor(r, device=device)\n",
    "        self.A = A\n",
    "        self.A_Macdonald = nn.Parameter(torch.unsqueeze(torch.tensor(A_Macdonald, requires_grad=True, device=device), 1))\n",
    "        \n",
    "    def forward(self, x, ct):\n",
    "        xy = x[:, :2]\n",
    "#        if self.periodic:\n",
    "#            xy = xy % L\n",
    "        xy = xy2distance(xy, self.L)\n",
    "#        print(np.shape(xy))\n",
    "#        print(xy[0].shape)\n",
    "        xy = torch.cat((torch.unsqueeze(xy[0], 2), torch.unsqueeze(xy[1], 2)), 2)\n",
    "        d = torch.norm(xy, p='fro', dim=2, keepdim=True)   # distance\n",
    "        dr = 0.5 + (torch.atan((self.r - d) * 100) / (np.pi))   # 1 if distance < r, else 0\n",
    "        xy = dr * torch.nn.functional.normalize(xy, p=2.0, dim=2)   # normalized distance vector\n",
    "        c = torch.cos(x[:, 2:])\n",
    "        s = torch.sin(x[:, 2:])\n",
    "\n",
    "        jcil = J_CIL(xy, d, self.r)\n",
    "        \n",
    "        jchem = J_chemMacdonald(xy, d)\n",
    "        \n",
    "        dx0 = self.v0 * torch.cat((c, s), 1)\n",
    "        dx1 = -self.beta * jcil\n",
    "        dtheta0 = torch.index_select(self.A_CF, 0, ct[:,0]) * J_CF(xy, dr, [c,s]) - self.A_CIL * jcil\n",
    "        dtheta0 = c * dtheta0[:, 1:] - s * dtheta0[:, :1]\n",
    "        dtheta1 = self.A * c\n",
    "        dtheta2 = torch.index_select(self.A_Macdonald, 0, ct[:,0]) * (c * jchem[:, 1:] - s * jchem[:, :1])\n",
    "        \n",
    "        return torch.cat((x[:,:2]+dx0+dx1, x[:,2:]+dtheta0+dtheta1+dtheta2), 1)  # shape (batch_size, state_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "b52c1e60",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-18T14:49:27.115496Z",
     "start_time": "2023-01-18T14:49:27.107358Z"
    }
   },
   "outputs": [],
   "source": [
    "def myLoss(out, target):\n",
    "    #dv = torch.sum(torch.square(out[..., :xy_dim] - target[..., :xy_dim]), dim=-1)\n",
    "    dv = torch.sum(torch.square(calc_dr(out[..., :xy_dim], target[..., :xy_dim])), dim=-1)\n",
    "    dcos = torch.cos(out[..., xy_dim] - target[..., xy_dim])\n",
    "    \n",
    "    wei_shape = np.ones([dv.dim()], dtype=int)\n",
    "    wei_shape[0] = T_pred\n",
    "    wei = torch.tensor(np.reshape(1/np.arange(1, T_pred+1), wei_shape)).to(dv.device)\n",
    "    wei = wei/wei.mean()\n",
    "    \n",
    "    return torch.mean(dv*wei), torch.mean((1-dcos)*wei)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "6033283c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-19T00:42:37.053751Z",
     "start_time": "2023-01-18T14:49:27.126642Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_18371/2533986396.py:20: RuntimeWarning: invalid value encountered in multiply\n",
      "  result = numpy_go * grad_kn\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 | train Loss: [nan, nan] | valid Loss: [nan, nan]\n",
      "Epoch 1 | train Loss: [nan, nan] | valid Loss: [nan, nan]\n",
      "Epoch 2 | train Loss: [nan, nan] | valid Loss: [nan, nan]\n",
      "Epoch 3 | train Loss: [nan, nan] | valid Loss: [nan, nan]\n",
      "Epoch 4 | train Loss: [nan, nan] | valid Loss: [nan, nan]\n",
      "Epoch 00006: reducing learning rate of group 0 to 1.5000e-07.\n",
      "Epoch 5 | train Loss: [nan, nan] | valid Loss: [nan, nan]\n",
      "Epoch 6 | train Loss: [nan, nan] | valid Loss: [nan, nan]\n",
      "Epoch 7 | train Loss: [nan, nan] | valid Loss: [nan, nan]\n",
      "Epoch 8 | train Loss: [nan, nan] | valid Loss: [nan, nan]\n",
      "Epoch 9 | train Loss: [nan, nan] | valid Loss: [nan, nan]\n",
      "Epoch 10 | train Loss: [nan, nan] | valid Loss: [nan, nan]\n",
      "Epoch 00012: reducing learning rate of group 0 to 7.5000e-08.\n",
      "Epoch 11 | train Loss: [nan, nan] | valid Loss: [nan, nan]\n",
      "Epoch 12 | train Loss: [nan, nan] | valid Loss: [nan, nan]\n",
      "Epoch 13 | train Loss: [nan, nan] | valid Loss: [nan, nan]\n",
      "Epoch 14 | train Loss: [nan, nan] | valid Loss: [nan, nan]\n",
      "Epoch 15 | train Loss: [nan, nan] | valid Loss: [nan, nan]\n",
      "Epoch 16 | train Loss: [nan, nan] | valid Loss: [nan, nan]\n",
      "Epoch 00018: reducing learning rate of group 0 to 3.7500e-08.\n",
      "Epoch 17 | train Loss: [nan, nan] | valid Loss: [nan, nan]\n",
      "Epoch 18 | train Loss: [nan, nan] | valid Loss: [nan, nan]\n",
      "Epoch 19 | train Loss: [nan, nan] | valid Loss: [nan, nan]\n",
      "Epoch 20 | train Loss: [nan, nan] | valid Loss: [nan, nan]\n",
      "Epoch 21 | train Loss: [nan, nan] | valid Loss: [nan, nan]\n",
      "Epoch 22 | train Loss: [nan, nan] | valid Loss: [nan, nan]\n",
      "Epoch 00024: reducing learning rate of group 0 to 1.8750e-08.\n",
      "Epoch 23 | train Loss: [nan, nan] | valid Loss: [nan, nan]\n",
      "Epoch 24 | train Loss: [nan, nan] | valid Loss: [nan, nan]\n",
      "Epoch 25 | train Loss: [nan, nan] | valid Loss: [nan, nan]\n",
      "Epoch 26 | train Loss: [nan, nan] | valid Loss: [nan, nan]\n",
      "Epoch 27 | train Loss: [nan, nan] | valid Loss: [nan, nan]\n",
      "Epoch 28 | train Loss: [nan, nan] | valid Loss: [nan, nan]\n",
      "Epoch 29 | train Loss: [nan, nan] | valid Loss: [nan, nan]\n",
      "Epoch 30 | train Loss: [nan, nan] | valid Loss: [nan, nan]\n",
      "Epoch 31 | train Loss: [nan, nan] | valid Loss: [nan, nan]\n",
      "Epoch 32 | train Loss: [nan, nan] | valid Loss: [nan, nan]\n",
      "Epoch 33 | train Loss: [nan, nan] | valid Loss: [nan, nan]\n",
      "Epoch 34 | train Loss: [nan, nan] | valid Loss: [nan, nan]\n",
      "Epoch 35 | train Loss: [nan, nan] | valid Loss: [nan, nan]\n",
      "Epoch 36 | train Loss: [nan, nan] | valid Loss: [nan, nan]\n",
      "Epoch 37 | train Loss: [nan, nan] | valid Loss: [nan, nan]\n",
      "Epoch 38 | train Loss: [nan, nan] | valid Loss: [nan, nan]\n",
      "Epoch 39 | train Loss: [nan, nan] | valid Loss: [nan, nan]\n",
      "Epoch 40 | train Loss: [nan, nan] | valid Loss: [nan, nan]\n",
      "Epoch 41 | train Loss: [nan, nan] | valid Loss: [nan, nan]\n",
      "Epoch 42 | train Loss: [nan, nan] | valid Loss: [nan, nan]\n",
      "Epoch 43 | train Loss: [nan, nan] | valid Loss: [nan, nan]\n",
      "Epoch 44 | train Loss: [nan, nan] | valid Loss: [nan, nan]\n",
      "Epoch 45 | train Loss: [nan, nan] | valid Loss: [nan, nan]\n",
      "Epoch 46 | train Loss: [nan, nan] | valid Loss: [nan, nan]\n",
      "Epoch 47 | train Loss: [nan, nan] | valid Loss: [nan, nan]\n",
      "Epoch 48 | train Loss: [nan, nan] | valid Loss: [nan, nan]\n",
      "Epoch 49 | train Loss: [nan, nan] | valid Loss: [nan, nan]\n"
     ]
    }
   ],
   "source": [
    "# モデルのインスタンス生成\n",
    "xy_dim = 2\n",
    "\n",
    "model = NeuralNet(params['L'],\n",
    "                  params['periodic'], \n",
    "                  0., 0., [0.,0.], 0., params['r'], 0., [0.,0.]).to(device)\n",
    "# input data\n",
    "#data = dataset[0]\n",
    "\n",
    "def calc_multiSteps(x0, ct):\n",
    "    outs = []\n",
    "    x_i = x0\n",
    "    for i_step in range(T_pred):\n",
    "        x_i = x_i + model(x_i, ct.int()) * dt\n",
    "        outs.append(x_i.clone())\n",
    "    return torch.stack(outs, dim=0)\n",
    "\n",
    "# optimizer\n",
    "#optimizer = torch.optim.SGD(model.parameters(), lr=1e-4, momentum=0.9)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-7)#, weight_decay=5e-4)\n",
    "#optimizer = torch.optim.Adadelta(model.parameters())#, rho=0.95)#, lr=1e-1, momentum=0.9)\n",
    "\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min', factor=0.5, patience=5, verbose=True)\n",
    "\n",
    "val_loss_log = []\n",
    "\n",
    "val_loss_min = np.Inf\n",
    "\n",
    "# learnig loop\n",
    "for epoch in range(50):\n",
    "    model.train()\n",
    "    for batch_x, batch_y, batch_ct in train_data:\n",
    "        optimizer.zero_grad()\n",
    "        lossv = 0\n",
    "        losstheta = 0\n",
    "        for ib in range(batch_x.size(0)):\n",
    "            out = calc_multiSteps(batch_x[ib].to(device), batch_ct[ib].to(device))\n",
    "            lv, ltheta = myLoss(out, batch_y[ib].to(device))\n",
    "            lossv = lossv + lv\n",
    "            losstheta = losstheta + ltheta\n",
    "        lossv = lossv / batch_x.size(0)\n",
    "        losstheta = losstheta / batch_x.size(0)\n",
    "        (lossv+losstheta).backward()\n",
    "        optimizer.step()\n",
    "    model.eval()\n",
    "    val_lossv = 0\n",
    "    val_losstheta = 0\n",
    "    val_count = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_y, batch_ct in valid_data:\n",
    "            for ib in range(batch_x.size(0)):\n",
    "                val_out = calc_multiSteps(batch_x[ib].to(device), batch_ct[ib].to(device))\n",
    "                lv, ltheta = myLoss(val_out, batch_y[ib].to(device))\n",
    "                val_lossv = val_lossv + lv\n",
    "                val_losstheta = val_losstheta + ltheta\n",
    "            val_count = val_count + batch_x.size(0)\n",
    "    val_lossv = val_lossv/val_count\n",
    "    val_losstheta = val_losstheta/val_count\n",
    "    val_loss = val_lossv + val_losstheta\n",
    "    scheduler.step(val_loss)\n",
    "    print('Epoch %d | train Loss: [%.4f, %.4f] | valid Loss: [%.4f, %.4f]' % (epoch,\n",
    "                                                                              lossv.item(), \n",
    "                                                                              losstheta.item(),\n",
    "                                                                              val_lossv.item(), \n",
    "                                                                              val_losstheta.item()))\n",
    "    val_loss_log.append([val_lossv.cpu().item(), val_losstheta.cpu().item()])\n",
    "    if val_loss.item() < val_loss_min:\n",
    "        stored_model = model\n",
    "        val_loss_min = val_loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "605ebb18",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-19T00:42:37.063085Z",
     "start_time": "2023-01-19T00:42:37.056887Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 400, 3])\n",
      "torch.Size([6, 3, 400, 3])\n",
      "torch.Size([6, 400, 1])\n"
     ]
    }
   ],
   "source": [
    "print(batch_x.shape)\n",
    "print(batch_y.shape)\n",
    "print(batch_ct.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "b3cf1e91",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-19T00:42:37.072497Z",
     "start_time": "2023-01-19T00:42:37.066081Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('v0', tensor([[nan]], device='cuda:1')),\n",
       "             ('beta', tensor([[nan]], device='cuda:1')),\n",
       "             ('A_CF',\n",
       "              tensor([[nan],\n",
       "                      [nan]], device='cuda:1')),\n",
       "             ('A_Macdonald',\n",
       "              tensor([[nan],\n",
       "                      [nan]], device='cuda:1'))])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "44265599",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-19T00:42:37.100834Z",
     "start_time": "2023-01-19T00:42:37.074600Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stored_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [101]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# モデルを評価モードに設定\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mstored_model\u001b[49m\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# 推論\u001b[39;00m\n\u001b[1;32m      5\u001b[0m test_lossv \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'stored_model' is not defined"
     ]
    }
   ],
   "source": [
    "# モデルを評価モードに設定\n",
    "stored_model.eval()\n",
    "\n",
    "# 推論\n",
    "test_lossv = 0\n",
    "test_losstheta = 0\n",
    "test_count = 0\n",
    "with torch.no_grad():\n",
    "    for batch_x, batch_y, batch_ct in test_data:\n",
    "        for ib in range(batch_x.size(0)):\n",
    "            test_out = calc_multiSteps(batch_x[ib].to(device), batch_ct[ib].to(device))\n",
    "            lv, ltheta = myLoss(test_out, batch_y[ib].to(device))\n",
    "            test_lossv = test_lossv + lv\n",
    "            test_losstheta = test_losstheta + ltheta\n",
    "        test_count = test_count + batch_x.size(0)\n",
    "test_lossv = test_lossv/test_count\n",
    "test_losstheta = test_losstheta/test_count\n",
    "print('test Loss: [%.4f, %.4f]' % (test_lossv.item(), test_losstheta.item()))\n",
    "test_loss = [test_lossv.item(), test_losstheta.item()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0283033",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-19T00:42:37.105222Z",
     "start_time": "2023-01-19T00:42:37.105204Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import datetime\n",
    "\n",
    "now = datetime.datetime.now()\n",
    "nowstr = now.strftime('%Y%m%d_%H%M%S')\n",
    "\n",
    "os.makedirs(savedirName + nowstr + '/', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445915ef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-19T00:42:37.106690Z",
     "start_time": "2023-01-19T00:42:37.106673Z"
    }
   },
   "outputs": [],
   "source": [
    "stored_model.selfpropel.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62154d31",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-19T00:42:37.108599Z",
     "start_time": "2023-01-19T00:42:37.108578Z"
    }
   },
   "outputs": [],
   "source": [
    "stored_model = stored_model.to('cpu')\n",
    "\n",
    "filename1 = savedirName + nowstr + '/' + nowstr + '_Model.pkl'\n",
    "with open(filename1, \"wb\") as f:\n",
    "    pickle.dump(stored_model, f)\n",
    "\n",
    "filename1_2 = savedirName + nowstr + '/' + nowstr + '_Model.pt'\n",
    "torch.save(stored_model, filename1_2)\n",
    "\n",
    "filename2 = savedirName + nowstr + '/' + nowstr\n",
    "torch.save(stored_model.interactNN.state_dict(), filename2 + '_interactNN.pkl')\n",
    "torch.save(stored_model.thetaDotNN.state_dict(), filename2 + '_thetaDotNN.pkl')\n",
    "torch.save(stored_model.selfpropel.detach(), filename2 + '_selfpropel.pkl')\n",
    "\n",
    "filename3 = savedirName + nowstr + '/' + nowstr + '_Separation.npz'\n",
    "np.savez(filename3, dr_thresh=dr_thresh, T_pred=T_pred, batch_size=batch_size,\n",
    "         train_inds=train_inds, valid_inds=valid_inds, test_inds=test_inds, \n",
    "         val_loss_log=val_loss_log, test_loss=test_loss)\n",
    "\n",
    "filename4 = savedirName + nowstr + '/' + nowstr + '_optimizer.pt'\n",
    "torch.save(optimizer, filename4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0bd69a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 249.41666600000002,
   "position": {
    "height": "185.833px",
    "left": "554px",
    "right": "20px",
    "top": "120px",
    "width": "443.833px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "none",
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
