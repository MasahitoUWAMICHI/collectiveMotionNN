{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "f855b76a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-03T14:53:45.935766Z",
     "start_time": "2023-03-03T14:53:45.933059Z"
    }
   },
   "outputs": [],
   "source": [
    "#!pip install dgl-cu113 dglgo -f https://data.dgl.ai/wheels/repo.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "7807020c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-03T14:53:45.943210Z",
     "start_time": "2023-03-03T14:53:45.937476Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['DGLBACKEND'] = 'pytorch'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "d12568c6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-03T14:53:45.949012Z",
     "start_time": "2023-03-03T14:53:45.944988Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "from gradient_descent_the_ultimate_optimizer import gdtuo\n",
    "\n",
    "import dgl\n",
    "import dgl.function as fn\n",
    "\n",
    "import networkx as nx\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import gc\n",
    "\n",
    "# デバイス設定\n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "61e1f3ee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-03T14:53:45.955262Z",
     "start_time": "2023-03-03T14:53:45.950807Z"
    }
   },
   "outputs": [],
   "source": [
    "def printNPZ(npz):\n",
    "    for kw in npz.files:\n",
    "        print(kw, npz[kw])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "0ba5ba52",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-03T14:53:45.980350Z",
     "start_time": "2023-03-03T14:53:45.971224Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v0 1.0\n",
      "r 1.0\n",
      "D 0.1\n",
      "A 0.0\n",
      "L 20\n",
      "rho 1.0\n",
      "beta 1.0\n",
      "A_CFs [0.9 0.5]\n",
      "A_CIL 0.0\n",
      "cellType_ratio [0.7 0.3]\n",
      "quiv_colors ['k' 'r']\n",
      "kappa 0.5\n",
      "A_Macdonalds [0.5 0.5]\n",
      "batch_size 400\n",
      "state_size 3\n",
      "brownian_size 1\n",
      "periodic True\n",
      "t_max 1000\n",
      "methodSDE heun\n",
      "isIto False\n",
      "stepSDE 0.01\n"
     ]
    }
   ],
   "source": [
    "dirName = '/home/uwamichi/jupyter/HiraiwaModel_chem20220922_180005/'\n",
    "savedirName = dirName + 'ActiveNet_vp_rotsym_noSelfLoop_batchNorm/'\n",
    "os.makedirs(savedirName, exist_ok=True)\n",
    "\n",
    "params = np.load(dirName+'params.npz')\n",
    "#traj = np.load(dirName+'result.npz')\n",
    "\n",
    "printNPZ(params)\n",
    "#printNPZ(traj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "c4e73591",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-03T14:53:45.988425Z",
     "start_time": "2023-03-03T14:53:45.982323Z"
    }
   },
   "outputs": [],
   "source": [
    "if params['periodic']:\n",
    "    L = torch.tensor(params['L'])\n",
    "    def calc_dr(r1, r2):\n",
    "        dr = torch.remainder((r1 - r2), L)\n",
    "        dr[dr > L/2] = dr[dr > L/2] - L\n",
    "        return dr\n",
    "else:\n",
    "    def calc_dr(r1, r2):\n",
    "        return r1 - r2\n",
    "    \n",
    "def makeGraph(x_data, r_thresh):\n",
    "        Ndata = x_data.size(0)\n",
    "        dx = calc_dr(torch.unsqueeze(x_data, 0), torch.unsqueeze(x_data, 1))\n",
    "        dx = torch.sum(dx**2, dim=2)\n",
    "        edges = torch.argwhere(torch.logical_and(dx > 0, dx < r_thresh/2))\n",
    "        return dgl.graph((edges[:,0], edges[:,1]), num_nodes=Ndata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "e3e01d8a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-03T14:53:45.997086Z",
     "start_time": "2023-03-03T14:53:45.990585Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/uwamichi/jupyter/HiraiwaModel_chem20220922_180005/20221026_220625', '/home/uwamichi/jupyter/HiraiwaModel_chem20220922_180005/20221027_020808', '/home/uwamichi/jupyter/HiraiwaModel_chem20220922_180005/20221027_061543', '/home/uwamichi/jupyter/HiraiwaModel_chem20220922_180005/20221027_102254', '/home/uwamichi/jupyter/HiraiwaModel_chem20220922_180005/20221027_144800', '/home/uwamichi/jupyter/HiraiwaModel_chem20220922_180005/ActiveNet_vp_rotsym_batchNorm', '/home/uwamichi/jupyter/HiraiwaModel_chem20220922_180005/20221027_190650', '/home/uwamichi/jupyter/HiraiwaModel_chem20220922_180005/ActiveNet_vp_rotsym_multiStep_transfer_batchNorm', '/home/uwamichi/jupyter/HiraiwaModel_chem20220922_180005/ActiveNet_vp_rotsym_multiStep_fineTuning_batchNorm', '/home/uwamichi/jupyter/HiraiwaModel_chem20220922_180005/20221114_163735', '/home/uwamichi/jupyter/HiraiwaModel_chem20220922_180005/20221114_205206', '/home/uwamichi/jupyter/HiraiwaModel_chem20220922_180005/20221115_004702', '/home/uwamichi/jupyter/HiraiwaModel_chem20220922_180005/20221115_044206', '/home/uwamichi/jupyter/HiraiwaModel_chem20220922_180005/ActiveNet_vp_rotsym_noSelfLoop_batchNorm']\n",
      "['/home/uwamichi/jupyter/HiraiwaModel_chem20220922_180005/20221026_220625', '/home/uwamichi/jupyter/HiraiwaModel_chem20220922_180005/20221027_020808', '/home/uwamichi/jupyter/HiraiwaModel_chem20220922_180005/20221027_061543', '/home/uwamichi/jupyter/HiraiwaModel_chem20220922_180005/20221027_102254', '/home/uwamichi/jupyter/HiraiwaModel_chem20220922_180005/20221027_144800', '/home/uwamichi/jupyter/HiraiwaModel_chem20220922_180005/20221027_190650', '/home/uwamichi/jupyter/HiraiwaModel_chem20220922_180005/20221114_163735', '/home/uwamichi/jupyter/HiraiwaModel_chem20220922_180005/20221114_205206', '/home/uwamichi/jupyter/HiraiwaModel_chem20220922_180005/20221115_004702', '/home/uwamichi/jupyter/HiraiwaModel_chem20220922_180005/20221115_044206']\n"
     ]
    }
   ],
   "source": [
    "subdir_list = [f.path for f in os.scandir(dirName) if f.is_dir()]\n",
    "\n",
    "print(subdir_list)\n",
    "\n",
    "datadir_list = [f for f in subdir_list if 'result.npz' in [ff.name for ff in os.scandir(f)]]\n",
    "\n",
    "print(datadir_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "3e9e2f01",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-03T14:53:48.123008Z",
     "start_time": "2023-03-03T14:53:45.999163Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "396"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dr_thresh = 4\n",
    "dt = 1\n",
    "batch_size = 8\n",
    "\n",
    "N_data = len(datadir_list)\n",
    "\n",
    "#TR_VA_rate = np.array([0.6, 0.2])\n",
    "\n",
    "TR_last = 5\n",
    "VA_last = 7\n",
    "\n",
    "shuffle_inds = np.arange(N_data, dtype=int)\n",
    "np.random.shuffle(shuffle_inds)\n",
    "\n",
    "train_inds = shuffle_inds[:TR_last]\n",
    "valid_inds = shuffle_inds[TR_last:VA_last]\n",
    "test_inds = shuffle_inds[VA_last:]\n",
    "\n",
    "celltype_lst = []\n",
    "\n",
    "train_x = []\n",
    "valid_x = []\n",
    "test_x = []\n",
    "\n",
    "train_y = []\n",
    "valid_y = []\n",
    "test_y = []\n",
    "\n",
    "train_i_dir = []\n",
    "valid_i_dir = []\n",
    "test_i_dir = []\n",
    "\n",
    "for i_dir, subdirName in enumerate(datadir_list):\n",
    "    \n",
    "    traj = np.load(subdirName+'/result.npz')\n",
    "    \n",
    "    celltype_lst.append(torch.tensor(traj['celltype_label']).view(-1,1))\n",
    "\n",
    "    xy_t = torch.tensor(traj['xy'][:-1,:,:])\n",
    "    v_t = calc_dr(torch.tensor(traj['xy'][1:,:,:]), torch.tensor(traj['xy'][:-1,:,:])) / dt\n",
    "    p_t = torch.unsqueeze(torch.tensor(traj['theta'][:-1,:]), dim=2)\n",
    "    w_t = torch.unsqueeze(torch.tensor((traj['theta'][1:,:]-traj['theta'][:-1,:])%(2*np.pi)), dim=2)\n",
    "    \n",
    "    if i_dir in train_inds:\n",
    "        train_x.append(torch.concat((xy_t, p_t), -1))\n",
    "        train_y.append(torch.concat((v_t, w_t), -1))\n",
    "        train_i_dir.append(torch.ones([xy_t.size(0)])*i_dir)\n",
    "\n",
    "    if i_dir in valid_inds:\n",
    "        valid_x.append(torch.concat((xy_t, p_t), -1))\n",
    "        valid_y.append(torch.concat((v_t, w_t), -1))\n",
    "        valid_i_dir.append(torch.ones([xy_t.size(0)])*i_dir)\n",
    "        \n",
    "    if i_dir in test_inds:\n",
    "        test_x.append(torch.concat((xy_t, p_t), -1))\n",
    "        test_y.append(torch.concat((v_t, w_t), -1))\n",
    "        test_i_dir.append(torch.ones([xy_t.size(0)])*i_dir)\n",
    "    \n",
    "train_dataset = torch.utils.data.TensorDataset(\n",
    "    torch.concat(train_x, 0), \n",
    "    torch.concat(train_y, 0), \n",
    "    torch.concat(train_i_dir, 0))\n",
    "\n",
    "valid_dataset = torch.utils.data.TensorDataset(\n",
    "    torch.concat(valid_x, 0), \n",
    "    torch.concat(valid_y, 0), \n",
    "    torch.concat(valid_i_dir, 0))\n",
    "\n",
    "test_dataset = torch.utils.data.TensorDataset(\n",
    "    torch.concat(test_x, 0), \n",
    "    torch.concat(test_y, 0), \n",
    "    torch.concat(test_i_dir, 0))\n",
    "\n",
    "train_data = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, pin_memory=True)\n",
    "valid_data = torch.utils.data.DataLoader(valid_dataset, batch_size=batch_size, pin_memory=True)\n",
    "test_data = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, pin_memory=True)\n",
    "\n",
    "del train_x, train_y, train_i_dir, train_dataset\n",
    "del valid_x, valid_y, valid_i_dir, valid_dataset\n",
    "del test_x, test_y, test_i_dir, test_dataset\n",
    "gc.collect()\n",
    "\n",
    "#print(data)\n",
    "#print(data.num_graphs)\n",
    "#print(data.x)\n",
    "#print(data.y)\n",
    "#print(data.edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "1c0a633b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-03T14:53:48.136897Z",
     "start_time": "2023-03-03T14:53:48.131681Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5 9 3 7 2]\n"
     ]
    }
   ],
   "source": [
    "print(train_inds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "4f35452f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-03T14:53:48.150127Z",
     "start_time": "2023-03-03T14:53:48.141746Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_DataLoader__initialized',\n",
       " '_DataLoader__multiprocessing_context',\n",
       " '_IterableDataset_len_called',\n",
       " '__annotations__',\n",
       " '__class__',\n",
       " '__class_getitem__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__iter__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__orig_bases__',\n",
       " '__parameters__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__slots__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_auto_collation',\n",
       " '_dataset_kind',\n",
       " '_get_iterator',\n",
       " '_get_shared_seed',\n",
       " '_index_sampler',\n",
       " '_is_protocol',\n",
       " '_iterator',\n",
       " 'batch_sampler',\n",
       " 'batch_size',\n",
       " 'check_worker_number_rationality',\n",
       " 'collate_fn',\n",
       " 'dataset',\n",
       " 'drop_last',\n",
       " 'generator',\n",
       " 'multiprocessing_context',\n",
       " 'num_workers',\n",
       " 'persistent_workers',\n",
       " 'pin_memory',\n",
       " 'pin_memory_device',\n",
       " 'prefetch_factor',\n",
       " 'sampler',\n",
       " 'timeout',\n",
       " 'worker_init_fn']"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "f94ee7f4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-03T14:53:48.174099Z",
     "start_time": "2023-03-03T14:53:48.163254Z"
    }
   },
   "outputs": [],
   "source": [
    "def plotGraph(data):\n",
    "\n",
    "    # networkxのグラフに変換\n",
    "    nxg = dgl.to_networkx(data)\n",
    "\n",
    "    # 可視化のためのページランク計算\n",
    "    pr = nx.pagerank(nxg)\n",
    "    pr_max = np.array(list(pr.values())).max()\n",
    "\n",
    "    # 可視化する際のノード位置\n",
    "    draw_pos = nx.spring_layout(nxg, seed=0) \n",
    "\n",
    "    # ノードの色設定\n",
    "    cmap = plt.get_cmap('tab10')\n",
    "    labels = data.y.numpy()\n",
    "    colors = [cmap(l) for l in labels]\n",
    "\n",
    "    # 図のサイズ\n",
    "    fig0 = plt.figure(figsize=(10, 10))\n",
    "\n",
    "    # 描画\n",
    "    nx.draw_networkx_nodes(nxg, \n",
    "                          draw_pos,\n",
    "                          node_size=[v / pr_max * 1000 for v in pr.values()])#,\n",
    "                          #node_color=colors, alpha=0.5)\n",
    "    nx.draw_networkx_edges(nxg, draw_pos, arrowstyle='-', alpha=0.2)\n",
    "    nx.draw_networkx_labels(nxg, draw_pos, font_size=10)\n",
    "\n",
    "    #plt.title('KarateClub')\n",
    "    plt.show()\n",
    "\n",
    "    return fig0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "21e97796",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-03T14:53:48.226109Z",
     "start_time": "2023-03-03T14:53:48.187099Z"
    }
   },
   "outputs": [],
   "source": [
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, Nchannels, dropout=0, batchN=False, flgBias=False):\n",
    "        super(NeuralNet, self).__init__()\n",
    "\n",
    "        if dropout:\n",
    "            self.dropout = nn.Dropout(p=dropout)\n",
    "        else:\n",
    "            self.dropout = 0\n",
    "            \n",
    "        if batchN:\n",
    "            self.bNorm1 = nn.BatchNorm1d(Nchannels)\n",
    "            self.bNorm2 = nn.BatchNorm1d(Nchannels)\n",
    "            self.bNorm3 = nn.BatchNorm1d(Nchannels)\n",
    "            \n",
    "        self.batchN=batchN\n",
    "        \n",
    "        self.layer1 = nn.Linear(in_channels, Nchannels, bias=flgBias)\n",
    "        self.layer2 = nn.Linear(Nchannels, Nchannels, bias=flgBias)\n",
    "        self.layer3 = nn.Linear(Nchannels, Nchannels, bias=flgBias)\n",
    "        self.layer4 = nn.Linear(Nchannels, out_channels, bias=flgBias)\n",
    "\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.layer1.reset_parameters()\n",
    "        self.layer2.reset_parameters()\n",
    "        self.layer3.reset_parameters()\n",
    "        self.layer4.reset_parameters()\n",
    "        #nn.init.zeros_(self.layer1.weight)\n",
    "        #nn.init.zeros_(self.layer2.weight)\n",
    "        #nn.init.zeros_(self.layer3.weight)\n",
    "        #nn.init.zeros_(self.layer4.weight)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.activation(self.layer1(x))\n",
    "        if self.batchN:\n",
    "            out = self.bNorm1(out)\n",
    "        if self.dropout:\n",
    "            out = self.dropout(out)\n",
    "        \n",
    "        out = self.activation(self.layer2(out))\n",
    "        if self.batchN:\n",
    "            out = self.bNorm2(out)\n",
    "        if self.dropout:\n",
    "            out = self.dropout(out)\n",
    "        \n",
    "        out = self.activation(self.layer3(out))\n",
    "        if self.batchN:\n",
    "            out = self.bNorm3(out)\n",
    "        if self.dropout:\n",
    "            out = self.dropout(out)\n",
    "        \n",
    "        out = self.layer4(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "class ActiveNet(nn.Module):\n",
    "    def __init__(self, xy_dim, r, dropout=0, batchN=False, bias=False, Nchannels=128):\n",
    "        super().__init__()\n",
    "\n",
    "        self.interactNN = NeuralNet(xy_dim*2 + 2, xy_dim, Nchannels, dropout, batchN, bias)\n",
    "\n",
    "        self.thetaDotNN = NeuralNet(xy_dim*2 + 2, 1, Nchannels, dropout, batchN, bias)\n",
    "        \n",
    "        self.selfpropel = nn.Parameter(torch.tensor(0.0, requires_grad=True, device=device))\n",
    "\n",
    "        #self.Normalizer = nn.Softmax(dim=1)\n",
    "\n",
    "        self.xy_dim = xy_dim\n",
    "        \n",
    "        self.r = r\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.interactNN.reset_parameters()\n",
    "\n",
    "        self.thetaDotNN.reset_parameters()\n",
    "        \n",
    "        nn.init.uniform_(self.selfpropel)\n",
    "\n",
    "        #self.bias.data.zero_()\n",
    "        \n",
    "    def load_celltypes(self, celltype):\n",
    "        self.celltype = celltype\n",
    "\n",
    "    def calc_message(self, edges):\n",
    "        dx = calc_dr(edges.dst['x'], edges.src['x'])\n",
    "\n",
    "        costheta = torch.cos(edges.dst['theta'])\n",
    "        sintheta = torch.sin(edges.dst['theta'])\n",
    "\n",
    "        dx_para = costheta * dx[..., :1] + sintheta * dx[..., 1:]\n",
    "        dx_perp = costheta * dx[..., 1:] - sintheta * dx[..., :1]\n",
    "\n",
    "        p_para_src = torch.cos(edges.src['theta'] - edges.dst['theta'])\n",
    "        p_perp_src = torch.sin(edges.src['theta'] - edges.dst['theta'])\n",
    "\n",
    "        rot_m_v = self.interactNN(torch.concat((dx_para, dx_perp, \n",
    "                                                p_para_src, p_perp_src,\n",
    "                                                edges.dst['type'], edges.src['type']), -1))\n",
    "\n",
    "        m_v = torch.concat((costheta * rot_m_v[..., :1] - sintheta * rot_m_v[..., 1:],\n",
    "                            costheta * rot_m_v[..., 1:] + sintheta * rot_m_v[..., :1]), -1)\n",
    "\n",
    "        m_theta = self.thetaDotNN(torch.concat((dx_para, dx_perp, \n",
    "                                                p_para_src, p_perp_src, \n",
    "                                                edges.dst['type'], edges.src['type']), -1))\n",
    "        \n",
    "        return {'m': torch.concat((m_v, m_theta), -1)}\n",
    "        \n",
    "    def forward(self, xv):\n",
    "        r_g = makeGraph(xv[..., :self.xy_dim], self.r/2)\n",
    "        r_g.ndata['x'] = xv[..., :self.xy_dim]\n",
    "        r_g.ndata['theta'] = xv[..., self.xy_dim:(self.xy_dim+1)]\n",
    "        r_g.ndata['type'] = self.celltype\n",
    "        r_g.update_all(self.calc_message, fn.sum('m', 'a'))\n",
    "        r_g.ndata['a'][..., :self.xy_dim] = r_g.ndata['a'][..., :self.xy_dim] + self.selfpropel * torch.concat((torch.cos(r_g.ndata['theta']), torch.sin(r_g.ndata['theta'])), -1)\n",
    "        r_g.ndata['a'][..., self.xy_dim:] = r_g.ndata['a'][..., self.xy_dim:]# + r_g.ndata['theta']\n",
    "        \n",
    "        return r_g.ndata['a']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "b52c1e60",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-03T14:53:48.235217Z",
     "start_time": "2023-03-03T14:53:48.231138Z"
    }
   },
   "outputs": [],
   "source": [
    "def myLoss(out, target):\n",
    "    dv = torch.sum(torch.square(out[..., :xy_dim] - target[..., :xy_dim]), dim=-1)\n",
    "    dcos = torch.cos(out[..., xy_dim] - target[..., xy_dim])\n",
    "    return torch.mean(dv), 1 - torch.mean(dcos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "6033283c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-03T17:24:28.540585Z",
     "start_time": "2023-03-03T14:53:48.241216Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 | train Loss: [0.0975, 0.0471] | valid Loss: [0.1033, 0.0567]\n",
      "Epoch 1 | train Loss: [0.0911, 0.0460] | valid Loss: [0.0965, 0.0548]\n",
      "Epoch 2 | train Loss: [0.0883, 0.0458] | valid Loss: [0.0936, 0.0543]\n",
      "Epoch 3 | train Loss: [0.0865, 0.0458] | valid Loss: [0.0916, 0.0540]\n",
      "Epoch 4 | train Loss: [0.0853, 0.0457] | valid Loss: [0.0900, 0.0539]\n",
      "Epoch 5 | train Loss: [0.0841, 0.0457] | valid Loss: [0.0884, 0.0538]\n",
      "Epoch 6 | train Loss: [0.0829, 0.0456] | valid Loss: [0.0870, 0.0537]\n",
      "Epoch 7 | train Loss: [0.0818, 0.0456] | valid Loss: [0.0856, 0.0537]\n",
      "Epoch 8 | train Loss: [0.0807, 0.0456] | valid Loss: [0.0842, 0.0536]\n",
      "Epoch 9 | train Loss: [0.0795, 0.0455] | valid Loss: [0.0828, 0.0536]\n",
      "Epoch 10 | train Loss: [0.0784, 0.0455] | valid Loss: [0.0814, 0.0535]\n",
      "Epoch 11 | train Loss: [0.0771, 0.0455] | valid Loss: [0.0798, 0.0535]\n",
      "Epoch 12 | train Loss: [0.0757, 0.0454] | valid Loss: [0.0782, 0.0535]\n",
      "Epoch 13 | train Loss: [0.0740, 0.0454] | valid Loss: [0.0766, 0.0534]\n",
      "Epoch 14 | train Loss: [0.0721, 0.0454] | valid Loss: [0.0748, 0.0534]\n",
      "Epoch 15 | train Loss: [0.0697, 0.0454] | valid Loss: [0.0730, 0.0534]\n",
      "Epoch 16 | train Loss: [0.0669, 0.0453] | valid Loss: [0.0711, 0.0533]\n",
      "Epoch 17 | train Loss: [0.0643, 0.0453] | valid Loss: [0.0692, 0.0533]\n",
      "Epoch 18 | train Loss: [0.0622, 0.0453] | valid Loss: [0.0675, 0.0533]\n",
      "Epoch 19 | train Loss: [0.0591, 0.0453] | valid Loss: [0.0660, 0.0533]\n",
      "Epoch 20 | train Loss: [0.0560, 0.0453] | valid Loss: [0.0635, 0.0532]\n",
      "Epoch 21 | train Loss: [0.0534, 0.0453] | valid Loss: [0.0596, 0.0532]\n",
      "Epoch 22 | train Loss: [0.0506, 0.0452] | valid Loss: [0.0572, 0.0532]\n",
      "Epoch 23 | train Loss: [0.0485, 0.0452] | valid Loss: [0.0549, 0.0532]\n",
      "Epoch 24 | train Loss: [0.0475, 0.0452] | valid Loss: [0.0532, 0.0532]\n",
      "Epoch 25 | train Loss: [0.0448, 0.0452] | valid Loss: [0.0524, 0.0531]\n",
      "Epoch 26 | train Loss: [0.0428, 0.0452] | valid Loss: [0.0516, 0.0531]\n",
      "Epoch 27 | train Loss: [0.0415, 0.0452] | valid Loss: [0.0501, 0.0531]\n",
      "Epoch 28 | train Loss: [0.0403, 0.0452] | valid Loss: [0.0491, 0.0531]\n",
      "Epoch 29 | train Loss: [0.0393, 0.0451] | valid Loss: [0.0484, 0.0531]\n",
      "Epoch 30 | train Loss: [0.0386, 0.0451] | valid Loss: [0.0480, 0.0530]\n",
      "Epoch 31 | train Loss: [0.0381, 0.0451] | valid Loss: [0.0475, 0.0530]\n",
      "Epoch 32 | train Loss: [0.0377, 0.0451] | valid Loss: [0.0470, 0.0530]\n",
      "Epoch 33 | train Loss: [0.0373, 0.0451] | valid Loss: [0.0463, 0.0530]\n",
      "Epoch 34 | train Loss: [0.0370, 0.0451] | valid Loss: [0.0456, 0.0530]\n",
      "Epoch 35 | train Loss: [0.0365, 0.0451] | valid Loss: [0.0449, 0.0530]\n",
      "Epoch 36 | train Loss: [0.0360, 0.0450] | valid Loss: [0.0444, 0.0529]\n",
      "Epoch 37 | train Loss: [0.0356, 0.0450] | valid Loss: [0.0440, 0.0529]\n",
      "Epoch 38 | train Loss: [0.0351, 0.0450] | valid Loss: [0.0436, 0.0529]\n",
      "Epoch 39 | train Loss: [0.0347, 0.0450] | valid Loss: [0.0432, 0.0529]\n",
      "Epoch 40 | train Loss: [0.0343, 0.0450] | valid Loss: [0.0429, 0.0529]\n",
      "Epoch 41 | train Loss: [0.0340, 0.0450] | valid Loss: [0.0427, 0.0528]\n",
      "Epoch 42 | train Loss: [0.0337, 0.0450] | valid Loss: [0.0424, 0.0528]\n",
      "Epoch 43 | train Loss: [0.0334, 0.0450] | valid Loss: [0.0422, 0.0528]\n",
      "Epoch 44 | train Loss: [0.0332, 0.0449] | valid Loss: [0.0420, 0.0528]\n",
      "Epoch 45 | train Loss: [0.0330, 0.0449] | valid Loss: [0.0418, 0.0528]\n",
      "Epoch 46 | train Loss: [0.0328, 0.0449] | valid Loss: [0.0416, 0.0528]\n",
      "Epoch 47 | train Loss: [0.0326, 0.0449] | valid Loss: [0.0415, 0.0527]\n",
      "Epoch 48 | train Loss: [0.0325, 0.0449] | valid Loss: [0.0413, 0.0527]\n",
      "Epoch 49 | train Loss: [0.0323, 0.0449] | valid Loss: [0.0412, 0.0527]\n",
      "Epoch 50 | train Loss: [0.0321, 0.0449] | valid Loss: [0.0411, 0.0527]\n",
      "Epoch 51 | train Loss: [0.0320, 0.0449] | valid Loss: [0.0410, 0.0527]\n",
      "Epoch 52 | train Loss: [0.0319, 0.0448] | valid Loss: [0.0409, 0.0527]\n",
      "Epoch 53 | train Loss: [0.0317, 0.0448] | valid Loss: [0.0408, 0.0526]\n",
      "Epoch 54 | train Loss: [0.0316, 0.0448] | valid Loss: [0.0407, 0.0526]\n",
      "Epoch 55 | train Loss: [0.0315, 0.0448] | valid Loss: [0.0406, 0.0526]\n",
      "Epoch 56 | train Loss: [0.0314, 0.0448] | valid Loss: [0.0405, 0.0526]\n",
      "Epoch 57 | train Loss: [0.0313, 0.0448] | valid Loss: [0.0404, 0.0526]\n",
      "Epoch 58 | train Loss: [0.0312, 0.0448] | valid Loss: [0.0403, 0.0525]\n",
      "Epoch 59 | train Loss: [0.0311, 0.0448] | valid Loss: [0.0403, 0.0525]\n",
      "Epoch 60 | train Loss: [0.0310, 0.0447] | valid Loss: [0.0402, 0.0525]\n",
      "Epoch 61 | train Loss: [0.0309, 0.0447] | valid Loss: [0.0401, 0.0525]\n",
      "Epoch 62 | train Loss: [0.0308, 0.0447] | valid Loss: [0.0401, 0.0525]\n",
      "Epoch 63 | train Loss: [0.0307, 0.0447] | valid Loss: [0.0400, 0.0525]\n",
      "Epoch 64 | train Loss: [0.0306, 0.0447] | valid Loss: [0.0400, 0.0524]\n",
      "Epoch 65 | train Loss: [0.0306, 0.0447] | valid Loss: [0.0399, 0.0524]\n",
      "Epoch 66 | train Loss: [0.0305, 0.0447] | valid Loss: [0.0398, 0.0524]\n",
      "Epoch 67 | train Loss: [0.0304, 0.0446] | valid Loss: [0.0398, 0.0524]\n",
      "Epoch 68 | train Loss: [0.0304, 0.0446] | valid Loss: [0.0398, 0.0524]\n",
      "Epoch 69 | train Loss: [0.0303, 0.0446] | valid Loss: [0.0397, 0.0524]\n",
      "Epoch 70 | train Loss: [0.0303, 0.0446] | valid Loss: [0.0397, 0.0523]\n",
      "Epoch 71 | train Loss: [0.0302, 0.0446] | valid Loss: [0.0396, 0.0523]\n",
      "Epoch 72 | train Loss: [0.0301, 0.0446] | valid Loss: [0.0396, 0.0523]\n",
      "Epoch 73 | train Loss: [0.0301, 0.0445] | valid Loss: [0.0395, 0.0523]\n",
      "Epoch 74 | train Loss: [0.0300, 0.0445] | valid Loss: [0.0395, 0.0523]\n",
      "Epoch 75 | train Loss: [0.0300, 0.0445] | valid Loss: [0.0395, 0.0523]\n",
      "Epoch 76 | train Loss: [0.0299, 0.0445] | valid Loss: [0.0394, 0.0522]\n",
      "Epoch 77 | train Loss: [0.0299, 0.0445] | valid Loss: [0.0394, 0.0522]\n",
      "Epoch 78 | train Loss: [0.0298, 0.0445] | valid Loss: [0.0394, 0.0522]\n",
      "Epoch 79 | train Loss: [0.0298, 0.0444] | valid Loss: [0.0393, 0.0522]\n",
      "Epoch 80 | train Loss: [0.0298, 0.0444] | valid Loss: [0.0393, 0.0522]\n",
      "Epoch 81 | train Loss: [0.0297, 0.0444] | valid Loss: [0.0393, 0.0522]\n",
      "Epoch 82 | train Loss: [0.0297, 0.0444] | valid Loss: [0.0392, 0.0521]\n",
      "Epoch 83 | train Loss: [0.0297, 0.0444] | valid Loss: [0.0392, 0.0521]\n",
      "Epoch 84 | train Loss: [0.0296, 0.0443] | valid Loss: [0.0392, 0.0521]\n",
      "Epoch 85 | train Loss: [0.0296, 0.0443] | valid Loss: [0.0392, 0.0521]\n",
      "Epoch 86 | train Loss: [0.0296, 0.0443] | valid Loss: [0.0391, 0.0521]\n",
      "Epoch 87 | train Loss: [0.0295, 0.0443] | valid Loss: [0.0391, 0.0521]\n",
      "Epoch 88 | train Loss: [0.0295, 0.0443] | valid Loss: [0.0391, 0.0520]\n",
      "Epoch 89 | train Loss: [0.0295, 0.0443] | valid Loss: [0.0391, 0.0520]\n",
      "Epoch 90 | train Loss: [0.0294, 0.0442] | valid Loss: [0.0390, 0.0520]\n",
      "Epoch 91 | train Loss: [0.0294, 0.0442] | valid Loss: [0.0390, 0.0520]\n",
      "Epoch 92 | train Loss: [0.0294, 0.0442] | valid Loss: [0.0390, 0.0520]\n",
      "Epoch 93 | train Loss: [0.0294, 0.0442] | valid Loss: [0.0390, 0.0519]\n",
      "Epoch 94 | train Loss: [0.0293, 0.0442] | valid Loss: [0.0389, 0.0519]\n",
      "Epoch 95 | train Loss: [0.0293, 0.0441] | valid Loss: [0.0389, 0.0519]\n",
      "Epoch 96 | train Loss: [0.0293, 0.0441] | valid Loss: [0.0389, 0.0519]\n",
      "Epoch 97 | train Loss: [0.0293, 0.0441] | valid Loss: [0.0389, 0.0519]\n",
      "Epoch 98 | train Loss: [0.0293, 0.0441] | valid Loss: [0.0389, 0.0519]\n",
      "Epoch 99 | train Loss: [0.0292, 0.0440] | valid Loss: [0.0388, 0.0518]\n",
      "Epoch 100 | train Loss: [0.0292, 0.0440] | valid Loss: [0.0388, 0.0518]\n",
      "Epoch 101 | train Loss: [0.0292, 0.0440] | valid Loss: [0.0388, 0.0518]\n",
      "Epoch 102 | train Loss: [0.0292, 0.0440] | valid Loss: [0.0388, 0.0518]\n",
      "Epoch 103 | train Loss: [0.0292, 0.0440] | valid Loss: [0.0388, 0.0518]\n",
      "Epoch 104 | train Loss: [0.0292, 0.0439] | valid Loss: [0.0387, 0.0517]\n",
      "Epoch 105 | train Loss: [0.0291, 0.0439] | valid Loss: [0.0387, 0.0517]\n",
      "Epoch 106 | train Loss: [0.0291, 0.0439] | valid Loss: [0.0387, 0.0517]\n",
      "Epoch 107 | train Loss: [0.0291, 0.0439] | valid Loss: [0.0387, 0.0517]\n",
      "Epoch 108 | train Loss: [0.0291, 0.0439] | valid Loss: [0.0387, 0.0517]\n",
      "Epoch 109 | train Loss: [0.0291, 0.0438] | valid Loss: [0.0386, 0.0516]\n",
      "Epoch 110 | train Loss: [0.0291, 0.0438] | valid Loss: [0.0386, 0.0516]\n",
      "Epoch 111 | train Loss: [0.0290, 0.0438] | valid Loss: [0.0386, 0.0516]\n",
      "Epoch 112 | train Loss: [0.0290, 0.0438] | valid Loss: [0.0386, 0.0516]\n",
      "Epoch 113 | train Loss: [0.0290, 0.0438] | valid Loss: [0.0386, 0.0516]\n",
      "Epoch 114 | train Loss: [0.0290, 0.0437] | valid Loss: [0.0386, 0.0516]\n",
      "Epoch 115 | train Loss: [0.0290, 0.0437] | valid Loss: [0.0385, 0.0515]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 116 | train Loss: [0.0290, 0.0437] | valid Loss: [0.0385, 0.0515]\n",
      "Epoch 117 | train Loss: [0.0290, 0.0437] | valid Loss: [0.0385, 0.0515]\n",
      "Epoch 118 | train Loss: [0.0290, 0.0437] | valid Loss: [0.0385, 0.0515]\n",
      "Epoch 119 | train Loss: [0.0289, 0.0436] | valid Loss: [0.0385, 0.0514]\n",
      "Epoch 120 | train Loss: [0.0289, 0.0436] | valid Loss: [0.0385, 0.0514]\n",
      "Epoch 121 | train Loss: [0.0289, 0.0436] | valid Loss: [0.0384, 0.0514]\n",
      "Epoch 122 | train Loss: [0.0289, 0.0436] | valid Loss: [0.0384, 0.0514]\n",
      "Epoch 123 | train Loss: [0.0289, 0.0436] | valid Loss: [0.0384, 0.0514]\n",
      "Epoch 124 | train Loss: [0.0289, 0.0435] | valid Loss: [0.0384, 0.0513]\n",
      "Epoch 125 | train Loss: [0.0289, 0.0435] | valid Loss: [0.0384, 0.0513]\n",
      "Epoch 126 | train Loss: [0.0289, 0.0435] | valid Loss: [0.0384, 0.0513]\n",
      "Epoch 127 | train Loss: [0.0289, 0.0435] | valid Loss: [0.0384, 0.0513]\n",
      "Epoch 128 | train Loss: [0.0289, 0.0435] | valid Loss: [0.0383, 0.0513]\n",
      "Epoch 129 | train Loss: [0.0289, 0.0434] | valid Loss: [0.0383, 0.0512]\n",
      "Epoch 130 | train Loss: [0.0289, 0.0434] | valid Loss: [0.0383, 0.0512]\n",
      "Epoch 131 | train Loss: [0.0289, 0.0434] | valid Loss: [0.0383, 0.0512]\n",
      "Epoch 132 | train Loss: [0.0289, 0.0434] | valid Loss: [0.0383, 0.0512]\n",
      "Epoch 133 | train Loss: [0.0289, 0.0434] | valid Loss: [0.0383, 0.0512]\n",
      "Epoch 134 | train Loss: [0.0289, 0.0434] | valid Loss: [0.0383, 0.0511]\n",
      "Epoch 135 | train Loss: [0.0289, 0.0434] | valid Loss: [0.0383, 0.0511]\n",
      "Epoch 136 | train Loss: [0.0289, 0.0433] | valid Loss: [0.0382, 0.0511]\n",
      "Epoch 137 | train Loss: [0.0289, 0.0433] | valid Loss: [0.0382, 0.0511]\n",
      "Epoch 138 | train Loss: [0.0289, 0.0433] | valid Loss: [0.0382, 0.0511]\n",
      "Epoch 139 | train Loss: [0.0289, 0.0433] | valid Loss: [0.0382, 0.0511]\n",
      "Epoch 140 | train Loss: [0.0288, 0.0433] | valid Loss: [0.0382, 0.0510]\n",
      "Epoch 141 | train Loss: [0.0288, 0.0433] | valid Loss: [0.0382, 0.0510]\n",
      "Epoch 142 | train Loss: [0.0288, 0.0433] | valid Loss: [0.0382, 0.0510]\n",
      "Epoch 143 | train Loss: [0.0288, 0.0433] | valid Loss: [0.0382, 0.0510]\n",
      "Epoch 144 | train Loss: [0.0288, 0.0433] | valid Loss: [0.0382, 0.0510]\n",
      "Epoch 145 | train Loss: [0.0288, 0.0432] | valid Loss: [0.0381, 0.0509]\n",
      "Epoch 146 | train Loss: [0.0288, 0.0432] | valid Loss: [0.0381, 0.0509]\n",
      "Epoch 147 | train Loss: [0.0288, 0.0432] | valid Loss: [0.0381, 0.0509]\n",
      "Epoch 148 | train Loss: [0.0288, 0.0432] | valid Loss: [0.0381, 0.0509]\n",
      "Epoch 149 | train Loss: [0.0288, 0.0432] | valid Loss: [0.0381, 0.0508]\n",
      "Epoch 150 | train Loss: [0.0288, 0.0432] | valid Loss: [0.0381, 0.0508]\n",
      "Epoch 151 | train Loss: [0.0288, 0.0432] | valid Loss: [0.0381, 0.0508]\n",
      "Epoch 152 | train Loss: [0.0288, 0.0432] | valid Loss: [0.0381, 0.0508]\n",
      "Epoch 153 | train Loss: [0.0288, 0.0432] | valid Loss: [0.0381, 0.0507]\n",
      "Epoch 154 | train Loss: [0.0288, 0.0432] | valid Loss: [0.0381, 0.0507]\n",
      "Epoch 155 | train Loss: [0.0288, 0.0432] | valid Loss: [0.0381, 0.0507]\n",
      "Epoch 156 | train Loss: [0.0288, 0.0431] | valid Loss: [0.0380, 0.0507]\n",
      "Epoch 157 | train Loss: [0.0288, 0.0431] | valid Loss: [0.0380, 0.0507]\n",
      "Epoch 158 | train Loss: [0.0288, 0.0431] | valid Loss: [0.0380, 0.0506]\n",
      "Epoch 159 | train Loss: [0.0288, 0.0431] | valid Loss: [0.0380, 0.0506]\n",
      "Epoch 160 | train Loss: [0.0288, 0.0431] | valid Loss: [0.0380, 0.0506]\n",
      "Epoch 161 | train Loss: [0.0288, 0.0431] | valid Loss: [0.0380, 0.0506]\n",
      "Epoch 162 | train Loss: [0.0288, 0.0431] | valid Loss: [0.0380, 0.0505]\n",
      "Epoch 163 | train Loss: [0.0288, 0.0431] | valid Loss: [0.0380, 0.0505]\n",
      "Epoch 164 | train Loss: [0.0288, 0.0431] | valid Loss: [0.0380, 0.0505]\n",
      "Epoch 165 | train Loss: [0.0288, 0.0431] | valid Loss: [0.0380, 0.0505]\n",
      "Epoch 166 | train Loss: [0.0288, 0.0430] | valid Loss: [0.0380, 0.0504]\n",
      "Epoch 167 | train Loss: [0.0287, 0.0430] | valid Loss: [0.0380, 0.0504]\n",
      "Epoch 168 | train Loss: [0.0287, 0.0430] | valid Loss: [0.0380, 0.0504]\n",
      "Epoch 169 | train Loss: [0.0287, 0.0430] | valid Loss: [0.0379, 0.0504]\n",
      "Epoch 170 | train Loss: [0.0287, 0.0430] | valid Loss: [0.0379, 0.0504]\n",
      "Epoch 171 | train Loss: [0.0287, 0.0430] | valid Loss: [0.0379, 0.0503]\n",
      "Epoch 172 | train Loss: [0.0287, 0.0430] | valid Loss: [0.0379, 0.0503]\n",
      "Epoch 173 | train Loss: [0.0287, 0.0430] | valid Loss: [0.0379, 0.0503]\n",
      "Epoch 174 | train Loss: [0.0287, 0.0429] | valid Loss: [0.0379, 0.0503]\n",
      "Epoch 175 | train Loss: [0.0287, 0.0429] | valid Loss: [0.0379, 0.0503]\n",
      "Epoch 176 | train Loss: [0.0287, 0.0429] | valid Loss: [0.0379, 0.0502]\n",
      "Epoch 177 | train Loss: [0.0287, 0.0429] | valid Loss: [0.0379, 0.0502]\n",
      "Epoch 178 | train Loss: [0.0287, 0.0429] | valid Loss: [0.0379, 0.0502]\n",
      "Epoch 179 | train Loss: [0.0287, 0.0429] | valid Loss: [0.0379, 0.0502]\n",
      "Epoch 180 | train Loss: [0.0287, 0.0429] | valid Loss: [0.0379, 0.0502]\n",
      "Epoch 181 | train Loss: [0.0286, 0.0429] | valid Loss: [0.0379, 0.0501]\n",
      "Epoch 182 | train Loss: [0.0286, 0.0429] | valid Loss: [0.0379, 0.0501]\n",
      "Epoch 183 | train Loss: [0.0286, 0.0428] | valid Loss: [0.0378, 0.0501]\n",
      "Epoch 184 | train Loss: [0.0286, 0.0428] | valid Loss: [0.0378, 0.0501]\n",
      "Epoch 185 | train Loss: [0.0286, 0.0428] | valid Loss: [0.0378, 0.0501]\n",
      "Epoch 186 | train Loss: [0.0286, 0.0428] | valid Loss: [0.0378, 0.0501]\n",
      "Epoch 187 | train Loss: [0.0286, 0.0428] | valid Loss: [0.0378, 0.0500]\n",
      "Epoch 188 | train Loss: [0.0286, 0.0428] | valid Loss: [0.0378, 0.0500]\n",
      "Epoch 189 | train Loss: [0.0286, 0.0428] | valid Loss: [0.0378, 0.0500]\n",
      "Epoch 190 | train Loss: [0.0286, 0.0428] | valid Loss: [0.0378, 0.0500]\n",
      "Epoch 191 | train Loss: [0.0286, 0.0428] | valid Loss: [0.0378, 0.0500]\n",
      "Epoch 192 | train Loss: [0.0286, 0.0427] | valid Loss: [0.0378, 0.0500]\n",
      "Epoch 193 | train Loss: [0.0286, 0.0427] | valid Loss: [0.0378, 0.0500]\n",
      "Epoch 194 | train Loss: [0.0285, 0.0427] | valid Loss: [0.0378, 0.0499]\n",
      "Epoch 195 | train Loss: [0.0285, 0.0427] | valid Loss: [0.0378, 0.0499]\n",
      "Epoch 196 | train Loss: [0.0285, 0.0427] | valid Loss: [0.0377, 0.0499]\n",
      "Epoch 197 | train Loss: [0.0285, 0.0427] | valid Loss: [0.0377, 0.0499]\n",
      "Epoch 198 | train Loss: [0.0285, 0.0427] | valid Loss: [0.0377, 0.0499]\n",
      "Epoch 199 | train Loss: [0.0285, 0.0427] | valid Loss: [0.0377, 0.0499]\n",
      "Epoch 200 | train Loss: [0.0285, 0.0427] | valid Loss: [0.0377, 0.0499]\n",
      "Epoch 201 | train Loss: [0.0285, 0.0427] | valid Loss: [0.0377, 0.0499]\n",
      "Epoch 202 | train Loss: [0.0285, 0.0427] | valid Loss: [0.0377, 0.0499]\n",
      "Epoch 203 | train Loss: [0.0285, 0.0426] | valid Loss: [0.0377, 0.0498]\n",
      "Epoch 204 | train Loss: [0.0285, 0.0426] | valid Loss: [0.0377, 0.0498]\n",
      "Epoch 205 | train Loss: [0.0285, 0.0426] | valid Loss: [0.0377, 0.0498]\n",
      "Epoch 206 | train Loss: [0.0285, 0.0426] | valid Loss: [0.0377, 0.0498]\n",
      "Epoch 207 | train Loss: [0.0285, 0.0426] | valid Loss: [0.0377, 0.0498]\n",
      "Epoch 208 | train Loss: [0.0285, 0.0426] | valid Loss: [0.0376, 0.0498]\n",
      "Epoch 209 | train Loss: [0.0285, 0.0426] | valid Loss: [0.0376, 0.0498]\n",
      "Epoch 210 | train Loss: [0.0285, 0.0426] | valid Loss: [0.0376, 0.0498]\n",
      "Epoch 211 | train Loss: [0.0285, 0.0426] | valid Loss: [0.0376, 0.0498]\n",
      "Epoch 212 | train Loss: [0.0285, 0.0426] | valid Loss: [0.0376, 0.0498]\n",
      "Epoch 213 | train Loss: [0.0285, 0.0426] | valid Loss: [0.0376, 0.0498]\n",
      "Epoch 214 | train Loss: [0.0284, 0.0426] | valid Loss: [0.0376, 0.0497]\n",
      "Epoch 215 | train Loss: [0.0284, 0.0426] | valid Loss: [0.0376, 0.0497]\n",
      "Epoch 216 | train Loss: [0.0284, 0.0425] | valid Loss: [0.0376, 0.0497]\n",
      "Epoch 217 | train Loss: [0.0284, 0.0425] | valid Loss: [0.0376, 0.0497]\n",
      "Epoch 218 | train Loss: [0.0284, 0.0425] | valid Loss: [0.0376, 0.0497]\n",
      "Epoch 219 | train Loss: [0.0284, 0.0425] | valid Loss: [0.0376, 0.0497]\n",
      "Epoch 220 | train Loss: [0.0284, 0.0425] | valid Loss: [0.0376, 0.0497]\n",
      "Epoch 221 | train Loss: [0.0284, 0.0425] | valid Loss: [0.0376, 0.0497]\n",
      "Epoch 222 | train Loss: [0.0284, 0.0425] | valid Loss: [0.0376, 0.0497]\n",
      "Epoch 223 | train Loss: [0.0284, 0.0425] | valid Loss: [0.0376, 0.0497]\n",
      "Epoch 224 | train Loss: [0.0284, 0.0425] | valid Loss: [0.0375, 0.0497]\n",
      "Epoch 225 | train Loss: [0.0284, 0.0425] | valid Loss: [0.0375, 0.0497]\n",
      "Epoch 226 | train Loss: [0.0284, 0.0425] | valid Loss: [0.0375, 0.0497]\n",
      "Epoch 227 | train Loss: [0.0284, 0.0425] | valid Loss: [0.0375, 0.0497]\n",
      "Epoch 228 | train Loss: [0.0285, 0.0425] | valid Loss: [0.0375, 0.0497]\n",
      "Epoch 229 | train Loss: [0.0285, 0.0425] | valid Loss: [0.0375, 0.0497]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 230 | train Loss: [0.0285, 0.0425] | valid Loss: [0.0375, 0.0496]\n",
      "Epoch 231 | train Loss: [0.0285, 0.0425] | valid Loss: [0.0375, 0.0496]\n",
      "Epoch 232 | train Loss: [0.0285, 0.0425] | valid Loss: [0.0375, 0.0496]\n",
      "Epoch 233 | train Loss: [0.0285, 0.0425] | valid Loss: [0.0375, 0.0496]\n",
      "Epoch 234 | train Loss: [0.0285, 0.0424] | valid Loss: [0.0375, 0.0496]\n",
      "Epoch 235 | train Loss: [0.0285, 0.0424] | valid Loss: [0.0375, 0.0496]\n",
      "Epoch 236 | train Loss: [0.0285, 0.0424] | valid Loss: [0.0375, 0.0496]\n",
      "Epoch 237 | train Loss: [0.0285, 0.0424] | valid Loss: [0.0375, 0.0496]\n",
      "Epoch 238 | train Loss: [0.0285, 0.0424] | valid Loss: [0.0375, 0.0496]\n",
      "Epoch 239 | train Loss: [0.0285, 0.0424] | valid Loss: [0.0375, 0.0496]\n",
      "Epoch 240 | train Loss: [0.0285, 0.0424] | valid Loss: [0.0375, 0.0496]\n",
      "Epoch 241 | train Loss: [0.0285, 0.0424] | valid Loss: [0.0375, 0.0496]\n",
      "Epoch 242 | train Loss: [0.0285, 0.0424] | valid Loss: [0.0375, 0.0496]\n",
      "Epoch 243 | train Loss: [0.0285, 0.0424] | valid Loss: [0.0375, 0.0496]\n",
      "Epoch 244 | train Loss: [0.0285, 0.0424] | valid Loss: [0.0374, 0.0496]\n",
      "Epoch 245 | train Loss: [0.0285, 0.0424] | valid Loss: [0.0374, 0.0496]\n",
      "Epoch 246 | train Loss: [0.0285, 0.0424] | valid Loss: [0.0374, 0.0496]\n",
      "Epoch 247 | train Loss: [0.0286, 0.0424] | valid Loss: [0.0374, 0.0496]\n",
      "Epoch 248 | train Loss: [0.0286, 0.0424] | valid Loss: [0.0374, 0.0496]\n",
      "Epoch 249 | train Loss: [0.0286, 0.0424] | valid Loss: [0.0374, 0.0496]\n",
      "Epoch 250 | train Loss: [0.0286, 0.0424] | valid Loss: [0.0374, 0.0496]\n",
      "Epoch 251 | train Loss: [0.0286, 0.0424] | valid Loss: [0.0374, 0.0496]\n",
      "Epoch 252 | train Loss: [0.0286, 0.0424] | valid Loss: [0.0374, 0.0496]\n",
      "Epoch 253 | train Loss: [0.0286, 0.0424] | valid Loss: [0.0374, 0.0496]\n",
      "Epoch 254 | train Loss: [0.0286, 0.0424] | valid Loss: [0.0374, 0.0496]\n",
      "Epoch 255 | train Loss: [0.0286, 0.0424] | valid Loss: [0.0374, 0.0496]\n",
      "Epoch 256 | train Loss: [0.0286, 0.0424] | valid Loss: [0.0374, 0.0495]\n",
      "Epoch 257 | train Loss: [0.0286, 0.0424] | valid Loss: [0.0374, 0.0495]\n",
      "Epoch 258 | train Loss: [0.0287, 0.0424] | valid Loss: [0.0374, 0.0495]\n",
      "Epoch 259 | train Loss: [0.0287, 0.0424] | valid Loss: [0.0374, 0.0495]\n",
      "Epoch 260 | train Loss: [0.0287, 0.0424] | valid Loss: [0.0374, 0.0495]\n",
      "Epoch 261 | train Loss: [0.0287, 0.0424] | valid Loss: [0.0374, 0.0495]\n",
      "Epoch 262 | train Loss: [0.0287, 0.0424] | valid Loss: [0.0374, 0.0495]\n",
      "Epoch 263 | train Loss: [0.0287, 0.0424] | valid Loss: [0.0374, 0.0495]\n",
      "Epoch 264 | train Loss: [0.0287, 0.0424] | valid Loss: [0.0374, 0.0495]\n",
      "Epoch 265 | train Loss: [0.0287, 0.0424] | valid Loss: [0.0374, 0.0495]\n",
      "Epoch 266 | train Loss: [0.0287, 0.0424] | valid Loss: [0.0374, 0.0495]\n",
      "Epoch 267 | train Loss: [0.0287, 0.0424] | valid Loss: [0.0374, 0.0495]\n",
      "Epoch 268 | train Loss: [0.0287, 0.0423] | valid Loss: [0.0374, 0.0495]\n",
      "Epoch 269 | train Loss: [0.0287, 0.0423] | valid Loss: [0.0374, 0.0495]\n",
      "Epoch 270 | train Loss: [0.0287, 0.0423] | valid Loss: [0.0374, 0.0495]\n",
      "Epoch 271 | train Loss: [0.0287, 0.0423] | valid Loss: [0.0374, 0.0495]\n",
      "Epoch 272 | train Loss: [0.0287, 0.0423] | valid Loss: [0.0374, 0.0495]\n",
      "Epoch 00274: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 273 | train Loss: [0.0287, 0.0423] | valid Loss: [0.0374, 0.0495]\n",
      "Epoch 274 | train Loss: [0.0285, 0.0422] | valid Loss: [0.0379, 0.0494]\n",
      "Epoch 275 | train Loss: [0.0285, 0.0422] | valid Loss: [0.0379, 0.0494]\n",
      "Epoch 276 | train Loss: [0.0285, 0.0422] | valid Loss: [0.0379, 0.0494]\n",
      "Epoch 277 | train Loss: [0.0285, 0.0422] | valid Loss: [0.0379, 0.0494]\n",
      "Epoch 278 | train Loss: [0.0285, 0.0422] | valid Loss: [0.0379, 0.0494]\n",
      "Epoch 00280: reducing learning rate of group 0 to 2.5000e-04.\n",
      "Epoch 279 | train Loss: [0.0285, 0.0422] | valid Loss: [0.0379, 0.0494]\n",
      "Epoch 280 | train Loss: [0.0285, 0.0424] | valid Loss: [0.0373, 0.0493]\n",
      "Epoch 281 | train Loss: [0.0284, 0.0424] | valid Loss: [0.0373, 0.0493]\n",
      "Epoch 282 | train Loss: [0.0284, 0.0424] | valid Loss: [0.0373, 0.0493]\n",
      "Epoch 283 | train Loss: [0.0284, 0.0424] | valid Loss: [0.0373, 0.0493]\n",
      "Epoch 284 | train Loss: [0.0284, 0.0424] | valid Loss: [0.0373, 0.0493]\n",
      "Epoch 285 | train Loss: [0.0284, 0.0424] | valid Loss: [0.0373, 0.0493]\n",
      "Epoch 286 | train Loss: [0.0284, 0.0424] | valid Loss: [0.0373, 0.0493]\n",
      "Epoch 287 | train Loss: [0.0284, 0.0424] | valid Loss: [0.0373, 0.0493]\n",
      "Epoch 288 | train Loss: [0.0284, 0.0424] | valid Loss: [0.0373, 0.0493]\n",
      "Epoch 289 | train Loss: [0.0284, 0.0424] | valid Loss: [0.0373, 0.0493]\n",
      "Epoch 290 | train Loss: [0.0284, 0.0424] | valid Loss: [0.0373, 0.0493]\n",
      "Epoch 291 | train Loss: [0.0284, 0.0424] | valid Loss: [0.0373, 0.0493]\n",
      "Epoch 292 | train Loss: [0.0284, 0.0424] | valid Loss: [0.0373, 0.0493]\n",
      "Epoch 293 | train Loss: [0.0284, 0.0424] | valid Loss: [0.0373, 0.0493]\n",
      "Epoch 294 | train Loss: [0.0284, 0.0424] | valid Loss: [0.0373, 0.0493]\n",
      "Epoch 295 | train Loss: [0.0284, 0.0424] | valid Loss: [0.0373, 0.0493]\n",
      "Epoch 296 | train Loss: [0.0284, 0.0424] | valid Loss: [0.0373, 0.0493]\n",
      "Epoch 00298: reducing learning rate of group 0 to 1.2500e-04.\n",
      "Epoch 297 | train Loss: [0.0284, 0.0424] | valid Loss: [0.0373, 0.0493]\n",
      "Epoch 298 | train Loss: [0.0281, 0.0425] | valid Loss: [0.0373, 0.0493]\n",
      "Epoch 299 | train Loss: [0.0281, 0.0425] | valid Loss: [0.0373, 0.0493]\n"
     ]
    }
   ],
   "source": [
    "# モデルのインスタンス生成\n",
    "xy_dim = 2\n",
    "\n",
    "model = ActiveNet(xy_dim, dr_thresh, dropout=0, batchN=False, bias=True, Nchannels=128).to(device)\n",
    "# input data\n",
    "#data = dataset[0]\n",
    "\n",
    "# optimizer\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3, momentum=0.9)\n",
    "#optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)#, weight_decay=5e-4)\n",
    "#optimizer = torch.optim.Adadelta(model.parameters())#, rho=0.95)#, lr=1e-1, momentum=0.9)\n",
    "\n",
    "#optim = gdtuo.Adam(alpha=1e-2, beta1=0.9, beta2=0.999, log_eps=-8., optimizer=gdtuo.SGD(1e-3))\n",
    "#mw = gdtuo.ModuleWrapper(model, optimizer=optim)\n",
    "#mw.initialize()\n",
    "\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min', factor=0.5, patience=5, verbose=True)\n",
    "\n",
    "val_loss_log = []\n",
    "\n",
    "val_loss_min = np.Inf\n",
    "\n",
    "# learnig loop\n",
    "for epoch in range(300):\n",
    "    model.train()\n",
    "    for batch_x, batch_y, batch_i_dir in train_data:\n",
    "        optimizer.zero_grad()\n",
    "        lossv = 0\n",
    "        losstheta = 0\n",
    "        for ib in range(batch_x.size(0)):\n",
    "            model.load_celltypes(celltype_lst[int(batch_i_dir[ib])].to(device))\n",
    "            out = model(batch_x[ib].to(device))\n",
    "            lv, ltheta = myLoss(out, batch_y[ib].to(device))\n",
    "            lossv = lossv + lv\n",
    "            losstheta = losstheta + ltheta\n",
    "        lossv = lossv / batch_x.size(0)\n",
    "        losstheta = losstheta / batch_x.size(0)\n",
    "        (lossv+losstheta).backward()\n",
    "        optimizer.step()\n",
    "    model.eval()\n",
    "    val_lossv = 0\n",
    "    val_losstheta = 0\n",
    "    val_count = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_y, batch_i_dir in valid_data:\n",
    "            for ib in range(batch_x.size(0)):\n",
    "                model.load_celltypes(celltype_lst[int(batch_i_dir[ib])].to(device))\n",
    "                val_out = model(batch_x[ib].to(device))\n",
    "                lv, ltheta = myLoss(val_out, batch_y[ib].to(device))\n",
    "                val_lossv = val_lossv + lv\n",
    "                val_losstheta = val_losstheta + ltheta\n",
    "            val_count = val_count + batch_x.size(0)\n",
    "    val_lossv = val_lossv/val_count\n",
    "    val_losstheta = val_losstheta/val_count\n",
    "    val_loss = val_lossv + val_losstheta\n",
    "    scheduler.step(val_loss)\n",
    "    print('Epoch %d | train Loss: [%.4f, %.4f] | valid Loss: [%.4f, %.4f]' % (epoch,\n",
    "                                                                              lossv.item(), \n",
    "                                                                              losstheta.item(),\n",
    "                                                                              val_lossv.item(), \n",
    "                                                                              val_losstheta.item()))\n",
    "    val_loss_log.append([val_lossv.cpu().item(), val_losstheta.cpu().item()])\n",
    "    if val_loss.item() < val_loss_min:\n",
    "        stored_model = model\n",
    "        val_loss_min = val_loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "b3cf1e91",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-03T17:24:28.648677Z",
     "start_time": "2023-03-03T17:24:28.546582Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('selfpropel', tensor(0.9251, device='cuda:1')),\n",
       "             ('interactNN.layer1.weight',\n",
       "              tensor([[ 0.2185,  0.2012,  0.0633, -0.1603, -0.2546,  0.2063],\n",
       "                      [ 0.0789,  0.0199, -0.2185, -0.0179,  0.0370,  0.2715],\n",
       "                      [-0.0823,  0.1029, -0.3984, -0.0192,  0.3186, -0.0922],\n",
       "                      [ 0.0451,  0.2858, -0.0804, -0.3068,  0.0925,  0.0947],\n",
       "                      [-0.4581,  0.0992, -0.2529, -0.3365,  0.3901, -0.0395],\n",
       "                      [-0.0803,  0.0635, -0.0571,  0.2846, -0.2998,  0.3691],\n",
       "                      [ 0.4822,  0.2574,  0.0750,  0.1314,  0.0157, -0.3035],\n",
       "                      [ 0.4956, -0.4538, -0.0259, -0.0735, -0.0517, -0.0602],\n",
       "                      [-0.5238, -0.5016, -0.0672, -0.1873, -0.1848, -0.1180],\n",
       "                      [ 0.3227, -0.3686, -0.2994,  0.1840, -0.3364,  0.3206],\n",
       "                      [-0.3876, -0.3320, -0.1246, -0.0276, -0.3060, -0.2636],\n",
       "                      [ 0.0414, -0.2237,  0.0751, -0.2037,  0.3727, -0.0216],\n",
       "                      [-0.0592, -0.5208, -0.0320,  0.0850, -0.0405, -0.1056],\n",
       "                      [ 0.1455, -0.2446, -0.3514, -0.0719,  0.2618,  0.1284],\n",
       "                      [-0.0565, -0.1492, -0.0831, -0.1943, -0.1521, -0.1215],\n",
       "                      [-0.2878, -0.1701, -0.2341, -0.1854, -0.0679, -0.1688],\n",
       "                      [ 0.3873, -0.0324,  0.2954,  0.2589, -0.3108, -0.0588],\n",
       "                      [-0.1483,  0.2490, -0.2442, -0.1345,  0.2618, -0.0403],\n",
       "                      [ 0.1744, -0.1206,  0.0881, -0.2177,  0.1242,  0.0803],\n",
       "                      [ 0.0844,  0.0088, -0.3440, -0.0758, -0.0117, -0.1185],\n",
       "                      [-0.2172,  0.0334, -0.1075, -0.1795,  0.3940,  0.0364],\n",
       "                      [ 0.3708, -0.1074, -0.2826,  0.1375,  0.1427, -0.1698],\n",
       "                      [-0.3192,  0.1795, -0.2543,  0.2412, -0.2549,  0.3609],\n",
       "                      [-0.2631,  0.4463,  0.2121, -0.2884,  0.1340, -0.1997],\n",
       "                      [-0.3019, -0.4789,  0.0531,  0.0145, -0.0175, -0.4020],\n",
       "                      [-0.4887, -0.5617,  0.2578, -0.1579,  0.3206, -0.1252],\n",
       "                      [-0.0043, -0.4223, -0.3155, -0.3387, -0.3278, -0.0449],\n",
       "                      [-0.3135,  0.2227, -0.1461, -0.3683, -0.2731, -0.0503],\n",
       "                      [-0.3510, -0.5411,  0.3040,  0.3083, -0.1662, -0.1129],\n",
       "                      [ 0.3602, -0.0933, -0.2638,  0.1042,  0.2378,  0.3284],\n",
       "                      [-0.5289, -0.1616,  0.3129,  0.1674, -0.0082,  0.2429],\n",
       "                      [ 0.1201, -0.0102,  0.2002,  0.2424, -0.3649, -0.2878],\n",
       "                      [ 0.2624,  0.0593, -0.3008,  0.2844,  0.3527,  0.2033],\n",
       "                      [-0.3084,  0.0508, -0.3693, -0.2994,  0.3194,  0.1786],\n",
       "                      [-0.0427,  0.0914, -0.3450,  0.2145,  0.2730,  0.3635],\n",
       "                      [ 0.0567,  0.0665,  0.1262, -0.0604,  0.0247,  0.1760],\n",
       "                      [-0.1662,  0.4749, -0.3628,  0.1392, -0.1303,  0.0221],\n",
       "                      [ 0.4738,  0.2035, -0.0015,  0.0948, -0.1030,  0.2984],\n",
       "                      [-0.1204, -0.1658, -0.3896, -0.1976,  0.0841, -0.1483],\n",
       "                      [ 0.3456, -0.0859,  0.2408, -0.0703,  0.3553,  0.1281],\n",
       "                      [-0.2906,  0.1099,  0.0066, -0.2128,  0.1974,  0.2596],\n",
       "                      [ 0.1991, -0.0535,  0.1352, -0.2324,  0.2329, -0.2314],\n",
       "                      [-0.0109,  0.2093, -0.2530,  0.1973,  0.2112,  0.3248],\n",
       "                      [ 0.4477, -0.2204,  0.3928, -0.2136,  0.1016, -0.1309],\n",
       "                      [ 0.4127,  0.4940, -0.2304,  0.0133,  0.4051, -0.2791],\n",
       "                      [-0.2378, -0.2888,  0.1947,  0.2068,  0.3128, -0.0921],\n",
       "                      [-0.2176, -0.4645,  0.1409, -0.0057,  0.3133,  0.2968],\n",
       "                      [ 0.1324,  0.1752,  0.0302, -0.3168, -0.2076, -0.1982],\n",
       "                      [-0.1124, -0.3341, -0.0718, -0.1102, -0.2444, -0.1175],\n",
       "                      [ 0.3384, -0.1782, -0.1245,  0.2729, -0.0229, -0.0785],\n",
       "                      [ 0.2029, -0.4480, -0.1588,  0.0976,  0.2306,  0.1640],\n",
       "                      [ 0.1450, -0.2087,  0.0533,  0.2282,  0.2185, -0.3030],\n",
       "                      [-0.4485,  0.4537,  0.1024, -0.0606,  0.1873,  0.1959],\n",
       "                      [ 0.4814,  0.3234,  0.1345,  0.0151, -0.0026, -0.0054],\n",
       "                      [-0.0543,  0.4131, -0.4089,  0.2678,  0.0286, -0.1909],\n",
       "                      [ 0.0844, -0.1175,  0.3776,  0.3276,  0.3257, -0.2933],\n",
       "                      [ 0.2723, -0.1320, -0.0776,  0.3741, -0.0354, -0.3349],\n",
       "                      [ 0.2724, -0.1611,  0.0359,  0.3570, -0.1465, -0.2158],\n",
       "                      [ 0.2178,  0.3538,  0.1485, -0.3457,  0.0160, -0.0651],\n",
       "                      [ 0.1510,  0.0935,  0.1823,  0.3141, -0.3351,  0.1357],\n",
       "                      [-0.0148, -0.0198, -0.1162,  0.2077,  0.1005, -0.0047],\n",
       "                      [ 0.0782, -0.3044, -0.0360, -0.0043,  0.3016, -0.3820],\n",
       "                      [ 0.3829,  0.0776, -0.3586, -0.3286, -0.3044,  0.2771],\n",
       "                      [ 0.3427, -0.0430, -0.3725, -0.3201, -0.3433, -0.2322],\n",
       "                      [ 0.0930, -0.4205, -0.0389, -0.1568,  0.3043,  0.1864],\n",
       "                      [ 0.3583,  0.0884,  0.4395,  0.2477,  0.0519, -0.1533],\n",
       "                      [-0.1000, -0.4690,  0.3666, -0.0418, -0.3309,  0.1451],\n",
       "                      [-0.1246, -0.3891, -0.3568,  0.3561,  0.1425, -0.2834],\n",
       "                      [-0.3923, -0.0529, -0.1124,  0.2120, -0.1495, -0.3524],\n",
       "                      [-0.1719, -0.2156,  0.3009, -0.1300,  0.2598, -0.1523],\n",
       "                      [ 0.5409,  0.4846, -0.0739, -0.0420,  0.0166,  0.0563],\n",
       "                      [ 0.3027,  0.3389,  0.0401, -0.1300,  0.2142,  0.2377],\n",
       "                      [ 0.4975, -0.2019,  0.4248,  0.1157, -0.1798,  0.2014],\n",
       "                      [-0.3033,  0.3394, -0.0968, -0.1241, -0.2482, -0.3701],\n",
       "                      [-0.1365, -0.1458, -0.2106,  0.0183,  0.4003, -0.0406],\n",
       "                      [ 0.2510, -0.0745, -0.0444,  0.1689,  0.3388, -0.1752],\n",
       "                      [-0.3419, -0.1876, -0.0845, -0.2241,  0.0511, -0.1261],\n",
       "                      [-0.1692,  0.3175,  0.3501,  0.1730,  0.3552, -0.3547],\n",
       "                      [ 0.2287, -0.4511, -0.0681,  0.2761,  0.0606, -0.1979],\n",
       "                      [-0.3927, -0.2110,  0.2174, -0.1132, -0.2850, -0.3947],\n",
       "                      [-0.3019, -0.0944,  0.2781, -0.1146, -0.1473,  0.2462],\n",
       "                      [ 0.3484,  0.2199,  0.2483,  0.3283, -0.0103, -0.1189],\n",
       "                      [-0.1525,  0.0522,  0.0256,  0.2465, -0.1124, -0.2554],\n",
       "                      [ 0.1840, -0.3907,  0.2320, -0.1924, -0.3628, -0.2726],\n",
       "                      [ 0.5364, -0.3467,  0.0171, -0.3099,  0.2276, -0.1410],\n",
       "                      [-0.4339,  0.1677,  0.4233, -0.2522, -0.0140,  0.1175],\n",
       "                      [-0.3962,  0.0699, -0.3787, -0.0942,  0.4017, -0.0754],\n",
       "                      [ 0.3615,  0.5062,  0.0864,  0.0622, -0.0154,  0.3811],\n",
       "                      [-0.3464,  0.5646,  0.1948, -0.0205,  0.0201, -0.2276],\n",
       "                      [ 0.1837,  0.2400, -0.0774,  0.1450,  0.2453, -0.3759],\n",
       "                      [ 0.3329,  0.0263,  0.0327, -0.2267, -0.3114,  0.2773],\n",
       "                      [ 0.1962,  0.0412, -0.2449,  0.1780,  0.3613,  0.4014],\n",
       "                      [ 0.5645, -0.5613, -0.0946,  0.0791,  0.0318,  0.0637],\n",
       "                      [ 0.5587, -0.5128,  0.0424, -0.1089,  0.0016, -0.0026],\n",
       "                      [-0.3503, -0.0371,  0.1492, -0.2924,  0.2165,  0.1837],\n",
       "                      [-0.3773,  0.6047,  0.2867, -0.0813, -0.2076,  0.2025],\n",
       "                      [ 0.4837,  0.1126, -0.1079, -0.1836,  0.1239, -0.2207],\n",
       "                      [-0.2692, -0.5998,  0.2049,  0.2749,  0.3426, -0.0194],\n",
       "                      [ 0.0121,  0.3778, -0.0008,  0.2293, -0.0764, -0.2752],\n",
       "                      [ 0.3443,  0.3089,  0.4380, -0.1357, -0.3371,  0.0260],\n",
       "                      [ 0.1088, -0.3919, -0.1417, -0.2549,  0.0378,  0.1536],\n",
       "                      [ 0.4014, -0.1240, -0.0127, -0.2398, -0.3065,  0.3059],\n",
       "                      [-0.1921, -0.3991,  0.2923,  0.2289, -0.3306,  0.3510],\n",
       "                      [-0.0266, -0.1677,  0.1888,  0.2411,  0.1059, -0.3274],\n",
       "                      [-0.1706, -0.1071,  0.3909, -0.2111, -0.3843, -0.1912],\n",
       "                      [ 0.2708, -0.4955,  0.3902, -0.2541, -0.1765,  0.2958],\n",
       "                      [-0.4190,  0.3360,  0.1195,  0.1815,  0.3421,  0.2054],\n",
       "                      [ 0.0898,  0.1402,  0.1645, -0.2424,  0.1644, -0.1117],\n",
       "                      [-0.1299, -0.4960,  0.1550, -0.3812, -0.0045,  0.2155],\n",
       "                      [ 0.3310, -0.3407, -0.2540,  0.2092, -0.1451,  0.0405],\n",
       "                      [-0.1740, -0.3494,  0.1431,  0.2187, -0.2979,  0.1452],\n",
       "                      [ 0.1740, -0.0969,  0.0290,  0.3864, -0.0394,  0.2446],\n",
       "                      [-0.0294,  0.3586,  0.1954, -0.0007,  0.1578,  0.3091],\n",
       "                      [ 0.1905, -0.1779,  0.1308, -0.0122,  0.3927, -0.0254],\n",
       "                      [-0.3689, -0.1034, -0.3706, -0.3327,  0.0087,  0.2497],\n",
       "                      [-0.3705, -0.0337, -0.2285,  0.0810,  0.0116,  0.2301],\n",
       "                      [-0.2065, -0.0577,  0.3088, -0.1413, -0.2265,  0.2835],\n",
       "                      [-0.2906, -0.1019, -0.2725, -0.0336, -0.3240,  0.0128],\n",
       "                      [ 0.4784, -0.2463,  0.2003,  0.0246,  0.1939, -0.1694],\n",
       "                      [ 0.3517, -0.0259, -0.2256, -0.3408, -0.2478,  0.0289],\n",
       "                      [ 0.0589, -0.0640,  0.1034, -0.1386, -0.2667,  0.3056],\n",
       "                      [ 0.1131, -0.3399, -0.0923,  0.1126,  0.0008, -0.0255],\n",
       "                      [ 0.4216, -0.2851,  0.0540,  0.0060,  0.0087,  0.1890],\n",
       "                      [-0.4553,  0.0540, -0.2286,  0.1792,  0.0137, -0.2898],\n",
       "                      [ 0.3611,  0.0644,  0.0285, -0.0715,  0.2929,  0.3899],\n",
       "                      [ 0.2282,  0.1992, -0.3835, -0.2345,  0.0292, -0.2750],\n",
       "                      [ 0.1366,  0.0093,  0.1180, -0.0810, -0.1229, -0.3182],\n",
       "                      [-0.1888,  0.2658, -0.4221, -0.0791, -0.1457, -0.0217]],\n",
       "                     device='cuda:1')),\n",
       "             ('interactNN.layer1.bias',\n",
       "              tensor([-0.3951, -0.0243, -0.1683, -0.1964,  0.3086,  0.0120,  0.1910, -0.0559,\n",
       "                       0.2873,  0.3928,  0.2153, -0.3864,  0.2506, -0.1873,  0.3265, -0.1293,\n",
       "                       0.0123,  0.2161,  0.4476,  0.3012, -0.0665,  0.1960,  0.4572,  0.1190,\n",
       "                      -0.1902,  0.3649, -0.2354,  0.1245,  0.4287, -0.3682,  0.2760, -0.2415,\n",
       "                      -0.0175, -0.3197,  0.1520, -0.3633,  0.3841,  0.4084, -0.2308, -0.2347,\n",
       "                       0.1534, -0.4003,  0.1664,  0.2680,  0.4820,  0.0695, -0.2868, -0.1420,\n",
       "                      -0.0104, -0.2116,  0.0564, -0.3811, -0.1431, -0.2197, -0.0317, -0.1022,\n",
       "                      -0.0649,  0.0713,  0.0947, -0.3002, -0.0859, -0.2012, -0.3060, -0.2002,\n",
       "                       0.0548,  0.3137, -0.1933, -0.3063, -0.1609, -0.0368, -0.0742, -0.0357,\n",
       "                       0.1235, -0.2325, -0.2334, -0.1612,  0.2554, -0.0027,  0.3991, -0.3193,\n",
       "                      -0.2246,  0.4051,  0.2404, -0.0533,  0.0165,  0.2410, -0.0506, -0.2512,\n",
       "                       0.3836,  0.2929, -0.3704,  0.3433, -0.0068,  0.3825,  0.2123,  0.2713,\n",
       "                       0.3706,  0.1505, -0.0798, -0.0982,  0.4152,  0.3564,  0.2288, -0.3707,\n",
       "                       0.3219, -0.3490,  0.2439,  0.3925,  0.1340,  0.1326, -0.1776, -0.0623,\n",
       "                      -0.0122,  0.2753,  0.1156,  0.1625,  0.1886,  0.2890,  0.4255, -0.2395,\n",
       "                      -0.3347,  0.0882,  0.1916,  0.4516, -0.1291, -0.0328, -0.3047,  0.0750],\n",
       "                     device='cuda:1')),\n",
       "             ('interactNN.layer2.weight',\n",
       "              tensor([[-0.0668,  0.0169, -0.0333,  ..., -0.0654,  0.0282,  0.0151],\n",
       "                      [ 0.0184,  0.0738, -0.0616,  ...,  0.0785, -0.0286,  0.0094],\n",
       "                      [-0.0079, -0.0738,  0.0184,  ...,  0.0444, -0.0290, -0.0391],\n",
       "                      ...,\n",
       "                      [-0.0757,  0.0241,  0.0486,  ...,  0.0065,  0.0026,  0.0665],\n",
       "                      [-0.0084,  0.0055,  0.0299,  ...,  0.0834,  0.0785,  0.0686],\n",
       "                      [ 0.0283,  0.0494, -0.0478,  ..., -0.0941,  0.0803, -0.0010]],\n",
       "                     device='cuda:1')),\n",
       "             ('interactNN.layer2.bias',\n",
       "              tensor([-0.1030,  0.0743, -0.1039, -0.0218,  0.1589,  0.0827,  0.0745, -0.0350,\n",
       "                      -0.0163,  0.1066, -0.0182, -0.0396, -0.0561, -0.0352, -0.0035, -0.0492,\n",
       "                      -0.1164,  0.0779, -0.0039,  0.0598,  0.1519,  0.0091, -0.0791,  0.0315,\n",
       "                      -0.0145, -0.0156,  0.0195,  0.0806, -0.1915, -0.0255,  0.0225,  0.1280,\n",
       "                       0.0086,  0.1194, -0.0487,  0.0153,  0.0258, -0.0136,  0.1072,  0.0459,\n",
       "                       0.0607,  0.0017, -0.0730,  0.0428, -0.0863, -0.0206, -0.0024, -0.0721,\n",
       "                      -0.0515,  0.0643,  0.0646, -0.0760,  0.1032, -0.0862, -0.0036, -0.0221,\n",
       "                       0.1272, -0.0732, -0.0494,  0.1638,  0.0966,  0.0567, -0.0454, -0.0535,\n",
       "                      -0.0249, -0.0686,  0.0075, -0.0256,  0.1195,  0.0663,  0.0996, -0.0052,\n",
       "                      -0.0719, -0.0916, -0.0347,  0.0486,  0.0631, -0.0165, -0.0538, -0.0655,\n",
       "                      -0.0021,  0.0681,  0.0970, -0.0646,  0.0121, -0.0098, -0.0420, -0.0554,\n",
       "                       0.0216,  0.0404,  0.0373,  0.0614,  0.0528, -0.0869, -0.0264,  0.0529,\n",
       "                       0.0288, -0.0618, -0.1144, -0.0489, -0.0059, -0.0140, -0.0114, -0.0436,\n",
       "                      -0.0175, -0.0445,  0.0441,  0.0004, -0.0669, -0.0259, -0.0749,  0.0091,\n",
       "                       0.0301,  0.1248, -0.0448, -0.0699, -0.0189,  0.0102, -0.0655, -0.0675,\n",
       "                       0.0256, -0.0512, -0.1666,  0.0732,  0.0453,  0.0221, -0.0233, -0.1189],\n",
       "                     device='cuda:1')),\n",
       "             ('interactNN.layer3.weight',\n",
       "              tensor([[-0.1339,  0.0003,  0.0254,  ..., -0.0453, -0.0675,  0.0477],\n",
       "                      [-0.0333, -0.0089,  0.0090,  ..., -0.0323, -0.0644,  0.0448],\n",
       "                      [-0.0043, -0.0077, -0.0722,  ...,  0.0768, -0.0270,  0.0533],\n",
       "                      ...,\n",
       "                      [-0.0192, -0.0417, -0.0137,  ..., -0.1147, -0.0311,  0.0414],\n",
       "                      [-0.0654,  0.0273,  0.0095,  ..., -0.0299,  0.0010, -0.0187],\n",
       "                      [-0.0041, -0.0880, -0.0874,  ..., -0.0316, -0.0502,  0.0008]],\n",
       "                     device='cuda:1')),\n",
       "             ('interactNN.layer3.bias',\n",
       "              tensor([-3.0175e-02,  5.4638e-02, -1.0050e-01,  2.0075e-01,  6.3115e-02,\n",
       "                       4.1813e-03,  5.1033e-02,  1.4305e-01, -1.7507e-02, -1.2743e-02,\n",
       "                      -5.5054e-02,  1.1906e-01, -8.6042e-02, -1.0539e-01, -5.0783e-05,\n",
       "                       9.2873e-02, -8.7933e-04,  9.0946e-02, -6.6508e-02,  9.0333e-02,\n",
       "                       6.3486e-02,  2.1351e-01, -6.2092e-02,  8.2018e-02,  1.9587e-01,\n",
       "                       8.3447e-02,  1.5490e-01,  7.1607e-02, -1.3199e-02, -1.6704e-03,\n",
       "                       1.4345e-01,  6.4984e-02,  4.1144e-02,  6.7756e-02, -3.1818e-02,\n",
       "                       1.4565e-01,  9.5908e-02,  3.8455e-02,  4.9465e-02, -7.9402e-02,\n",
       "                      -1.2315e-02,  4.0496e-02, -1.5525e-01, -1.2136e-02,  1.1070e-01,\n",
       "                       6.4743e-02, -5.0617e-02,  1.1670e-01,  2.9201e-02,  1.2194e-01,\n",
       "                       1.5314e-01,  8.0250e-02, -1.2516e-01, -1.7800e-01,  7.7962e-02,\n",
       "                       3.4597e-02,  9.0507e-02, -1.7894e-02,  2.1019e-03, -3.2532e-02,\n",
       "                       1.5144e-01, -7.0768e-02,  3.3158e-02, -9.6489e-02, -2.5092e-02,\n",
       "                       1.0960e-01,  1.8019e-01, -2.5527e-02, -2.7759e-02,  1.5240e-01,\n",
       "                      -8.0551e-02, -1.0729e-01,  1.0016e-01,  4.4897e-02, -1.1889e-01,\n",
       "                       2.0276e-02, -6.0132e-02,  2.2324e-03,  8.6445e-02,  1.1557e-01,\n",
       "                       9.8926e-03,  4.3468e-02,  1.2414e-01, -1.8147e-01, -3.6219e-02,\n",
       "                      -1.4412e-01, -8.9938e-02, -1.4632e-02, -5.2608e-02, -2.1929e-02,\n",
       "                       3.1930e-02,  7.9420e-02, -8.0841e-02,  1.9032e-01,  2.0213e-02,\n",
       "                      -6.0936e-02,  1.5627e-01,  7.6405e-02,  8.7800e-02,  1.0074e-01,\n",
       "                       1.5278e-01,  2.2075e-01, -7.0543e-02, -4.6296e-03,  8.1769e-03,\n",
       "                      -7.6628e-02,  3.6217e-02, -1.3937e-01,  1.4593e-01, -4.8144e-02,\n",
       "                       7.1522e-03,  1.7368e-01,  1.6488e-01, -3.4612e-02,  1.1956e-01,\n",
       "                       2.1880e-02, -3.3280e-02,  7.6245e-02, -7.4405e-02, -1.5052e-01,\n",
       "                       4.8622e-02,  5.7530e-02,  6.8321e-03, -9.9571e-03, -3.7206e-03,\n",
       "                       1.1444e-01, -5.6750e-02,  8.9807e-02], device='cuda:1')),\n",
       "             ('interactNN.layer4.weight',\n",
       "              tensor([[ 2.3713e-01,  7.2071e-02, -9.9760e-02, -5.5385e-01, -2.6886e-01,\n",
       "                       -1.7826e-01,  8.4607e-03,  1.3754e-01,  9.0726e-02,  2.0243e-01,\n",
       "                        3.0726e-02,  3.4133e-01,  5.9895e-02,  9.2525e-02,  4.4644e-02,\n",
       "                        3.4917e-02, -1.3479e-01,  2.5857e-02, -2.8088e-02, -1.6289e-01,\n",
       "                       -1.6613e-01, -4.2974e-01,  1.2264e-03,  2.0115e-01,  3.1693e-01,\n",
       "                       -7.4032e-02,  3.8394e-01, -1.9610e-01, -1.9735e-05, -1.3432e-01,\n",
       "                        1.9828e-01, -1.5770e-01,  4.8913e-01, -7.4956e-02, -1.6940e-01,\n",
       "                        3.0144e-01,  2.4360e-01, -1.5569e-01,  2.0862e-01,  3.4340e-02,\n",
       "                        2.0448e-02,  5.5740e-02, -2.0203e-01,  2.3093e-02,  1.8615e-01,\n",
       "                        5.0358e-02, -7.2085e-03,  9.0151e-02, -3.4652e-02, -7.5827e-02,\n",
       "                       -2.4534e-01,  1.2791e-01,  1.2416e-01,  3.7301e-01, -6.3351e-02,\n",
       "                        8.1374e-02,  6.2184e-02,  2.9680e-02, -2.7552e-02, -4.0294e-02,\n",
       "                       -2.5601e-01, -3.8547e-02,  9.6805e-02,  5.9780e-02, -1.4763e-01,\n",
       "                       -1.6725e-02,  2.8888e-01, -1.1819e-01, -6.8701e-03, -1.1989e-01,\n",
       "                       -4.8700e-02, -2.0427e-01,  1.6289e-01,  9.7717e-02,  2.5255e-01,\n",
       "                       -1.1761e-01, -6.9853e-02,  1.1768e-01,  1.6305e-01,  1.3889e-01,\n",
       "                        1.0464e-02, -2.9495e-02,  9.6641e-02,  1.1247e-01, -6.7760e-02,\n",
       "                        1.5489e-01,  1.3533e-01,  3.6044e-02, -1.2513e-01,  3.5614e-02,\n",
       "                        1.1390e-01,  1.7968e-01, -8.0003e-03,  2.2170e-01, -1.4355e-01,\n",
       "                        2.0068e-02, -2.6251e-01, -1.8569e-02,  1.0137e-01, -1.9730e-02,\n",
       "                        1.8958e-01,  4.6756e-01, -1.7064e-01, -7.5612e-02, -1.6703e-01,\n",
       "                       -6.3134e-02,  5.2967e-03, -1.4772e-01, -3.3229e-01,  1.4404e-01,\n",
       "                        6.1336e-02,  2.9653e-01,  2.3681e-01, -5.9265e-02, -6.2626e-02,\n",
       "                        2.3902e-01, -4.8297e-02, -1.3042e-02, -8.3069e-02, -4.2449e-02,\n",
       "                       -4.0389e-02,  1.6235e-01, -2.2158e-02,  7.0257e-02, -7.1980e-02,\n",
       "                       -1.8568e-01, -7.5284e-02, -9.8213e-02],\n",
       "                      [-1.0186e-01,  6.1817e-02,  7.9131e-02, -2.1916e-01, -6.4206e-02,\n",
       "                        3.3883e-02, -4.5531e-02,  1.5567e-01, -1.4060e-01,  1.2102e-01,\n",
       "                       -7.7791e-02,  4.3738e-02,  6.9942e-02, -5.1111e-02, -6.7343e-02,\n",
       "                        6.8033e-02,  6.6960e-02, -1.3214e-01,  1.1581e-01,  9.4375e-02,\n",
       "                       -7.3219e-02, -2.9435e-01, -1.2125e-02,  1.8134e-01,  2.7962e-01,\n",
       "                       -1.9580e-01, -1.7029e-01, -1.5712e-01, -3.1310e-02,  1.5481e-01,\n",
       "                       -1.9335e-01,  2.0257e-01, -1.8801e-01,  5.1039e-03, -3.4866e-02,\n",
       "                        3.2504e-01, -6.5205e-02,  8.0336e-02, -2.0592e-01,  5.8456e-02,\n",
       "                       -9.2384e-02,  8.2883e-02,  2.2125e-01, -1.5187e-01,  4.2308e-01,\n",
       "                        9.2195e-02,  1.4902e-02,  2.7216e-01,  1.3450e-01, -2.5913e-01,\n",
       "                       -5.4054e-02, -2.0382e-01, -2.1567e-01, -6.4746e-02,  2.2422e-01,\n",
       "                        2.8944e-02, -3.1771e-01,  2.4474e-02, -1.0745e-01,  8.5949e-02,\n",
       "                       -2.7824e-02,  2.3533e-01, -7.4001e-03, -2.8553e-02, -6.6059e-02,\n",
       "                        1.6250e-01,  8.0376e-02, -4.0005e-02,  2.7457e-02,  4.8420e-01,\n",
       "                        1.4344e-02,  1.5424e-01,  2.5944e-01,  8.0920e-03,  2.9894e-02,\n",
       "                        5.1682e-02,  1.5170e-02, -1.4392e-01,  6.4869e-02,  1.6212e-01,\n",
       "                        1.3582e-02,  4.5180e-02, -2.8319e-01, -3.1721e-01,  1.1107e-01,\n",
       "                        4.9858e-02,  1.6792e-01,  5.4798e-02,  1.2267e-01,  8.6678e-02,\n",
       "                        1.1065e-01,  1.1371e-01,  4.0058e-03,  2.0641e-01, -5.6639e-02,\n",
       "                       -1.7313e-02,  1.3863e-01,  1.4174e-01, -2.2703e-01, -2.8000e-01,\n",
       "                       -2.5793e-01, -7.4678e-02, -1.0209e-02,  2.6109e-02, -1.6669e-01,\n",
       "                       -8.2401e-02, -4.9184e-02,  1.8432e-01,  1.6536e-01, -4.5995e-02,\n",
       "                        7.0334e-02,  1.3965e-01, -3.7337e-01,  1.3222e-01,  2.5584e-01,\n",
       "                        5.6996e-03,  1.0466e-01,  1.7378e-01, -2.7208e-02,  1.3355e-01,\n",
       "                       -3.4379e-02, -3.4461e-02,  1.6938e-02, -6.0353e-03, -2.6974e-01,\n",
       "                        1.9726e-01,  7.4258e-02,  1.0622e-01]], device='cuda:1')),\n",
       "             ('interactNN.layer4.bias',\n",
       "              tensor([-0.3892, -0.0828], device='cuda:1')),\n",
       "             ('thetaDotNN.layer1.weight',\n",
       "              tensor([[ 7.4943e-03,  3.6430e-01, -9.7822e-02, -2.7230e-01,  3.5963e-01,\n",
       "                       -3.8458e-01],\n",
       "                      [ 9.1144e-02,  3.9407e-01,  2.8300e-01, -3.3730e-01, -1.2162e-02,\n",
       "                        3.3366e-01],\n",
       "                      [ 9.7412e-02,  2.7804e-01, -1.2161e-01,  2.9148e-01, -1.5190e-03,\n",
       "                        2.4034e-01],\n",
       "                      [-3.1832e-01, -1.3279e-01, -3.8906e-01, -4.8472e-02, -1.2655e-01,\n",
       "                       -2.5074e-01],\n",
       "                      [-1.9514e-01,  1.2246e-01, -1.9228e-01, -1.9679e-01, -2.1232e-01,\n",
       "                       -3.3845e-01],\n",
       "                      [-1.6378e-01,  1.4638e-01,  1.0166e-01, -2.4589e-01, -3.1583e-01,\n",
       "                       -1.9258e-01],\n",
       "                      [ 4.6353e-03,  1.3976e-01, -1.8539e-01, -1.0164e-01, -5.5464e-02,\n",
       "                       -1.3620e-01],\n",
       "                      [ 6.4912e-02,  9.9379e-02, -3.7990e-01,  1.4108e-01, -2.6272e-01,\n",
       "                       -2.8397e-01],\n",
       "                      [-1.4204e-01,  8.1813e-02, -2.8501e-01, -3.4629e-01,  3.2458e-01,\n",
       "                        2.8368e-01],\n",
       "                      [-3.9583e-01, -2.8188e-01,  1.6568e-01,  5.0438e-02, -3.1285e-01,\n",
       "                       -1.9852e-01],\n",
       "                      [-1.2511e-01,  1.8126e-01,  1.1783e-01, -1.8358e-01, -1.4182e-01,\n",
       "                       -1.4848e-01],\n",
       "                      [ 2.0128e-01, -2.9212e-01,  4.3794e-02,  3.5471e-01,  3.4567e-01,\n",
       "                       -2.3328e-01],\n",
       "                      [ 4.1903e-01, -4.8832e-02,  1.3833e-01,  4.7491e-02, -7.1295e-02,\n",
       "                        1.9831e-01],\n",
       "                      [ 3.5398e-01,  7.6768e-02,  3.5724e-01, -1.7162e-01,  1.5879e-01,\n",
       "                        2.5527e-01],\n",
       "                      [ 1.2989e-01,  1.8536e-01, -2.7181e-01, -3.5910e-01,  3.9856e-01,\n",
       "                       -3.7544e-01],\n",
       "                      [ 6.7059e-03,  2.3998e-02,  1.1307e-01, -1.0358e-01, -3.8093e-01,\n",
       "                       -1.8204e-02],\n",
       "                      [ 4.2013e-01,  7.9248e-03,  1.2380e-01, -3.4146e-01,  1.1453e-01,\n",
       "                       -3.3587e-01],\n",
       "                      [-1.5270e-01,  1.7622e-01,  3.3678e-01,  6.7025e-03, -1.7388e-01,\n",
       "                       -1.1040e-01],\n",
       "                      [ 6.9777e-02,  1.9645e-01, -4.0385e-01, -3.8429e-01,  5.5965e-02,\n",
       "                       -3.2364e-01],\n",
       "                      [-1.1044e-01,  4.6685e-03, -2.6119e-02,  2.2610e-02,  1.6261e-01,\n",
       "                        1.5290e-01],\n",
       "                      [ 3.8956e-01, -1.5537e-01,  3.5622e-01,  2.5654e-01, -1.8839e-01,\n",
       "                        2.8435e-01],\n",
       "                      [-5.1552e-02,  1.6922e-02, -3.1890e-01,  4.4112e-02, -1.8386e-01,\n",
       "                        2.8979e-01],\n",
       "                      [ 1.0654e-01, -3.7047e-02, -2.4310e-01,  3.2939e-01,  2.5426e-01,\n",
       "                        1.0880e-01],\n",
       "                      [-2.7745e-01, -9.6837e-02,  1.0049e-01,  2.7084e-01, -1.1926e-01,\n",
       "                        9.9676e-02],\n",
       "                      [-2.3654e-01, -4.0585e-02,  2.2582e-01, -7.6814e-02,  5.3348e-02,\n",
       "                        1.5717e-01],\n",
       "                      [-2.1573e-02,  2.3881e-01,  1.7063e-01, -1.4503e-01, -2.6800e-01,\n",
       "                        3.0234e-01],\n",
       "                      [-3.0134e-01, -1.5287e-01,  3.6990e-01,  3.3192e-01, -8.4694e-02,\n",
       "                        7.5182e-02],\n",
       "                      [ 7.0992e-02,  1.9779e-01,  3.6267e-01,  2.1922e-01,  2.9149e-01,\n",
       "                        3.1182e-01],\n",
       "                      [ 1.2458e-01, -4.6361e-01,  3.6846e-01, -2.7723e-01, -3.1764e-01,\n",
       "                       -5.0028e-03],\n",
       "                      [-1.5434e-01, -4.5156e-01,  8.2063e-02, -2.0278e-02,  3.4244e-01,\n",
       "                        3.9716e-01],\n",
       "                      [ 5.1428e-02, -2.1907e-01, -1.2271e-01,  1.5360e-01,  2.7167e-01,\n",
       "                       -3.3990e-01],\n",
       "                      [ 3.1095e-01,  1.1273e-01, -1.3834e-01,  4.6969e-02, -2.4233e-01,\n",
       "                        7.3554e-02],\n",
       "                      [-4.2892e-01, -4.2375e-01,  2.6343e-02,  3.4888e-01,  3.8475e-01,\n",
       "                        2.2031e-01],\n",
       "                      [ 1.5139e-01, -1.7906e-01, -8.0669e-02, -1.2029e-01, -3.7087e-01,\n",
       "                        2.1082e-01],\n",
       "                      [ 1.3923e-01, -4.2131e-01,  3.7633e-01,  1.2585e-01, -3.0148e-01,\n",
       "                       -1.1105e-02],\n",
       "                      [-4.1415e-01,  1.2194e-01,  1.5365e-01,  2.2638e-01, -3.2537e-01,\n",
       "                        1.5315e-01],\n",
       "                      [ 2.3568e-01, -8.9149e-03,  3.0663e-01, -6.9534e-02,  2.4945e-01,\n",
       "                        2.5501e-01],\n",
       "                      [-3.7667e-01, -3.3027e-01, -1.0142e-01, -8.6769e-02, -2.9525e-01,\n",
       "                        2.4063e-02],\n",
       "                      [ 6.5367e-02, -1.4361e-01, -1.1519e-01,  2.2637e-01,  2.7083e-01,\n",
       "                       -1.7494e-01],\n",
       "                      [ 1.5295e-03, -2.5592e-01, -2.7273e-01, -8.3662e-02,  1.5668e-01,\n",
       "                       -1.8020e-01],\n",
       "                      [-1.8002e-01,  1.9107e-02,  2.6641e-01,  1.0652e-01, -2.5719e-02,\n",
       "                        8.1461e-03],\n",
       "                      [-1.7263e-01,  1.5701e-01,  8.6451e-02, -2.8229e-01, -3.9434e-01,\n",
       "                       -3.5828e-01],\n",
       "                      [-3.2826e-01,  7.2658e-03, -3.8743e-01,  3.0841e-01,  1.1992e-02,\n",
       "                       -2.5064e-01],\n",
       "                      [ 3.8587e-02,  4.3332e-01,  9.1100e-02,  2.7703e-01,  3.2290e-01,\n",
       "                       -2.5299e-01],\n",
       "                      [-1.5050e-02,  3.3899e-02, -5.0666e-02, -3.8940e-01, -2.5507e-01,\n",
       "                       -3.0325e-01],\n",
       "                      [ 2.2489e-01, -1.5793e-02,  3.9180e-01, -2.7752e-01, -3.4981e-01,\n",
       "                        6.2218e-02],\n",
       "                      [-4.3250e-01, -3.3917e-01,  2.2055e-01, -1.7750e-01, -2.9452e-01,\n",
       "                        7.8989e-02],\n",
       "                      [-3.2820e-01,  2.6988e-03, -2.6227e-01,  3.1111e-01, -2.6185e-01,\n",
       "                       -2.9708e-01],\n",
       "                      [ 1.9956e-01, -1.5345e-02, -3.3409e-01, -4.2519e-02, -3.0581e-01,\n",
       "                       -2.9885e-01],\n",
       "                      [ 3.7149e-01,  3.6569e-01, -5.8057e-02, -3.9561e-01,  2.9025e-02,\n",
       "                       -3.8820e-01],\n",
       "                      [ 3.8885e-01, -1.4591e-01,  3.4832e-01,  2.1402e-01, -1.3790e-01,\n",
       "                       -1.9774e-01],\n",
       "                      [ 3.3765e-01, -3.7371e-01, -2.2819e-01,  1.3272e-01,  4.0035e-01,\n",
       "                        2.2675e-01],\n",
       "                      [ 1.2242e-01, -2.8213e-01,  3.7509e-01,  2.2536e-01, -2.8728e-01,\n",
       "                       -2.6865e-01],\n",
       "                      [-4.7667e-01, -3.7146e-01, -1.1555e-01, -2.2266e-01,  2.3578e-01,\n",
       "                       -2.5434e-01],\n",
       "                      [ 1.0494e-01,  1.8723e-01, -3.8498e-01, -4.0714e-01, -2.7187e-01,\n",
       "                        1.2865e-01],\n",
       "                      [-3.7373e-01, -3.7119e-01,  3.4145e-01, -1.6233e-01, -3.1955e-01,\n",
       "                       -3.3000e-01],\n",
       "                      [ 8.4983e-02, -2.8306e-01,  2.9584e-01, -9.3853e-02, -1.2621e-01,\n",
       "                       -3.5149e-01],\n",
       "                      [ 5.0747e-02, -2.9433e-01, -3.2177e-01, -9.0377e-02, -3.4713e-01,\n",
       "                        3.9998e-01],\n",
       "                      [-4.0655e-01, -2.7570e-01, -3.7050e-02, -6.3858e-02,  2.2838e-01,\n",
       "                       -2.8012e-01],\n",
       "                      [-3.1297e-02,  2.0797e-01,  6.1077e-02,  4.8886e-02,  4.1788e-02,\n",
       "                       -1.8431e-02],\n",
       "                      [-3.6439e-01, -4.5558e-01, -3.7750e-01,  1.2096e-01,  3.9795e-01,\n",
       "                       -3.1465e-01],\n",
       "                      [-4.0682e-02,  3.1627e-01, -2.4345e-01,  1.8178e-01, -1.8735e-01,\n",
       "                       -2.1997e-02],\n",
       "                      [ 1.3052e-01, -1.0758e-01, -3.6484e-02, -4.6544e-02, -2.6563e-01,\n",
       "                       -1.9160e-01],\n",
       "                      [-3.3050e-01,  1.3185e-01,  3.7659e-01,  6.9805e-02,  2.5923e-04,\n",
       "                        3.5268e-01],\n",
       "                      [-2.3491e-01,  4.9156e-02,  2.5371e-01, -2.9178e-01,  3.9284e-01,\n",
       "                       -1.5886e-01],\n",
       "                      [ 1.6277e-01, -9.7820e-02,  1.3852e-02, -2.0896e-01, -9.1975e-03,\n",
       "                       -1.1594e-01],\n",
       "                      [ 2.0181e-01,  1.0348e-01,  7.6866e-02,  1.2353e-01,  2.7886e-01,\n",
       "                        2.9531e-01],\n",
       "                      [-2.3360e-01,  1.6715e-01,  3.6547e-01,  1.9246e-02,  3.4472e-01,\n",
       "                       -6.2736e-02],\n",
       "                      [ 1.1644e-02,  3.1868e-01,  3.3326e-01, -1.3945e-01,  3.9706e-02,\n",
       "                        1.2489e-02],\n",
       "                      [-9.6922e-02, -3.4392e-01, -3.8453e-01,  3.6285e-01,  3.0580e-01,\n",
       "                        1.0800e-01],\n",
       "                      [ 9.8610e-02, -3.6408e-01, -2.9602e-01, -3.6286e-01, -3.5299e-01,\n",
       "                        1.1704e-01],\n",
       "                      [ 3.0652e-01,  1.3536e-02, -1.3723e-01, -2.0408e-01,  3.0707e-01,\n",
       "                        1.0176e-01],\n",
       "                      [-3.5732e-01,  2.1505e-01, -7.5213e-02,  1.5978e-01,  1.1167e-01,\n",
       "                       -1.5845e-01],\n",
       "                      [ 1.6435e-01,  3.3558e-01,  2.5909e-01, -2.5297e-01,  1.7601e-01,\n",
       "                       -1.5720e-01],\n",
       "                      [-2.9867e-01,  5.2539e-01,  1.0426e-02,  1.3438e-01,  3.6143e-01,\n",
       "                        9.7911e-02],\n",
       "                      [-3.5799e-02, -2.0882e-01, -2.8427e-01,  3.2284e-01, -2.9836e-01,\n",
       "                       -2.1255e-01],\n",
       "                      [ 3.1099e-01,  2.0999e-01,  2.9583e-01,  3.6767e-01,  2.2744e-01,\n",
       "                       -4.3491e-03],\n",
       "                      [ 1.4906e-01,  2.2570e-02, -2.4016e-02,  2.0503e-01,  3.3316e-01,\n",
       "                        1.2960e-01],\n",
       "                      [ 3.1970e-01,  1.4248e-01, -7.3190e-02, -2.2381e-01, -7.3602e-02,\n",
       "                       -2.8230e-01],\n",
       "                      [ 1.8754e-01,  4.0087e-01, -2.9361e-01, -4.1640e-01, -3.5518e-01,\n",
       "                       -3.7084e-01],\n",
       "                      [ 1.2372e-01, -5.5565e-02,  1.9173e-01,  1.0691e-01,  1.4612e-01,\n",
       "                        6.9942e-02],\n",
       "                      [ 2.8374e-01,  6.4855e-02, -3.3007e-01,  3.6228e-01,  2.5162e-02,\n",
       "                        3.2929e-01],\n",
       "                      [ 3.5087e-01,  4.2902e-01,  2.9214e-02, -8.1711e-02, -2.9221e-01,\n",
       "                       -4.3479e-02],\n",
       "                      [-4.2780e-01, -4.5513e-02,  1.9296e-01, -1.2467e-01,  3.6015e-01,\n",
       "                       -1.5102e-01],\n",
       "                      [-8.9546e-02, -9.9280e-02,  9.5508e-03,  3.2348e-01, -2.1468e-01,\n",
       "                       -2.4088e-01],\n",
       "                      [ 4.5880e-01,  1.2351e-01,  3.5046e-01, -5.8510e-02,  2.8134e-01,\n",
       "                        7.1558e-02],\n",
       "                      [-3.2508e-01,  7.9233e-02, -4.7536e-02,  2.2169e-01,  2.0880e-01,\n",
       "                        3.3477e-01],\n",
       "                      [-1.2989e-01, -4.1426e-01,  2.4687e-01, -4.1072e-01,  2.7398e-01,\n",
       "                       -1.6366e-01],\n",
       "                      [-2.8259e-01,  4.1223e-01, -4.7633e-02,  3.1938e-01, -3.0325e-01,\n",
       "                       -9.9231e-02],\n",
       "                      [ 2.7594e-01, -2.5756e-01,  3.3782e-01, -1.2560e-01,  2.1503e-01,\n",
       "                       -3.1351e-01],\n",
       "                      [-1.2703e-01, -2.7830e-01, -2.0126e-01,  6.3646e-02, -2.0874e-02,\n",
       "                       -3.3177e-01],\n",
       "                      [ 2.7190e-01, -4.0263e-01,  3.5252e-01, -2.6830e-01,  8.4652e-02,\n",
       "                       -2.2521e-01],\n",
       "                      [ 3.1131e-01, -1.0090e-01, -2.8869e-01, -1.5639e-01,  3.1511e-01,\n",
       "                       -2.0341e-01],\n",
       "                      [-4.9333e-01, -2.0796e-01,  8.1331e-02,  8.5002e-02, -3.1921e-01,\n",
       "                       -5.9871e-02],\n",
       "                      [-3.6893e-01, -1.1016e-01, -3.0300e-03, -1.3327e-01, -2.5774e-01,\n",
       "                        3.6438e-01],\n",
       "                      [ 1.4870e-02,  1.6601e-01, -2.3525e-01,  1.6023e-01, -2.1623e-01,\n",
       "                       -3.4617e-01],\n",
       "                      [ 3.8485e-01,  2.9634e-01, -2.4076e-01,  3.2781e-01,  2.8590e-01,\n",
       "                       -4.0406e-01],\n",
       "                      [ 2.2169e-01,  1.3423e-02,  1.9446e-01,  3.6490e-01, -2.2899e-01,\n",
       "                        1.8663e-01],\n",
       "                      [-1.8285e-02, -4.4826e-01,  3.0481e-01, -3.0691e-01, -6.8377e-02,\n",
       "                        9.0152e-02],\n",
       "                      [ 1.8491e-01, -3.1848e-01, -3.7212e-01,  7.7895e-02, -1.5128e-01,\n",
       "                       -5.9360e-02],\n",
       "                      [ 6.8946e-02,  1.5987e-01, -1.6970e-01,  1.0023e-01, -2.2329e-01,\n",
       "                        1.6341e-01],\n",
       "                      [-2.3420e-01, -4.0385e-01, -6.2447e-02, -2.6744e-01, -3.6713e-01,\n",
       "                       -3.7617e-01],\n",
       "                      [ 6.4836e-02,  2.5984e-01,  1.8295e-01, -2.6491e-01,  2.6119e-01,\n",
       "                        9.9938e-02],\n",
       "                      [ 4.0706e-01, -2.0698e-01, -3.3346e-01, -3.5413e-01,  2.4114e-01,\n",
       "                       -2.6945e-01],\n",
       "                      [ 2.8635e-01, -3.5684e-01,  3.6699e-01, -1.8672e-02, -1.1696e-01,\n",
       "                       -1.7334e-01],\n",
       "                      [-1.8698e-01,  8.4603e-02,  2.3155e-01,  2.6576e-02, -7.6963e-04,\n",
       "                       -1.8905e-01],\n",
       "                      [ 1.8564e-01, -1.7947e-01, -3.8745e-01, -3.0632e-01,  2.7381e-01,\n",
       "                       -2.5788e-01],\n",
       "                      [ 2.6073e-01, -1.3147e-01,  2.0813e-01, -3.0958e-01, -2.6460e-01,\n",
       "                        3.1610e-01],\n",
       "                      [ 3.9012e-01, -3.9966e-01, -2.6383e-01, -2.2717e-01, -9.9295e-02,\n",
       "                        3.4905e-02],\n",
       "                      [-1.5498e-01,  6.2994e-02, -1.6644e-01, -1.9528e-01,  1.0097e-01,\n",
       "                        3.1780e-01],\n",
       "                      [ 1.0675e-01,  3.3051e-01, -1.2547e-01, -3.9739e-01,  2.2010e-02,\n",
       "                       -3.2884e-01],\n",
       "                      [ 4.9900e-02, -1.7983e-01,  1.7301e-01,  2.0117e-01, -2.5676e-01,\n",
       "                       -3.5367e-01],\n",
       "                      [-3.6286e-02,  4.1450e-01,  1.7548e-02, -1.2627e-01, -7.6967e-02,\n",
       "                       -2.2113e-01],\n",
       "                      [ 1.7540e-01,  2.8093e-01,  1.5438e-01, -2.2127e-01,  8.2296e-02,\n",
       "                        2.4170e-01],\n",
       "                      [-1.7966e-01, -1.6222e-01,  2.8702e-02, -3.2162e-01,  2.5574e-01,\n",
       "                       -2.0211e-01],\n",
       "                      [-2.9145e-01,  2.5563e-01,  2.0950e-02, -1.8026e-01,  1.1778e-01,\n",
       "                       -9.0043e-02],\n",
       "                      [-8.2595e-04, -1.8925e-01, -9.9816e-02,  1.8645e-01,  8.2751e-02,\n",
       "                       -3.7634e-02],\n",
       "                      [-5.7038e-02,  3.2757e-01, -1.4184e-01,  4.8400e-02, -4.9008e-02,\n",
       "                        1.0140e-01],\n",
       "                      [-3.3273e-01,  4.7022e-01, -3.3179e-01, -2.3138e-01, -3.1176e-01,\n",
       "                        3.4450e-01],\n",
       "                      [-3.7080e-01,  3.3538e-01, -1.5331e-02,  3.5365e-02, -2.0370e-01,\n",
       "                       -2.8395e-01],\n",
       "                      [-4.2090e-01, -1.7005e-01,  1.5680e-01,  2.7294e-01,  2.9451e-01,\n",
       "                        1.7977e-01],\n",
       "                      [ 3.9266e-01,  2.0083e-01,  4.1431e-01,  3.1006e-01, -3.5290e-01,\n",
       "                       -9.6820e-02],\n",
       "                      [ 1.6197e-01, -3.1179e-01,  6.7011e-04, -3.7624e-01,  2.9506e-01,\n",
       "                        2.1224e-01],\n",
       "                      [-2.9809e-01, -3.5624e-01, -2.6907e-01,  8.6920e-02, -2.6836e-01,\n",
       "                        1.4213e-01],\n",
       "                      [ 2.1266e-01,  1.9396e-02,  2.0419e-01,  3.2450e-01, -3.2683e-01,\n",
       "                       -5.3532e-02],\n",
       "                      [-2.3708e-01, -2.6340e-01,  2.1671e-02,  2.7920e-01,  1.7162e-01,\n",
       "                       -1.1099e-01],\n",
       "                      [ 2.9828e-01, -1.5282e-01, -1.9728e-01,  2.2024e-01,  2.3848e-01,\n",
       "                       -3.0505e-01],\n",
       "                      [ 1.2458e-03,  4.0079e-01, -1.4236e-01,  2.0638e-01,  2.2559e-01,\n",
       "                        1.4372e-01]], device='cuda:1')),\n",
       "             ('thetaDotNN.layer1.bias',\n",
       "              tensor([-0.0065, -0.1746, -0.1004,  0.2460, -0.1504,  0.1441, -0.3843, -0.0025,\n",
       "                       0.2291,  0.3294,  0.2315,  0.1481, -0.0065, -0.1453,  0.3232,  0.1754,\n",
       "                      -0.0461,  0.0116,  0.1032,  0.3331,  0.3524,  0.2042, -0.3628,  0.1140,\n",
       "                       0.0346,  0.2869, -0.0715,  0.1860,  0.1279,  0.1807,  0.1225, -0.2998,\n",
       "                      -0.2955, -0.2929, -0.2863,  0.1828, -0.3046, -0.1937,  0.3022, -0.2615,\n",
       "                       0.2567, -0.1378,  0.3692, -0.1176, -0.3356, -0.1408,  0.3210,  0.2073,\n",
       "                       0.3688,  0.0068,  0.0766,  0.1124, -0.3916,  0.1595, -0.1349,  0.2463,\n",
       "                       0.1961,  0.2326, -0.1206,  0.0441,  0.0924, -0.3578,  0.0989,  0.3357,\n",
       "                      -0.3852, -0.4055, -0.2951,  0.3139,  0.0910, -0.1064,  0.2903,  0.0312,\n",
       "                      -0.0160, -0.2023,  0.2607, -0.0419, -0.3010, -0.3025,  0.0633, -0.0009,\n",
       "                      -0.0080, -0.0048,  0.4196,  0.1981, -0.3341, -0.2041,  0.0948, -0.0885,\n",
       "                       0.0523, -0.1063, -0.0661,  0.3099, -0.1144, -0.0458, -0.1495,  0.1108,\n",
       "                      -0.0622, -0.3916,  0.1086,  0.2321, -0.3125, -0.3231, -0.3589,  0.1354,\n",
       "                      -0.3259,  0.3745, -0.3110, -0.1706, -0.1370,  0.3580, -0.2096,  0.2141,\n",
       "                       0.4287,  0.1733, -0.3503,  0.3612, -0.3225,  0.1658,  0.3670, -0.2916,\n",
       "                       0.0600, -0.0984, -0.2839,  0.1928, -0.4046, -0.0419, -0.0086, -0.0188],\n",
       "                     device='cuda:1')),\n",
       "             ('thetaDotNN.layer2.weight',\n",
       "              tensor([[-0.0569, -0.0468,  0.0737,  ...,  0.0651, -0.0820,  0.0350],\n",
       "                      [-0.0241, -0.0555, -0.0537,  ..., -0.0702, -0.0177, -0.0919],\n",
       "                      [ 0.0357, -0.0516,  0.0378,  ..., -0.0178,  0.0295, -0.0444],\n",
       "                      ...,\n",
       "                      [-0.0221,  0.0231,  0.0624,  ...,  0.0577,  0.0420,  0.0220],\n",
       "                      [ 0.0570, -0.0376,  0.0272,  ...,  0.0626,  0.0397, -0.0497],\n",
       "                      [ 0.0073, -0.0062, -0.0583,  ...,  0.0821, -0.0125,  0.0736]],\n",
       "                     device='cuda:1')),\n",
       "             ('thetaDotNN.layer2.bias',\n",
       "              tensor([ 1.6307e-02,  9.2544e-03,  6.2295e-02, -7.4133e-02,  2.4186e-03,\n",
       "                      -3.0355e-03,  1.0112e-01, -3.2551e-02,  7.1695e-02,  2.5786e-02,\n",
       "                      -3.3247e-02, -8.6947e-02,  8.9234e-03, -9.2997e-03, -6.3155e-02,\n",
       "                       2.6694e-02,  2.9050e-02, -2.1812e-02,  6.3633e-02, -4.9547e-02,\n",
       "                       3.7423e-02, -2.4172e-03,  1.2020e-01, -6.0411e-02, -4.3936e-02,\n",
       "                      -7.3020e-02,  4.7208e-03, -1.3588e-01,  7.1357e-02, -6.3054e-02,\n",
       "                      -5.9265e-02,  3.1374e-02, -4.3637e-02,  5.5296e-02, -4.7762e-02,\n",
       "                      -8.0601e-02,  9.7559e-02, -5.0384e-02, -6.4247e-03,  9.8132e-02,\n",
       "                       4.0088e-02, -8.2778e-02, -3.2733e-02, -7.8689e-02,  9.0254e-02,\n",
       "                       4.7665e-02,  9.7062e-02,  2.1999e-02,  4.0135e-02,  4.9851e-02,\n",
       "                      -1.0438e-02,  9.6799e-02,  6.0571e-02,  6.8647e-02, -4.6107e-02,\n",
       "                      -9.8916e-02, -5.6212e-02,  2.6120e-03,  7.3818e-02,  5.9639e-03,\n",
       "                       3.2689e-02,  2.5208e-02,  8.7447e-04, -1.1041e-01, -7.0866e-02,\n",
       "                       6.9002e-02, -5.1020e-02,  8.7884e-02, -4.5964e-02, -8.8416e-02,\n",
       "                       3.8557e-02, -1.7520e-03, -6.5954e-02, -8.8132e-02, -1.5707e-02,\n",
       "                      -2.0091e-02,  4.0303e-02, -4.2854e-02, -8.6100e-03, -2.1454e-03,\n",
       "                      -7.3050e-02, -1.1987e-01,  2.1908e-02, -4.1375e-03,  7.1959e-02,\n",
       "                       4.9633e-02, -2.4054e-03, -9.0752e-02, -2.9159e-02, -4.3114e-03,\n",
       "                      -5.8353e-02, -6.8406e-02, -4.5948e-02, -1.2111e-02, -6.0787e-02,\n",
       "                       7.6163e-02, -7.7155e-02, -2.2944e-02, -1.3871e-02,  5.0864e-02,\n",
       "                      -5.3390e-02,  1.0788e-01,  5.0963e-02,  1.0898e-01,  7.5355e-02,\n",
       "                       3.1447e-02, -7.2898e-02, -7.6541e-02, -5.5852e-02,  1.1404e-01,\n",
       "                      -3.3595e-02, -9.5311e-03,  1.6465e-02, -2.5102e-02,  5.1941e-02,\n",
       "                       7.9465e-02,  7.3077e-03, -3.3475e-02,  1.3030e-03, -8.4094e-02,\n",
       "                       1.3315e-02,  1.1029e-02, -1.3204e-04,  1.2938e-02, -2.0002e-02,\n",
       "                      -3.9712e-02,  8.5353e-02, -5.1980e-02], device='cuda:1')),\n",
       "             ('thetaDotNN.layer3.weight',\n",
       "              tensor([[ 0.0026, -0.0448,  0.0609,  ..., -0.0021, -0.0785, -0.0509],\n",
       "                      [-0.0542, -0.0218, -0.0496,  ..., -0.0122,  0.0671, -0.0724],\n",
       "                      [ 0.0223,  0.0504,  0.0538,  ..., -0.0328, -0.0129, -0.0322],\n",
       "                      ...,\n",
       "                      [-0.0490,  0.0722,  0.0610,  ...,  0.0346, -0.0444,  0.0045],\n",
       "                      [-0.0541, -0.0727,  0.0828,  ...,  0.0870, -0.0469,  0.0408],\n",
       "                      [-0.0607,  0.0593, -0.0634,  ...,  0.0363,  0.0189, -0.0558]],\n",
       "                     device='cuda:1')),\n",
       "             ('thetaDotNN.layer3.bias',\n",
       "              tensor([-0.0865,  0.0131,  0.0751,  0.0397,  0.0125, -0.0197,  0.0384,  0.0561,\n",
       "                       0.0041,  0.0362, -0.0224,  0.0825,  0.1063,  0.0150, -0.0543, -0.0731,\n",
       "                       0.0981, -0.0701,  0.0280,  0.0008, -0.0052, -0.0553,  0.0425, -0.0238,\n",
       "                      -0.0476,  0.0245,  0.0043, -0.0175, -0.0923,  0.0193, -0.0397,  0.0021,\n",
       "                       0.0334,  0.0437,  0.0981,  0.0707,  0.1512, -0.0668,  0.0045,  0.1165,\n",
       "                       0.0687,  0.0201,  0.0987,  0.1174, -0.0182,  0.0083,  0.0500,  0.1181,\n",
       "                      -0.0668, -0.0439,  0.0475,  0.0329,  0.0480,  0.0027,  0.0884,  0.0332,\n",
       "                      -0.0084, -0.0684, -0.0410, -0.0307,  0.1708,  0.0310, -0.0489,  0.0292,\n",
       "                       0.0506, -0.0762,  0.0311,  0.0441, -0.0442, -0.0113,  0.0847, -0.0554,\n",
       "                       0.0982,  0.0155, -0.0025, -0.0845,  0.1180,  0.0617, -0.0717,  0.1135,\n",
       "                      -0.0977, -0.0119, -0.0238, -0.0583,  0.0553, -0.0430, -0.0148,  0.0168,\n",
       "                       0.0471,  0.0852,  0.0338, -0.0743,  0.1870,  0.1244, -0.2249,  0.0990,\n",
       "                       0.0820,  0.0250, -0.0713, -0.0819,  0.0367, -0.0113,  0.0274,  0.1191,\n",
       "                       0.0913, -0.0164, -0.0390,  0.0657, -0.1849, -0.0178, -0.0815,  0.0807,\n",
       "                      -0.0642,  0.1308, -0.0529,  0.0032,  0.0984, -0.0951, -0.0323,  0.0172,\n",
       "                      -0.0319, -0.0659,  0.1016,  0.0673,  0.0423, -0.0432,  0.0998,  0.0814],\n",
       "                     device='cuda:1')),\n",
       "             ('thetaDotNN.layer4.weight',\n",
       "              tensor([[-0.1250, -0.0019, -0.1533, -0.0547, -0.0239, -0.0092,  0.0779,  0.0633,\n",
       "                        0.0240,  0.1037,  0.0200,  0.1973,  0.2272, -0.0616,  0.1250, -0.0791,\n",
       "                        0.2026, -0.0416,  0.1545,  0.0836, -0.0248, -0.0332, -0.0501, -0.0526,\n",
       "                        0.0201,  0.1703, -0.1086,  0.0545, -0.0383, -0.0493, -0.0657, -0.1712,\n",
       "                       -0.0537,  0.1206,  0.1527,  0.0017,  0.2503, -0.0298, -0.1221, -0.1477,\n",
       "                       -0.1749,  0.0394, -0.0835, -0.3136, -0.0328, -0.0561,  0.0814, -0.1596,\n",
       "                       -0.0132,  0.0301,  0.0553,  0.0099,  0.1341, -0.0605, -0.1944,  0.0087,\n",
       "                       -0.0089,  0.0427,  0.0783,  0.1233,  0.3404, -0.0568, -0.0914, -0.0518,\n",
       "                        0.0630, -0.0145,  0.1242, -0.0930,  0.1349,  0.0626,  0.0732, -0.0687,\n",
       "                        0.2285,  0.0912,  0.1432,  0.1377, -0.1627,  0.0189,  0.0461, -0.2072,\n",
       "                       -0.1016, -0.0115,  0.0082,  0.0286,  0.2378,  0.1033, -0.0504,  0.0834,\n",
       "                       -0.1005, -0.1270, -0.1229,  0.0598,  0.2939,  0.1676,  0.4320, -0.2381,\n",
       "                       -0.1577,  0.0807, -0.0496,  0.0416,  0.0369,  0.0716,  0.0606,  0.3293,\n",
       "                       -0.1776, -0.0961,  0.0550, -0.1942,  0.3108, -0.1903,  0.0543, -0.1437,\n",
       "                       -0.0278,  0.1078, -0.0194,  0.0262,  0.0988, -0.1989,  0.0447,  0.0543,\n",
       "                       -0.0225, -0.0638, -0.1919,  0.0788,  0.0186,  0.1247,  0.1383, -0.1661]],\n",
       "                     device='cuda:1')),\n",
       "             ('thetaDotNN.layer4.bias', tensor([-0.0022], device='cuda:1'))])"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "44265599",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-03T17:24:37.251518Z",
     "start_time": "2023-03-03T17:24:28.650466Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss: [0.0373, 0.0494]\n"
     ]
    }
   ],
   "source": [
    "# モデルを評価モードに設定\n",
    "stored_model.eval()\n",
    "\n",
    "# 推論\n",
    "test_lossv = 0\n",
    "test_losstheta = 0\n",
    "test_count = 0\n",
    "with torch.no_grad():\n",
    "    for batch_x, batch_y, batch_i_dir in test_data:\n",
    "        for ib in range(batch_x.size(0)):\n",
    "            model.load_celltypes(celltype_lst[int(batch_i_dir[ib])].to(device))\n",
    "            test_out = model(batch_x[ib].to(device))\n",
    "            lv, ltheta = myLoss(test_out, batch_y[ib].to(device))\n",
    "            test_lossv = test_lossv + lv\n",
    "            test_losstheta = test_losstheta + ltheta\n",
    "        test_count = test_count + batch_x.size(0)\n",
    "test_lossv = test_lossv/test_count\n",
    "test_losstheta = test_losstheta/test_count\n",
    "print('test Loss: [%.4f, %.4f]' % (test_lossv.item(), test_losstheta.item()))\n",
    "test_loss = [test_lossv.item(), test_losstheta.item()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "d0283033",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-03T17:24:37.256404Z",
     "start_time": "2023-03-03T17:24:37.253073Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import datetime\n",
    "\n",
    "now = datetime.datetime.now()\n",
    "nowstr = now.strftime('%Y%m%d_%H%M%S')\n",
    "\n",
    "os.makedirs(savedirName + nowstr + '/', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "445915ef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-03T17:24:37.263506Z",
     "start_time": "2023-03-03T17:24:37.258497Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9251, device='cuda:1')"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stored_model.selfpropel.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "62154d31",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-03T17:24:37.287074Z",
     "start_time": "2023-03-03T17:24:37.265319Z"
    }
   },
   "outputs": [],
   "source": [
    "stored_model = stored_model.to('cpu')\n",
    "\n",
    "filename1 = savedirName + nowstr + '/' + nowstr + '_Model.pkl'\n",
    "with open(filename1, \"wb\") as f:\n",
    "    pickle.dump(stored_model, f)\n",
    "\n",
    "filename1_2 = savedirName + nowstr + '/' + nowstr + '_Model.pt'\n",
    "torch.save(stored_model, filename1_2)\n",
    "\n",
    "filename2 = savedirName + nowstr + '/' + nowstr\n",
    "torch.save(stored_model.interactNN.state_dict(), filename2 + '_interactNN.pkl')\n",
    "torch.save(stored_model.thetaDotNN.state_dict(), filename2 + '_thetaDotNN.pkl')\n",
    "torch.save(stored_model.selfpropel.detach(), filename2 + '_selfpropel.pkl')\n",
    "\n",
    "filename3 = savedirName + nowstr + '/' + nowstr + '_Separation.npz'\n",
    "np.savez(filename3, dr_thresh=dr_thresh, batch_size=batch_size,\n",
    "         train_inds=train_inds, valid_inds=valid_inds, test_inds=test_inds, \n",
    "         val_loss_log=val_loss_log, test_loss=test_loss)\n",
    "\n",
    "filename4 = savedirName + nowstr + '/' + nowstr + '_optimizer.pt'\n",
    "torch.save(optimizer, filename4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0bd69a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "265.683px",
    "left": "494px",
    "right": "20px",
    "top": "120px",
    "width": "535.833px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
