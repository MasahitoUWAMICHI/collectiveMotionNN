{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f855b76a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-01T11:00:55.256081Z",
     "start_time": "2023-03-01T11:00:55.253427Z"
    }
   },
   "outputs": [],
   "source": [
    "#!pip install dgl-cu113 dglgo -f https://data.dgl.ai/wheels/repo.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7807020c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-01T11:00:55.263565Z",
     "start_time": "2023-03-01T11:00:55.260121Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['DGLBACKEND'] = 'pytorch'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d12568c6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-01T11:00:55.285078Z",
     "start_time": "2023-03-01T11:00:55.279302Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "import dgl\n",
    "import dgl.function as fn\n",
    "\n",
    "import networkx as nx\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import gc\n",
    "\n",
    "# デバイス設定\n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "61e1f3ee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-01T11:00:55.291487Z",
     "start_time": "2023-03-01T11:00:55.287682Z"
    }
   },
   "outputs": [],
   "source": [
    "def printNPZ(npz):\n",
    "    for kw in npz.files:\n",
    "        print(kw, npz[kw])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0ba5ba52",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-01T11:00:55.318467Z",
     "start_time": "2023-03-01T11:00:55.294577Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v0 1.0\n",
      "r 1.0\n",
      "D 0.0\n",
      "A 0.0\n",
      "L 20\n",
      "rho 1.0\n",
      "beta 1.0\n",
      "A_CFs [0.9 0.5]\n",
      "A_CIL 0.0\n",
      "cellType_ratio [0.7 0.3]\n",
      "quiv_colors ['k' 'r']\n",
      "kappa 0.5\n",
      "A_Macdonalds [0.5 0.5]\n",
      "batch_size 400\n",
      "state_size 3\n",
      "brownian_size 1\n",
      "periodic True\n",
      "t_max 1000\n",
      "methodSDE heun\n",
      "isIto False\n",
      "stepSDE 0.01\n"
     ]
    }
   ],
   "source": [
    "dirName = '/home/uwamichi/jupyter/HiraiwaModel_chem20230227_160803/'\n",
    "savedirName = dirName + 'ActiveNet_vp_rotsym_batchNorm/'\n",
    "os.makedirs(savedirName, exist_ok=True)\n",
    "\n",
    "params = np.load(dirName+'params.npz')\n",
    "#traj = np.load(dirName+'result.npz')\n",
    "\n",
    "printNPZ(params)\n",
    "#printNPZ(traj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c4e73591",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-01T11:00:55.332654Z",
     "start_time": "2023-03-01T11:00:55.320173Z"
    }
   },
   "outputs": [],
   "source": [
    "if params['periodic']:\n",
    "    L = torch.tensor(params['L'])\n",
    "    def calc_dr(r1, r2):\n",
    "        dr = torch.remainder((r1 - r2), L)\n",
    "        dr[dr > L/2] = dr[dr > L/2] - L\n",
    "        return dr\n",
    "else:\n",
    "    def calc_dr(r1, r2):\n",
    "        return r1 - r2\n",
    "    \n",
    "def makeGraph(x_data, r_thresh):\n",
    "        Ndata = x_data.size(0)\n",
    "        dx = calc_dr(torch.unsqueeze(x_data, 0), torch.unsqueeze(x_data, 1))\n",
    "        dx = torch.sum(dx**2, dim=2)\n",
    "        edges = torch.argwhere(dx < r_thresh/2)\n",
    "        return dgl.graph((edges[:,0], edges[:,1]), num_nodes=Ndata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e3e01d8a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-01T11:00:55.340445Z",
     "start_time": "2023-03-01T11:00:55.334396Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/uwamichi/jupyter/HiraiwaModel_chem20230227_160803/20230228_024901', '/home/uwamichi/jupyter/HiraiwaModel_chem20230227_160803/20230228_064020', '/home/uwamichi/jupyter/HiraiwaModel_chem20230227_160803/20230228_103154', '/home/uwamichi/jupyter/HiraiwaModel_chem20230227_160803/20230228_142155', '/home/uwamichi/jupyter/HiraiwaModel_chem20230227_160803/20230228_181400', '/home/uwamichi/jupyter/HiraiwaModel_chem20230227_160803/20230228_220458', '/home/uwamichi/jupyter/HiraiwaModel_chem20230227_160803/20230301_015428', '/home/uwamichi/jupyter/HiraiwaModel_chem20230227_160803/20230301_054439', '/home/uwamichi/jupyter/HiraiwaModel_chem20230227_160803/20230301_093612', '/home/uwamichi/jupyter/HiraiwaModel_chem20230227_160803/20230301_132715', '/home/uwamichi/jupyter/HiraiwaModel_chem20230227_160803/ActiveNet_vp_rotsym_multiStep_batchNorm', '/home/uwamichi/jupyter/HiraiwaModel_chem20230227_160803/ActiveNet_vp_rotsym_batchNorm']\n",
      "['/home/uwamichi/jupyter/HiraiwaModel_chem20230227_160803/20230228_024901', '/home/uwamichi/jupyter/HiraiwaModel_chem20230227_160803/20230228_064020', '/home/uwamichi/jupyter/HiraiwaModel_chem20230227_160803/20230228_103154', '/home/uwamichi/jupyter/HiraiwaModel_chem20230227_160803/20230228_142155', '/home/uwamichi/jupyter/HiraiwaModel_chem20230227_160803/20230228_181400', '/home/uwamichi/jupyter/HiraiwaModel_chem20230227_160803/20230228_220458', '/home/uwamichi/jupyter/HiraiwaModel_chem20230227_160803/20230301_015428', '/home/uwamichi/jupyter/HiraiwaModel_chem20230227_160803/20230301_054439', '/home/uwamichi/jupyter/HiraiwaModel_chem20230227_160803/20230301_093612', '/home/uwamichi/jupyter/HiraiwaModel_chem20230227_160803/20230301_132715']\n"
     ]
    }
   ],
   "source": [
    "subdir_list = [f.path for f in os.scandir(dirName) if f.is_dir()]\n",
    "\n",
    "print(subdir_list)\n",
    "\n",
    "datadir_list = [f for f in subdir_list if 'result.npz' in [ff.name for ff in os.scandir(f)]]\n",
    "\n",
    "print(datadir_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3e9e2f01",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-01T11:00:57.491933Z",
     "start_time": "2023-03-01T11:00:55.341986Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "522"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dr_thresh = 4\n",
    "dt = 1\n",
    "batch_size = 8\n",
    "\n",
    "N_data = len(datadir_list)\n",
    "\n",
    "#TR_VA_rate = np.array([0.6, 0.2])\n",
    "\n",
    "TR_last = 5\n",
    "VA_last = 7\n",
    "\n",
    "shuffle_inds = np.arange(N_data, dtype=int)\n",
    "np.random.shuffle(shuffle_inds)\n",
    "\n",
    "train_inds = shuffle_inds[:TR_last]\n",
    "valid_inds = shuffle_inds[TR_last:VA_last]\n",
    "test_inds = shuffle_inds[VA_last:]\n",
    "\n",
    "celltype_lst = []\n",
    "\n",
    "train_x = []\n",
    "valid_x = []\n",
    "test_x = []\n",
    "\n",
    "train_y = []\n",
    "valid_y = []\n",
    "test_y = []\n",
    "\n",
    "train_i_dir = []\n",
    "valid_i_dir = []\n",
    "test_i_dir = []\n",
    "\n",
    "for i_dir, subdirName in enumerate(datadir_list):\n",
    "    \n",
    "    traj = np.load(subdirName+'/result.npz')\n",
    "    \n",
    "    celltype_lst.append(torch.tensor(traj['celltype_label']).view(-1,1))\n",
    "\n",
    "    xy_t = torch.tensor(traj['xy'][:-1,:,:])\n",
    "    v_t = calc_dr(torch.tensor(traj['xy'][1:,:,:]), torch.tensor(traj['xy'][:-1,:,:])) / dt\n",
    "    p_t = torch.unsqueeze(torch.tensor(traj['theta'][:-1,:]), dim=2)\n",
    "    w_t = torch.unsqueeze(torch.tensor((traj['theta'][1:,:]-traj['theta'][:-1,:])%(2*np.pi)/dt), dim=2)\n",
    "    \n",
    "    if i_dir in train_inds:\n",
    "        train_x.append(torch.concat((xy_t, p_t), -1))\n",
    "        train_y.append(torch.concat((v_t, w_t), -1))\n",
    "        train_i_dir.append(torch.ones([xy_t.size(0)])*i_dir)\n",
    "\n",
    "    if i_dir in valid_inds:\n",
    "        valid_x.append(torch.concat((xy_t, p_t), -1))\n",
    "        valid_y.append(torch.concat((v_t, w_t), -1))\n",
    "        valid_i_dir.append(torch.ones([xy_t.size(0)])*i_dir)\n",
    "        \n",
    "    if i_dir in test_inds:\n",
    "        test_x.append(torch.concat((xy_t, p_t), -1))\n",
    "        test_y.append(torch.concat((v_t, w_t), -1))\n",
    "        test_i_dir.append(torch.ones([xy_t.size(0)])*i_dir)\n",
    "    \n",
    "train_dataset = torch.utils.data.TensorDataset(\n",
    "    torch.concat(train_x, 0), \n",
    "    torch.concat(train_y, 0), \n",
    "    torch.concat(train_i_dir, 0))\n",
    "\n",
    "valid_dataset = torch.utils.data.TensorDataset(\n",
    "    torch.concat(valid_x, 0), \n",
    "    torch.concat(valid_y, 0), \n",
    "    torch.concat(valid_i_dir, 0))\n",
    "\n",
    "test_dataset = torch.utils.data.TensorDataset(\n",
    "    torch.concat(test_x, 0), \n",
    "    torch.concat(test_y, 0), \n",
    "    torch.concat(test_i_dir, 0))\n",
    "\n",
    "train_data = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, pin_memory=True)\n",
    "valid_data = torch.utils.data.DataLoader(valid_dataset, batch_size=batch_size, pin_memory=True)\n",
    "test_data = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, pin_memory=True)\n",
    "\n",
    "del train_x, train_y, train_i_dir, train_dataset\n",
    "del valid_x, valid_y, valid_i_dir, valid_dataset\n",
    "del test_x, test_y, test_i_dir, test_dataset\n",
    "gc.collect()\n",
    "\n",
    "#print(data)\n",
    "#print(data.num_graphs)\n",
    "#print(data.x)\n",
    "#print(data.y)\n",
    "#print(data.edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1c0a633b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-01T11:00:57.497507Z",
     "start_time": "2023-03-01T11:00:57.493667Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6 1 4 5 7]\n"
     ]
    }
   ],
   "source": [
    "print(train_inds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4f35452f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-01T11:00:57.508015Z",
     "start_time": "2023-03-01T11:00:57.502082Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_DataLoader__initialized',\n",
       " '_DataLoader__multiprocessing_context',\n",
       " '_IterableDataset_len_called',\n",
       " '__annotations__',\n",
       " '__class__',\n",
       " '__class_getitem__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__iter__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__orig_bases__',\n",
       " '__parameters__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__slots__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_auto_collation',\n",
       " '_dataset_kind',\n",
       " '_get_iterator',\n",
       " '_get_shared_seed',\n",
       " '_index_sampler',\n",
       " '_is_protocol',\n",
       " '_iterator',\n",
       " 'batch_sampler',\n",
       " 'batch_size',\n",
       " 'check_worker_number_rationality',\n",
       " 'collate_fn',\n",
       " 'dataset',\n",
       " 'drop_last',\n",
       " 'generator',\n",
       " 'multiprocessing_context',\n",
       " 'num_workers',\n",
       " 'persistent_workers',\n",
       " 'pin_memory',\n",
       " 'pin_memory_device',\n",
       " 'prefetch_factor',\n",
       " 'sampler',\n",
       " 'timeout',\n",
       " 'worker_init_fn']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f94ee7f4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-01T11:00:57.539132Z",
     "start_time": "2023-03-01T11:00:57.517190Z"
    }
   },
   "outputs": [],
   "source": [
    "def plotGraph(data):\n",
    "\n",
    "    # networkxのグラフに変換\n",
    "    nxg = dgl.to_networkx(data)\n",
    "\n",
    "    # 可視化のためのページランク計算\n",
    "    pr = nx.pagerank(nxg)\n",
    "    pr_max = np.array(list(pr.values())).max()\n",
    "\n",
    "    # 可視化する際のノード位置\n",
    "    draw_pos = nx.spring_layout(nxg, seed=0) \n",
    "\n",
    "    # ノードの色設定\n",
    "    cmap = plt.get_cmap('tab10')\n",
    "    labels = data.y.numpy()\n",
    "    colors = [cmap(l) for l in labels]\n",
    "\n",
    "    # 図のサイズ\n",
    "    fig0 = plt.figure(figsize=(10, 10))\n",
    "\n",
    "    # 描画\n",
    "    nx.draw_networkx_nodes(nxg, \n",
    "                          draw_pos,\n",
    "                          node_size=[v / pr_max * 1000 for v in pr.values()])#,\n",
    "                          #node_color=colors, alpha=0.5)\n",
    "    nx.draw_networkx_edges(nxg, draw_pos, arrowstyle='-', alpha=0.2)\n",
    "    nx.draw_networkx_labels(nxg, draw_pos, font_size=10)\n",
    "\n",
    "    #plt.title('KarateClub')\n",
    "    plt.show()\n",
    "\n",
    "    return fig0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "21e97796",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-01T11:00:57.591101Z",
     "start_time": "2023-03-01T11:00:57.549661Z"
    }
   },
   "outputs": [],
   "source": [
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, Nchannels, dropout=0, batchN=False, flgBias=False):\n",
    "        super(NeuralNet, self).__init__()\n",
    "\n",
    "        if dropout:\n",
    "            self.dropout = nn.Dropout(p=dropout)\n",
    "        else:\n",
    "            self.dropout = 0\n",
    "            \n",
    "        if batchN:\n",
    "            self.bNorm1 = nn.BatchNorm1d(Nchannels)\n",
    "            self.bNorm2 = nn.BatchNorm1d(Nchannels)\n",
    "            self.bNorm3 = nn.BatchNorm1d(Nchannels)\n",
    "            \n",
    "        self.batchN=batchN\n",
    "        \n",
    "        self.layer1 = nn.Linear(in_channels, Nchannels, bias=flgBias)\n",
    "        self.layer2 = nn.Linear(Nchannels, Nchannels, bias=flgBias)\n",
    "        self.layer3 = nn.Linear(Nchannels, Nchannels, bias=flgBias)\n",
    "        self.layer4 = nn.Linear(Nchannels, out_channels, bias=flgBias)\n",
    "\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.layer1.reset_parameters()\n",
    "        self.layer2.reset_parameters()\n",
    "        self.layer3.reset_parameters()\n",
    "        self.layer4.reset_parameters()\n",
    "        #nn.init.zeros_(self.layer1.weight)\n",
    "        #nn.init.zeros_(self.layer2.weight)\n",
    "        #nn.init.zeros_(self.layer3.weight)\n",
    "        #nn.init.zeros_(self.layer4.weight)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.activation(self.layer1(x))\n",
    "        if self.batchN:\n",
    "            out = self.bNorm1(out)\n",
    "        if self.dropout:\n",
    "            out = self.dropout(out)\n",
    "        \n",
    "        out = self.activation(self.layer2(out))\n",
    "        if self.batchN:\n",
    "            out = self.bNorm2(out)\n",
    "        if self.dropout:\n",
    "            out = self.dropout(out)\n",
    "        \n",
    "        out = self.activation(self.layer3(out))\n",
    "        if self.batchN:\n",
    "            out = self.bNorm3(out)\n",
    "        if self.dropout:\n",
    "            out = self.dropout(out)\n",
    "        \n",
    "        out = self.layer4(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "class ActiveNet(nn.Module):\n",
    "    def __init__(self, xy_dim, r, dropout=0, batchN=False, bias=False, Nchannels=128):\n",
    "        super().__init__()\n",
    "\n",
    "        self.interactNN = NeuralNet(xy_dim*2 + 2, xy_dim, Nchannels, dropout, batchN, bias)\n",
    "\n",
    "        self.thetaDotNN = NeuralNet(xy_dim*2 + 2, 1, Nchannels, dropout, batchN, bias)\n",
    "        \n",
    "        self.selfpropel = nn.Parameter(torch.tensor(0.0, requires_grad=True, device=device))\n",
    "\n",
    "        #self.Normalizer = nn.Softmax(dim=1)\n",
    "\n",
    "        self.xy_dim = xy_dim\n",
    "        \n",
    "        self.r = r\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.interactNN.reset_parameters()\n",
    "\n",
    "        self.thetaDotNN.reset_parameters()\n",
    "        \n",
    "        nn.init.uniform_(self.selfpropel)\n",
    "\n",
    "        #self.bias.data.zero_()\n",
    "        \n",
    "    def load_celltypes(self, celltype):\n",
    "        self.celltype = celltype\n",
    "\n",
    "    def calc_message(self, edges):\n",
    "        dx = calc_dr(edges.dst['x'], edges.src['x'])\n",
    "\n",
    "        costheta = torch.cos(edges.dst['theta'])\n",
    "        sintheta = torch.sin(edges.dst['theta'])\n",
    "\n",
    "        dx_para = costheta * dx[..., :1] + sintheta * dx[..., 1:]\n",
    "        dx_perp = costheta * dx[..., 1:] - sintheta * dx[..., :1]\n",
    "\n",
    "        p_para_src = torch.cos(edges.src['theta'] - edges.dst['theta'])\n",
    "        p_perp_src = torch.sin(edges.src['theta'] - edges.dst['theta'])\n",
    "\n",
    "        rot_m_v = self.interactNN(torch.concat((dx_para, dx_perp, \n",
    "                                                p_para_src, p_perp_src,\n",
    "                                                edges.dst['type'], edges.src['type']), -1))\n",
    "\n",
    "        m_v = torch.concat((costheta * rot_m_v[..., :1] - sintheta * rot_m_v[..., 1:],\n",
    "                            costheta * rot_m_v[..., 1:] + sintheta * rot_m_v[..., :1]), -1)\n",
    "\n",
    "        m_theta = self.thetaDotNN(torch.concat((dx_para, dx_perp, \n",
    "                                                p_para_src, p_perp_src, \n",
    "                                                edges.dst['type'], edges.src['type']), -1))\n",
    "        \n",
    "        return {'m': torch.concat((m_v, m_theta), -1)}\n",
    "        \n",
    "    def forward(self, xv):\n",
    "        r_g = makeGraph(xv[..., :self.xy_dim], self.r/2)\n",
    "        r_g.ndata['x'] = xv[..., :self.xy_dim]\n",
    "        r_g.ndata['theta'] = xv[..., self.xy_dim:(self.xy_dim+1)]\n",
    "        r_g.ndata['type'] = self.celltype\n",
    "        r_g.update_all(self.calc_message, fn.sum('m', 'a'))\n",
    "        r_g.ndata['a'][..., :self.xy_dim] = r_g.ndata['a'][..., :self.xy_dim] + self.selfpropel * torch.concat((torch.cos(r_g.ndata['theta']), torch.sin(r_g.ndata['theta'])), -1)\n",
    "        \n",
    "        return r_g.ndata['a']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b52c1e60",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-01T11:00:57.609732Z",
     "start_time": "2023-03-01T11:00:57.597014Z"
    }
   },
   "outputs": [],
   "source": [
    "def myLoss(out, target):\n",
    "    dv = torch.sum(torch.square(out[..., :xy_dim] - target[..., :xy_dim]), dim=-1)\n",
    "    dcos = torch.cos(out[..., xy_dim] - target[..., xy_dim])\n",
    "    return torch.mean(dv), 1 - torch.mean(dcos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6033283c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-01T14:15:54.755596Z",
     "start_time": "2023-03-01T11:00:57.621818Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 | train Loss: [0.0562, 0.0103] | valid Loss: [0.0696, 0.0114]\n",
      "Epoch 1 | train Loss: [0.0525, 0.0100] | valid Loss: [0.0672, 0.0114]\n",
      "Epoch 2 | train Loss: [0.0469, 0.0099] | valid Loss: [0.0585, 0.0108]\n",
      "Epoch 3 | train Loss: [0.0436, 0.0096] | valid Loss: [0.0546, 0.0105]\n",
      "Epoch 4 | train Loss: [0.0339, 0.0094] | valid Loss: [0.0412, 0.0103]\n",
      "Epoch 5 | train Loss: [0.0361, 0.0093] | valid Loss: [0.0458, 0.0101]\n",
      "Epoch 6 | train Loss: [0.0322, 0.0091] | valid Loss: [0.0364, 0.0099]\n",
      "Epoch 7 | train Loss: [0.0312, 0.0090] | valid Loss: [0.0351, 0.0099]\n",
      "Epoch 8 | train Loss: [0.0332, 0.0080] | valid Loss: [0.0366, 0.0087]\n",
      "Epoch 9 | train Loss: [0.0291, 0.0078] | valid Loss: [0.0347, 0.0087]\n",
      "Epoch 10 | train Loss: [0.0298, 0.0075] | valid Loss: [0.0329, 0.0083]\n",
      "Epoch 11 | train Loss: [0.0296, 0.0075] | valid Loss: [0.0330, 0.0082]\n",
      "Epoch 12 | train Loss: [0.0255, 0.0074] | valid Loss: [0.0273, 0.0082]\n",
      "Epoch 13 | train Loss: [0.0293, 0.0076] | valid Loss: [0.0348, 0.0082]\n",
      "Epoch 14 | train Loss: [0.0283, 0.0074] | valid Loss: [0.0330, 0.0081]\n",
      "Epoch 15 | train Loss: [0.0280, 0.0074] | valid Loss: [0.0323, 0.0080]\n",
      "Epoch 16 | train Loss: [0.0284, 0.0072] | valid Loss: [0.0307, 0.0079]\n",
      "Epoch 17 | train Loss: [0.0262, 0.0072] | valid Loss: [0.0300, 0.0078]\n",
      "Epoch 00019: reducing learning rate of group 0 to 5.0000e-01.\n",
      "Epoch 18 | train Loss: [0.0262, 0.0072] | valid Loss: [0.0294, 0.0078]\n",
      "Epoch 19 | train Loss: [0.0202, 0.0070] | valid Loss: [0.0215, 0.0078]\n",
      "Epoch 20 | train Loss: [0.0195, 0.0068] | valid Loss: [0.0208, 0.0078]\n",
      "Epoch 21 | train Loss: [0.0206, 0.0070] | valid Loss: [0.0228, 0.0079]\n",
      "Epoch 22 | train Loss: [0.0190, 0.0069] | valid Loss: [0.0203, 0.0078]\n",
      "Epoch 23 | train Loss: [0.0188, 0.0069] | valid Loss: [0.0201, 0.0077]\n",
      "Epoch 24 | train Loss: [0.0188, 0.0069] | valid Loss: [0.0203, 0.0077]\n",
      "Epoch 25 | train Loss: [0.0176, 0.0069] | valid Loss: [0.0185, 0.0077]\n",
      "Epoch 26 | train Loss: [0.0185, 0.0069] | valid Loss: [0.0201, 0.0077]\n",
      "Epoch 27 | train Loss: [0.0173, 0.0069] | valid Loss: [0.0181, 0.0076]\n",
      "Epoch 28 | train Loss: [0.0180, 0.0067] | valid Loss: [0.0193, 0.0075]\n",
      "Epoch 29 | train Loss: [0.0179, 0.0068] | valid Loss: [0.0193, 0.0076]\n",
      "Epoch 30 | train Loss: [0.0177, 0.0068] | valid Loss: [0.0191, 0.0076]\n",
      "Epoch 31 | train Loss: [0.0178, 0.0068] | valid Loss: [0.0193, 0.0076]\n",
      "Epoch 32 | train Loss: [0.0174, 0.0069] | valid Loss: [0.0188, 0.0076]\n",
      "Epoch 00034: reducing learning rate of group 0 to 2.5000e-01.\n",
      "Epoch 33 | train Loss: [0.0172, 0.0068] | valid Loss: [0.0186, 0.0075]\n",
      "Epoch 34 | train Loss: [0.0137, 0.0057] | valid Loss: [0.0150, 0.0064]\n",
      "Epoch 35 | train Loss: [0.0134, 0.0057] | valid Loss: [0.0147, 0.0064]\n",
      "Epoch 36 | train Loss: [0.0129, 0.0057] | valid Loss: [0.0141, 0.0063]\n",
      "Epoch 37 | train Loss: [0.0133, 0.0057] | valid Loss: [0.0147, 0.0063]\n",
      "Epoch 38 | train Loss: [0.0131, 0.0057] | valid Loss: [0.0145, 0.0063]\n",
      "Epoch 39 | train Loss: [0.0130, 0.0057] | valid Loss: [0.0145, 0.0063]\n",
      "Epoch 40 | train Loss: [0.0122, 0.0057] | valid Loss: [0.0136, 0.0063]\n",
      "Epoch 41 | train Loss: [0.0126, 0.0057] | valid Loss: [0.0140, 0.0063]\n",
      "Epoch 42 | train Loss: [0.0128, 0.0057] | valid Loss: [0.0143, 0.0063]\n",
      "Epoch 43 | train Loss: [0.0120, 0.0057] | valid Loss: [0.0134, 0.0063]\n",
      "Epoch 44 | train Loss: [0.0125, 0.0057] | valid Loss: [0.0140, 0.0063]\n",
      "Epoch 45 | train Loss: [0.0126, 0.0057] | valid Loss: [0.0142, 0.0063]\n",
      "Epoch 46 | train Loss: [0.0124, 0.0057] | valid Loss: [0.0139, 0.0063]\n",
      "Epoch 47 | train Loss: [0.0118, 0.0057] | valid Loss: [0.0132, 0.0063]\n",
      "Epoch 48 | train Loss: [0.0118, 0.0057] | valid Loss: [0.0132, 0.0063]\n",
      "Epoch 49 | train Loss: [0.0118, 0.0057] | valid Loss: [0.0132, 0.0063]\n",
      "Epoch 50 | train Loss: [0.0118, 0.0057] | valid Loss: [0.0132, 0.0063]\n",
      "Epoch 51 | train Loss: [0.0118, 0.0056] | valid Loss: [0.0132, 0.0062]\n",
      "Epoch 52 | train Loss: [0.0117, 0.0056] | valid Loss: [0.0131, 0.0063]\n",
      "Epoch 53 | train Loss: [0.0117, 0.0056] | valid Loss: [0.0131, 0.0062]\n",
      "Epoch 54 | train Loss: [0.0117, 0.0056] | valid Loss: [0.0130, 0.0062]\n",
      "Epoch 55 | train Loss: [0.0122, 0.0056] | valid Loss: [0.0138, 0.0062]\n",
      "Epoch 56 | train Loss: [0.0116, 0.0056] | valid Loss: [0.0130, 0.0062]\n",
      "Epoch 57 | train Loss: [0.0115, 0.0056] | valid Loss: [0.0130, 0.0062]\n",
      "Epoch 58 | train Loss: [0.0115, 0.0056] | valid Loss: [0.0129, 0.0062]\n",
      "Epoch 59 | train Loss: [0.0115, 0.0056] | valid Loss: [0.0129, 0.0062]\n",
      "Epoch 60 | train Loss: [0.0115, 0.0056] | valid Loss: [0.0129, 0.0062]\n",
      "Epoch 61 | train Loss: [0.0113, 0.0056] | valid Loss: [0.0127, 0.0062]\n",
      "Epoch 62 | train Loss: [0.0120, 0.0056] | valid Loss: [0.0135, 0.0062]\n",
      "Epoch 63 | train Loss: [0.0114, 0.0056] | valid Loss: [0.0128, 0.0062]\n",
      "Epoch 64 | train Loss: [0.0112, 0.0056] | valid Loss: [0.0126, 0.0062]\n",
      "Epoch 65 | train Loss: [0.0113, 0.0056] | valid Loss: [0.0127, 0.0062]\n",
      "Epoch 66 | train Loss: [0.0113, 0.0056] | valid Loss: [0.0127, 0.0062]\n",
      "Epoch 67 | train Loss: [0.0113, 0.0056] | valid Loss: [0.0127, 0.0062]\n",
      "Epoch 68 | train Loss: [0.0112, 0.0056] | valid Loss: [0.0126, 0.0062]\n",
      "Epoch 69 | train Loss: [0.0113, 0.0056] | valid Loss: [0.0127, 0.0062]\n",
      "Epoch 00071: reducing learning rate of group 0 to 1.2500e-01.\n",
      "Epoch 70 | train Loss: [0.0112, 0.0056] | valid Loss: [0.0126, 0.0062]\n",
      "Epoch 71 | train Loss: [0.0100, 0.0052] | valid Loss: [0.0115, 0.0057]\n",
      "Epoch 72 | train Loss: [0.0102, 0.0052] | valid Loss: [0.0117, 0.0057]\n",
      "Epoch 73 | train Loss: [0.0102, 0.0052] | valid Loss: [0.0117, 0.0057]\n",
      "Epoch 74 | train Loss: [0.0101, 0.0052] | valid Loss: [0.0116, 0.0057]\n",
      "Epoch 75 | train Loss: [0.0097, 0.0052] | valid Loss: [0.0112, 0.0057]\n",
      "Epoch 76 | train Loss: [0.0100, 0.0052] | valid Loss: [0.0115, 0.0057]\n",
      "Epoch 77 | train Loss: [0.0097, 0.0052] | valid Loss: [0.0111, 0.0057]\n",
      "Epoch 78 | train Loss: [0.0096, 0.0052] | valid Loss: [0.0111, 0.0057]\n",
      "Epoch 79 | train Loss: [0.0096, 0.0051] | valid Loss: [0.0111, 0.0057]\n",
      "Epoch 80 | train Loss: [0.0096, 0.0051] | valid Loss: [0.0110, 0.0057]\n",
      "Epoch 81 | train Loss: [0.0100, 0.0051] | valid Loss: [0.0114, 0.0057]\n",
      "Epoch 82 | train Loss: [0.0099, 0.0051] | valid Loss: [0.0114, 0.0057]\n",
      "Epoch 83 | train Loss: [0.0096, 0.0051] | valid Loss: [0.0111, 0.0057]\n",
      "Epoch 84 | train Loss: [0.0099, 0.0051] | valid Loss: [0.0114, 0.0057]\n",
      "Epoch 85 | train Loss: [0.0099, 0.0051] | valid Loss: [0.0114, 0.0057]\n",
      "Epoch 00087: reducing learning rate of group 0 to 6.2500e-02.\n",
      "Epoch 86 | train Loss: [0.0099, 0.0051] | valid Loss: [0.0113, 0.0057]\n",
      "Epoch 87 | train Loss: [0.0095, 0.0051] | valid Loss: [0.0109, 0.0056]\n",
      "Epoch 88 | train Loss: [0.0095, 0.0051] | valid Loss: [0.0109, 0.0056]\n",
      "Epoch 89 | train Loss: [0.0095, 0.0051] | valid Loss: [0.0109, 0.0056]\n",
      "Epoch 90 | train Loss: [0.0095, 0.0051] | valid Loss: [0.0109, 0.0056]\n",
      "Epoch 91 | train Loss: [0.0095, 0.0051] | valid Loss: [0.0109, 0.0056]\n",
      "Epoch 92 | train Loss: [0.0095, 0.0051] | valid Loss: [0.0109, 0.0056]\n",
      "Epoch 93 | train Loss: [0.0094, 0.0051] | valid Loss: [0.0109, 0.0056]\n",
      "Epoch 94 | train Loss: [0.0094, 0.0051] | valid Loss: [0.0109, 0.0056]\n",
      "Epoch 95 | train Loss: [0.0094, 0.0051] | valid Loss: [0.0109, 0.0056]\n",
      "Epoch 96 | train Loss: [0.0094, 0.0051] | valid Loss: [0.0109, 0.0056]\n",
      "Epoch 97 | train Loss: [0.0094, 0.0051] | valid Loss: [0.0108, 0.0056]\n",
      "Epoch 98 | train Loss: [0.0094, 0.0051] | valid Loss: [0.0108, 0.0056]\n",
      "Epoch 99 | train Loss: [0.0094, 0.0051] | valid Loss: [0.0108, 0.0056]\n",
      "Epoch 100 | train Loss: [0.0094, 0.0051] | valid Loss: [0.0108, 0.0056]\n",
      "Epoch 101 | train Loss: [0.0094, 0.0051] | valid Loss: [0.0108, 0.0056]\n",
      "Epoch 102 | train Loss: [0.0094, 0.0051] | valid Loss: [0.0108, 0.0056]\n",
      "Epoch 103 | train Loss: [0.0094, 0.0051] | valid Loss: [0.0108, 0.0056]\n",
      "Epoch 104 | train Loss: [0.0094, 0.0051] | valid Loss: [0.0108, 0.0056]\n",
      "Epoch 105 | train Loss: [0.0094, 0.0051] | valid Loss: [0.0108, 0.0056]\n",
      "Epoch 106 | train Loss: [0.0094, 0.0051] | valid Loss: [0.0108, 0.0056]\n",
      "Epoch 107 | train Loss: [0.0094, 0.0051] | valid Loss: [0.0108, 0.0056]\n",
      "Epoch 108 | train Loss: [0.0094, 0.0051] | valid Loss: [0.0108, 0.0056]\n",
      "Epoch 109 | train Loss: [0.0094, 0.0051] | valid Loss: [0.0108, 0.0056]\n",
      "Epoch 110 | train Loss: [0.0094, 0.0051] | valid Loss: [0.0108, 0.0056]\n",
      "Epoch 111 | train Loss: [0.0093, 0.0051] | valid Loss: [0.0108, 0.0056]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 112 | train Loss: [0.0094, 0.0051] | valid Loss: [0.0108, 0.0056]\n",
      "Epoch 113 | train Loss: [0.0094, 0.0051] | valid Loss: [0.0108, 0.0056]\n",
      "Epoch 114 | train Loss: [0.0093, 0.0051] | valid Loss: [0.0107, 0.0056]\n",
      "Epoch 115 | train Loss: [0.0093, 0.0051] | valid Loss: [0.0108, 0.0056]\n",
      "Epoch 116 | train Loss: [0.0093, 0.0051] | valid Loss: [0.0107, 0.0056]\n",
      "Epoch 117 | train Loss: [0.0094, 0.0051] | valid Loss: [0.0108, 0.0056]\n",
      "Epoch 118 | train Loss: [0.0093, 0.0051] | valid Loss: [0.0107, 0.0056]\n",
      "Epoch 119 | train Loss: [0.0093, 0.0051] | valid Loss: [0.0107, 0.0056]\n",
      "Epoch 00121: reducing learning rate of group 0 to 3.1250e-02.\n",
      "Epoch 120 | train Loss: [0.0093, 0.0051] | valid Loss: [0.0107, 0.0056]\n",
      "Epoch 121 | train Loss: [0.0092, 0.0050] | valid Loss: [0.0106, 0.0056]\n",
      "Epoch 122 | train Loss: [0.0092, 0.0050] | valid Loss: [0.0106, 0.0056]\n",
      "Epoch 123 | train Loss: [0.0092, 0.0050] | valid Loss: [0.0106, 0.0056]\n",
      "Epoch 124 | train Loss: [0.0092, 0.0050] | valid Loss: [0.0106, 0.0056]\n",
      "Epoch 125 | train Loss: [0.0092, 0.0050] | valid Loss: [0.0106, 0.0056]\n",
      "Epoch 126 | train Loss: [0.0092, 0.0050] | valid Loss: [0.0106, 0.0056]\n",
      "Epoch 127 | train Loss: [0.0092, 0.0050] | valid Loss: [0.0106, 0.0056]\n",
      "Epoch 128 | train Loss: [0.0092, 0.0050] | valid Loss: [0.0106, 0.0056]\n",
      "Epoch 129 | train Loss: [0.0092, 0.0050] | valid Loss: [0.0106, 0.0056]\n",
      "Epoch 130 | train Loss: [0.0092, 0.0050] | valid Loss: [0.0106, 0.0056]\n",
      "Epoch 131 | train Loss: [0.0092, 0.0050] | valid Loss: [0.0106, 0.0056]\n",
      "Epoch 132 | train Loss: [0.0092, 0.0050] | valid Loss: [0.0106, 0.0056]\n",
      "Epoch 133 | train Loss: [0.0092, 0.0050] | valid Loss: [0.0106, 0.0056]\n",
      "Epoch 134 | train Loss: [0.0092, 0.0050] | valid Loss: [0.0106, 0.0056]\n",
      "Epoch 135 | train Loss: [0.0092, 0.0050] | valid Loss: [0.0106, 0.0056]\n",
      "Epoch 136 | train Loss: [0.0092, 0.0050] | valid Loss: [0.0106, 0.0056]\n",
      "Epoch 137 | train Loss: [0.0092, 0.0050] | valid Loss: [0.0106, 0.0056]\n",
      "Epoch 138 | train Loss: [0.0092, 0.0050] | valid Loss: [0.0106, 0.0056]\n",
      "Epoch 139 | train Loss: [0.0092, 0.0050] | valid Loss: [0.0106, 0.0056]\n",
      "Epoch 140 | train Loss: [0.0092, 0.0050] | valid Loss: [0.0106, 0.0056]\n",
      "Epoch 141 | train Loss: [0.0092, 0.0050] | valid Loss: [0.0106, 0.0056]\n",
      "Epoch 142 | train Loss: [0.0092, 0.0050] | valid Loss: [0.0105, 0.0056]\n",
      "Epoch 143 | train Loss: [0.0092, 0.0050] | valid Loss: [0.0105, 0.0056]\n",
      "Epoch 144 | train Loss: [0.0092, 0.0050] | valid Loss: [0.0105, 0.0056]\n",
      "Epoch 145 | train Loss: [0.0092, 0.0050] | valid Loss: [0.0105, 0.0055]\n",
      "Epoch 146 | train Loss: [0.0092, 0.0050] | valid Loss: [0.0105, 0.0055]\n",
      "Epoch 147 | train Loss: [0.0092, 0.0050] | valid Loss: [0.0105, 0.0055]\n",
      "Epoch 148 | train Loss: [0.0092, 0.0050] | valid Loss: [0.0105, 0.0055]\n",
      "Epoch 149 | train Loss: [0.0092, 0.0050] | valid Loss: [0.0105, 0.0055]\n",
      "Epoch 150 | train Loss: [0.0091, 0.0050] | valid Loss: [0.0105, 0.0055]\n",
      "Epoch 151 | train Loss: [0.0091, 0.0050] | valid Loss: [0.0105, 0.0055]\n",
      "Epoch 152 | train Loss: [0.0091, 0.0050] | valid Loss: [0.0105, 0.0055]\n",
      "Epoch 153 | train Loss: [0.0091, 0.0050] | valid Loss: [0.0105, 0.0055]\n",
      "Epoch 154 | train Loss: [0.0091, 0.0050] | valid Loss: [0.0105, 0.0055]\n",
      "Epoch 155 | train Loss: [0.0091, 0.0050] | valid Loss: [0.0105, 0.0055]\n",
      "Epoch 156 | train Loss: [0.0091, 0.0050] | valid Loss: [0.0105, 0.0055]\n",
      "Epoch 157 | train Loss: [0.0091, 0.0050] | valid Loss: [0.0105, 0.0055]\n",
      "Epoch 158 | train Loss: [0.0091, 0.0050] | valid Loss: [0.0105, 0.0055]\n",
      "Epoch 159 | train Loss: [0.0091, 0.0050] | valid Loss: [0.0105, 0.0055]\n",
      "Epoch 160 | train Loss: [0.0091, 0.0050] | valid Loss: [0.0105, 0.0055]\n",
      "Epoch 161 | train Loss: [0.0091, 0.0050] | valid Loss: [0.0105, 0.0055]\n",
      "Epoch 162 | train Loss: [0.0091, 0.0050] | valid Loss: [0.0105, 0.0055]\n",
      "Epoch 163 | train Loss: [0.0091, 0.0050] | valid Loss: [0.0105, 0.0055]\n",
      "Epoch 164 | train Loss: [0.0091, 0.0050] | valid Loss: [0.0105, 0.0055]\n",
      "Epoch 165 | train Loss: [0.0091, 0.0050] | valid Loss: [0.0105, 0.0055]\n",
      "Epoch 166 | train Loss: [0.0091, 0.0050] | valid Loss: [0.0105, 0.0055]\n",
      "Epoch 167 | train Loss: [0.0091, 0.0050] | valid Loss: [0.0105, 0.0055]\n",
      "Epoch 168 | train Loss: [0.0091, 0.0050] | valid Loss: [0.0105, 0.0055]\n",
      "Epoch 169 | train Loss: [0.0091, 0.0050] | valid Loss: [0.0105, 0.0055]\n",
      "Epoch 170 | train Loss: [0.0091, 0.0050] | valid Loss: [0.0105, 0.0055]\n",
      "Epoch 171 | train Loss: [0.0091, 0.0050] | valid Loss: [0.0105, 0.0055]\n",
      "Epoch 172 | train Loss: [0.0091, 0.0050] | valid Loss: [0.0105, 0.0055]\n",
      "Epoch 173 | train Loss: [0.0091, 0.0050] | valid Loss: [0.0105, 0.0055]\n",
      "Epoch 174 | train Loss: [0.0091, 0.0050] | valid Loss: [0.0105, 0.0055]\n",
      "Epoch 175 | train Loss: [0.0091, 0.0050] | valid Loss: [0.0105, 0.0055]\n",
      "Epoch 176 | train Loss: [0.0091, 0.0050] | valid Loss: [0.0105, 0.0055]\n",
      "Epoch 177 | train Loss: [0.0091, 0.0050] | valid Loss: [0.0105, 0.0055]\n",
      "Epoch 178 | train Loss: [0.0091, 0.0050] | valid Loss: [0.0105, 0.0055]\n",
      "Epoch 179 | train Loss: [0.0091, 0.0050] | valid Loss: [0.0105, 0.0055]\n",
      "Epoch 180 | train Loss: [0.0091, 0.0050] | valid Loss: [0.0105, 0.0055]\n",
      "Epoch 181 | train Loss: [0.0091, 0.0050] | valid Loss: [0.0105, 0.0055]\n",
      "Epoch 182 | train Loss: [0.0091, 0.0050] | valid Loss: [0.0105, 0.0055]\n",
      "Epoch 183 | train Loss: [0.0091, 0.0050] | valid Loss: [0.0105, 0.0055]\n",
      "Epoch 184 | train Loss: [0.0091, 0.0050] | valid Loss: [0.0105, 0.0055]\n",
      "Epoch 185 | train Loss: [0.0091, 0.0050] | valid Loss: [0.0105, 0.0055]\n",
      "Epoch 186 | train Loss: [0.0091, 0.0050] | valid Loss: [0.0105, 0.0055]\n",
      "Epoch 187 | train Loss: [0.0091, 0.0050] | valid Loss: [0.0104, 0.0055]\n",
      "Epoch 188 | train Loss: [0.0091, 0.0050] | valid Loss: [0.0104, 0.0055]\n",
      "Epoch 189 | train Loss: [0.0091, 0.0050] | valid Loss: [0.0104, 0.0055]\n",
      "Epoch 190 | train Loss: [0.0091, 0.0050] | valid Loss: [0.0104, 0.0055]\n",
      "Epoch 191 | train Loss: [0.0091, 0.0050] | valid Loss: [0.0104, 0.0055]\n",
      "Epoch 192 | train Loss: [0.0091, 0.0050] | valid Loss: [0.0104, 0.0055]\n",
      "Epoch 193 | train Loss: [0.0091, 0.0050] | valid Loss: [0.0104, 0.0055]\n",
      "Epoch 194 | train Loss: [0.0091, 0.0050] | valid Loss: [0.0104, 0.0055]\n",
      "Epoch 195 | train Loss: [0.0091, 0.0050] | valid Loss: [0.0104, 0.0055]\n",
      "Epoch 196 | train Loss: [0.0091, 0.0050] | valid Loss: [0.0104, 0.0055]\n",
      "Epoch 197 | train Loss: [0.0090, 0.0050] | valid Loss: [0.0104, 0.0055]\n",
      "Epoch 198 | train Loss: [0.0090, 0.0050] | valid Loss: [0.0104, 0.0055]\n",
      "Epoch 199 | train Loss: [0.0090, 0.0050] | valid Loss: [0.0104, 0.0055]\n",
      "Epoch 200 | train Loss: [0.0090, 0.0050] | valid Loss: [0.0104, 0.0055]\n",
      "Epoch 201 | train Loss: [0.0090, 0.0050] | valid Loss: [0.0104, 0.0055]\n",
      "Epoch 202 | train Loss: [0.0090, 0.0050] | valid Loss: [0.0104, 0.0055]\n",
      "Epoch 203 | train Loss: [0.0090, 0.0050] | valid Loss: [0.0104, 0.0055]\n",
      "Epoch 204 | train Loss: [0.0090, 0.0050] | valid Loss: [0.0104, 0.0055]\n",
      "Epoch 205 | train Loss: [0.0090, 0.0050] | valid Loss: [0.0104, 0.0055]\n",
      "Epoch 206 | train Loss: [0.0090, 0.0050] | valid Loss: [0.0104, 0.0055]\n",
      "Epoch 207 | train Loss: [0.0090, 0.0050] | valid Loss: [0.0104, 0.0055]\n",
      "Epoch 208 | train Loss: [0.0090, 0.0050] | valid Loss: [0.0104, 0.0055]\n",
      "Epoch 209 | train Loss: [0.0090, 0.0050] | valid Loss: [0.0104, 0.0055]\n",
      "Epoch 210 | train Loss: [0.0090, 0.0050] | valid Loss: [0.0104, 0.0055]\n",
      "Epoch 211 | train Loss: [0.0090, 0.0050] | valid Loss: [0.0104, 0.0055]\n",
      "Epoch 212 | train Loss: [0.0090, 0.0050] | valid Loss: [0.0104, 0.0055]\n",
      "Epoch 213 | train Loss: [0.0090, 0.0050] | valid Loss: [0.0104, 0.0055]\n",
      "Epoch 214 | train Loss: [0.0090, 0.0050] | valid Loss: [0.0104, 0.0055]\n",
      "Epoch 215 | train Loss: [0.0090, 0.0050] | valid Loss: [0.0104, 0.0055]\n",
      "Epoch 216 | train Loss: [0.0090, 0.0050] | valid Loss: [0.0104, 0.0055]\n",
      "Epoch 217 | train Loss: [0.0090, 0.0050] | valid Loss: [0.0104, 0.0055]\n",
      "Epoch 218 | train Loss: [0.0090, 0.0050] | valid Loss: [0.0104, 0.0055]\n",
      "Epoch 219 | train Loss: [0.0090, 0.0050] | valid Loss: [0.0104, 0.0055]\n",
      "Epoch 220 | train Loss: [0.0090, 0.0050] | valid Loss: [0.0104, 0.0055]\n",
      "Epoch 221 | train Loss: [0.0090, 0.0050] | valid Loss: [0.0104, 0.0055]\n",
      "Epoch 222 | train Loss: [0.0090, 0.0050] | valid Loss: [0.0104, 0.0055]\n",
      "Epoch 223 | train Loss: [0.0090, 0.0050] | valid Loss: [0.0104, 0.0055]\n",
      "Epoch 224 | train Loss: [0.0090, 0.0050] | valid Loss: [0.0104, 0.0055]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 225 | train Loss: [0.0090, 0.0050] | valid Loss: [0.0104, 0.0055]\n",
      "Epoch 226 | train Loss: [0.0090, 0.0050] | valid Loss: [0.0104, 0.0055]\n",
      "Epoch 227 | train Loss: [0.0090, 0.0050] | valid Loss: [0.0104, 0.0055]\n",
      "Epoch 228 | train Loss: [0.0090, 0.0050] | valid Loss: [0.0104, 0.0055]\n",
      "Epoch 229 | train Loss: [0.0090, 0.0050] | valid Loss: [0.0104, 0.0055]\n",
      "Epoch 230 | train Loss: [0.0090, 0.0050] | valid Loss: [0.0104, 0.0055]\n",
      "Epoch 231 | train Loss: [0.0090, 0.0050] | valid Loss: [0.0104, 0.0055]\n",
      "Epoch 232 | train Loss: [0.0090, 0.0050] | valid Loss: [0.0104, 0.0055]\n",
      "Epoch 233 | train Loss: [0.0090, 0.0050] | valid Loss: [0.0104, 0.0055]\n",
      "Epoch 234 | train Loss: [0.0090, 0.0050] | valid Loss: [0.0104, 0.0055]\n",
      "Epoch 235 | train Loss: [0.0090, 0.0050] | valid Loss: [0.0104, 0.0055]\n",
      "Epoch 236 | train Loss: [0.0090, 0.0050] | valid Loss: [0.0104, 0.0055]\n",
      "Epoch 237 | train Loss: [0.0090, 0.0050] | valid Loss: [0.0104, 0.0055]\n",
      "Epoch 238 | train Loss: [0.0090, 0.0050] | valid Loss: [0.0104, 0.0055]\n",
      "Epoch 239 | train Loss: [0.0090, 0.0050] | valid Loss: [0.0104, 0.0055]\n",
      "Epoch 240 | train Loss: [0.0090, 0.0050] | valid Loss: [0.0104, 0.0055]\n",
      "Epoch 241 | train Loss: [0.0090, 0.0050] | valid Loss: [0.0104, 0.0055]\n",
      "Epoch 242 | train Loss: [0.0090, 0.0050] | valid Loss: [0.0104, 0.0055]\n",
      "Epoch 243 | train Loss: [0.0090, 0.0050] | valid Loss: [0.0104, 0.0055]\n",
      "Epoch 244 | train Loss: [0.0090, 0.0050] | valid Loss: [0.0104, 0.0055]\n",
      "Epoch 245 | train Loss: [0.0090, 0.0050] | valid Loss: [0.0104, 0.0055]\n",
      "Epoch 246 | train Loss: [0.0090, 0.0050] | valid Loss: [0.0104, 0.0055]\n",
      "Epoch 247 | train Loss: [0.0090, 0.0050] | valid Loss: [0.0104, 0.0055]\n",
      "Epoch 248 | train Loss: [0.0090, 0.0050] | valid Loss: [0.0104, 0.0055]\n",
      "Epoch 249 | train Loss: [0.0090, 0.0050] | valid Loss: [0.0104, 0.0055]\n",
      "Epoch 250 | train Loss: [0.0090, 0.0050] | valid Loss: [0.0103, 0.0055]\n",
      "Epoch 251 | train Loss: [0.0090, 0.0050] | valid Loss: [0.0103, 0.0055]\n",
      "Epoch 252 | train Loss: [0.0090, 0.0050] | valid Loss: [0.0103, 0.0055]\n",
      "Epoch 253 | train Loss: [0.0090, 0.0050] | valid Loss: [0.0103, 0.0055]\n",
      "Epoch 254 | train Loss: [0.0090, 0.0050] | valid Loss: [0.0103, 0.0055]\n",
      "Epoch 255 | train Loss: [0.0090, 0.0050] | valid Loss: [0.0103, 0.0055]\n",
      "Epoch 256 | train Loss: [0.0089, 0.0050] | valid Loss: [0.0103, 0.0055]\n",
      "Epoch 257 | train Loss: [0.0089, 0.0050] | valid Loss: [0.0103, 0.0055]\n",
      "Epoch 258 | train Loss: [0.0089, 0.0050] | valid Loss: [0.0103, 0.0055]\n",
      "Epoch 259 | train Loss: [0.0089, 0.0050] | valid Loss: [0.0103, 0.0055]\n",
      "Epoch 260 | train Loss: [0.0089, 0.0050] | valid Loss: [0.0103, 0.0055]\n",
      "Epoch 261 | train Loss: [0.0089, 0.0050] | valid Loss: [0.0103, 0.0055]\n",
      "Epoch 262 | train Loss: [0.0089, 0.0050] | valid Loss: [0.0103, 0.0055]\n",
      "Epoch 263 | train Loss: [0.0089, 0.0050] | valid Loss: [0.0103, 0.0055]\n",
      "Epoch 264 | train Loss: [0.0089, 0.0050] | valid Loss: [0.0103, 0.0055]\n",
      "Epoch 265 | train Loss: [0.0089, 0.0050] | valid Loss: [0.0103, 0.0055]\n",
      "Epoch 266 | train Loss: [0.0089, 0.0050] | valid Loss: [0.0103, 0.0055]\n",
      "Epoch 267 | train Loss: [0.0089, 0.0050] | valid Loss: [0.0103, 0.0055]\n",
      "Epoch 268 | train Loss: [0.0089, 0.0050] | valid Loss: [0.0103, 0.0055]\n",
      "Epoch 269 | train Loss: [0.0089, 0.0050] | valid Loss: [0.0103, 0.0055]\n",
      "Epoch 270 | train Loss: [0.0089, 0.0050] | valid Loss: [0.0103, 0.0055]\n",
      "Epoch 271 | train Loss: [0.0089, 0.0050] | valid Loss: [0.0103, 0.0055]\n",
      "Epoch 272 | train Loss: [0.0089, 0.0050] | valid Loss: [0.0103, 0.0055]\n",
      "Epoch 273 | train Loss: [0.0089, 0.0050] | valid Loss: [0.0103, 0.0055]\n",
      "Epoch 274 | train Loss: [0.0089, 0.0050] | valid Loss: [0.0103, 0.0055]\n",
      "Epoch 275 | train Loss: [0.0089, 0.0050] | valid Loss: [0.0103, 0.0055]\n",
      "Epoch 276 | train Loss: [0.0089, 0.0050] | valid Loss: [0.0103, 0.0055]\n",
      "Epoch 277 | train Loss: [0.0089, 0.0050] | valid Loss: [0.0103, 0.0055]\n",
      "Epoch 278 | train Loss: [0.0089, 0.0050] | valid Loss: [0.0103, 0.0055]\n",
      "Epoch 279 | train Loss: [0.0089, 0.0050] | valid Loss: [0.0103, 0.0055]\n",
      "Epoch 280 | train Loss: [0.0089, 0.0050] | valid Loss: [0.0103, 0.0055]\n",
      "Epoch 281 | train Loss: [0.0089, 0.0050] | valid Loss: [0.0103, 0.0055]\n",
      "Epoch 282 | train Loss: [0.0089, 0.0050] | valid Loss: [0.0103, 0.0055]\n",
      "Epoch 283 | train Loss: [0.0089, 0.0050] | valid Loss: [0.0103, 0.0055]\n",
      "Epoch 284 | train Loss: [0.0089, 0.0050] | valid Loss: [0.0103, 0.0055]\n",
      "Epoch 285 | train Loss: [0.0089, 0.0050] | valid Loss: [0.0103, 0.0055]\n",
      "Epoch 286 | train Loss: [0.0089, 0.0050] | valid Loss: [0.0103, 0.0055]\n",
      "Epoch 287 | train Loss: [0.0089, 0.0050] | valid Loss: [0.0103, 0.0055]\n",
      "Epoch 288 | train Loss: [0.0089, 0.0050] | valid Loss: [0.0103, 0.0055]\n",
      "Epoch 289 | train Loss: [0.0089, 0.0050] | valid Loss: [0.0103, 0.0055]\n",
      "Epoch 290 | train Loss: [0.0089, 0.0050] | valid Loss: [0.0103, 0.0055]\n",
      "Epoch 291 | train Loss: [0.0089, 0.0050] | valid Loss: [0.0103, 0.0055]\n",
      "Epoch 292 | train Loss: [0.0089, 0.0050] | valid Loss: [0.0103, 0.0055]\n",
      "Epoch 293 | train Loss: [0.0089, 0.0050] | valid Loss: [0.0103, 0.0055]\n",
      "Epoch 294 | train Loss: [0.0089, 0.0050] | valid Loss: [0.0103, 0.0055]\n",
      "Epoch 295 | train Loss: [0.0089, 0.0050] | valid Loss: [0.0103, 0.0055]\n",
      "Epoch 296 | train Loss: [0.0089, 0.0050] | valid Loss: [0.0103, 0.0055]\n",
      "Epoch 297 | train Loss: [0.0089, 0.0050] | valid Loss: [0.0103, 0.0055]\n",
      "Epoch 298 | train Loss: [0.0089, 0.0050] | valid Loss: [0.0103, 0.0055]\n",
      "Epoch 299 | train Loss: [0.0089, 0.0050] | valid Loss: [0.0103, 0.0055]\n"
     ]
    }
   ],
   "source": [
    "# モデルのインスタンス生成\n",
    "xy_dim = 2\n",
    "\n",
    "model = ActiveNet(xy_dim, dr_thresh, dropout=0, batchN=False, bias=True, Nchannels=128).to(device)\n",
    "# input data\n",
    "#data = dataset[0]\n",
    "\n",
    "# optimizer\n",
    "#optimizer = torch.optim.SGD(model.parameters(), lr=1e-4, momentum=0.9)\n",
    "#optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)#, weight_decay=5e-4)\n",
    "optimizer = torch.optim.Adadelta(model.parameters())#, rho=0.95)#, lr=1e-1, momentum=0.9)\n",
    "\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min', factor=0.5, patience=5, verbose=True)\n",
    "\n",
    "val_loss_log = []\n",
    "\n",
    "val_loss_min = np.Inf\n",
    "\n",
    "# learnig loop\n",
    "for epoch in range(300):\n",
    "    model.train()\n",
    "    for batch_x, batch_y, batch_i_dir in train_data:\n",
    "        optimizer.zero_grad()\n",
    "        lossv = 0\n",
    "        losstheta = 0\n",
    "        for ib in range(batch_x.size(0)):\n",
    "            model.load_celltypes(celltype_lst[int(batch_i_dir[ib])].to(device))\n",
    "            out = model(batch_x[ib].to(device))\n",
    "            lv, ltheta = myLoss(out, batch_y[ib].to(device))\n",
    "            lossv = lossv + lv\n",
    "            losstheta = losstheta + ltheta\n",
    "        lossv = lossv / batch_x.size(0)\n",
    "        losstheta = losstheta / batch_x.size(0)\n",
    "        (lossv+losstheta).backward()\n",
    "        optimizer.step()\n",
    "    model.eval()\n",
    "    val_lossv = 0\n",
    "    val_losstheta = 0\n",
    "    val_count = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_y, batch_i_dir in valid_data:\n",
    "            for ib in range(batch_x.size(0)):\n",
    "                model.load_celltypes(celltype_lst[int(batch_i_dir[ib])].to(device))\n",
    "                val_out = model(batch_x[ib].to(device))\n",
    "                lv, ltheta = myLoss(val_out, batch_y[ib].to(device))\n",
    "                val_lossv = val_lossv + lv\n",
    "                val_losstheta = val_losstheta + ltheta\n",
    "            val_count = val_count + batch_x.size(0)\n",
    "    val_lossv = val_lossv/val_count\n",
    "    val_losstheta = val_losstheta/val_count\n",
    "    val_loss = val_lossv + val_losstheta\n",
    "    scheduler.step(val_loss)\n",
    "    print('Epoch %d | train Loss: [%.4f, %.4f] | valid Loss: [%.4f, %.4f]' % (epoch,\n",
    "                                                                              lossv.item(), \n",
    "                                                                              losstheta.item(),\n",
    "                                                                              val_lossv.item(), \n",
    "                                                                              val_losstheta.item()))\n",
    "    val_loss_log.append([val_lossv.cpu().item(), val_losstheta.cpu().item()])\n",
    "    if val_loss.item() < val_loss_min:\n",
    "        stored_model = model\n",
    "        val_loss_min = val_loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b3cf1e91",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-01T14:15:54.832040Z",
     "start_time": "2023-03-01T14:15:54.758979Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('selfpropel', tensor(1.0141, device='cuda:1')),\n",
       "             ('interactNN.layer1.weight',\n",
       "              tensor([[-6.2266e-01,  4.4167e-01, -3.2138e-01,  2.3302e-01, -3.5725e-02,\n",
       "                        1.4890e-02],\n",
       "                      [ 1.1347e-03, -2.4960e-01, -1.8026e-02,  2.2986e-01,  1.0570e-01,\n",
       "                        1.0582e-01],\n",
       "                      [ 8.0803e-02, -6.8272e-02,  6.2502e-02,  2.8935e-02, -2.7933e-01,\n",
       "                       -3.7462e-01],\n",
       "                      [ 2.7368e-01, -3.7883e-01, -3.7455e-01, -2.8946e-02,  1.2011e-02,\n",
       "                        5.4283e-03],\n",
       "                      [-2.0050e-01,  4.9132e-02, -2.8937e-01,  1.4709e-01,  2.3163e-01,\n",
       "                        1.0594e-01],\n",
       "                      [-4.1302e-01, -2.9607e-01, -1.8509e-01, -1.3551e-01, -2.7676e-02,\n",
       "                        1.6397e-02],\n",
       "                      [-3.4916e-02,  1.0692e-01,  2.0816e-02, -1.5173e-01, -3.7849e-03,\n",
       "                        1.0648e-02],\n",
       "                      [-1.5946e-01, -2.0698e-02,  4.1921e-01, -2.0344e-01,  7.1196e-03,\n",
       "                        9.1505e-03],\n",
       "                      [ 8.0882e-01,  8.7474e-02, -1.3755e-01, -2.1862e-01, -8.7613e-03,\n",
       "                        2.4956e-02],\n",
       "                      [-1.1031e-01, -7.9919e-01,  1.9812e-01, -1.7410e-01, -6.7612e-03,\n",
       "                       -1.4623e-02],\n",
       "                      [ 1.9942e-01,  6.6291e-01,  2.4657e-01, -3.1394e-01, -8.8693e-03,\n",
       "                        1.1358e-02],\n",
       "                      [-5.0981e-01, -4.0788e-01, -4.0634e-01, -2.2643e-01,  3.2477e-03,\n",
       "                        2.4227e-01],\n",
       "                      [ 1.6245e-01, -3.8713e-01, -4.1391e-01,  3.4460e-01,  4.5770e-02,\n",
       "                       -7.1911e-02],\n",
       "                      [ 2.1458e-01,  9.3701e-02, -7.5111e-02,  1.4155e-01,  1.9317e-01,\n",
       "                       -1.8617e-01],\n",
       "                      [ 3.0417e-01,  3.4107e-01, -1.4764e-01,  1.9850e-02,  1.4085e-02,\n",
       "                        7.4668e-02],\n",
       "                      [-4.2603e-01, -8.9801e-01,  4.9843e-02,  4.6551e-02, -3.4907e-02,\n",
       "                        1.6603e-01],\n",
       "                      [ 2.0500e-01, -5.4272e-02,  2.6770e-02,  2.4260e-01, -3.4999e-02,\n",
       "                        4.6932e-03],\n",
       "                      [-4.4778e-01,  1.5165e-01, -1.0238e-01, -1.4269e-02, -2.3820e-01,\n",
       "                       -2.0516e-01],\n",
       "                      [ 3.8243e-01, -2.5311e-01, -2.0706e-01, -1.7247e-01, -3.2059e-01,\n",
       "                       -1.2853e-02],\n",
       "                      [ 1.2055e-01, -4.8975e-01,  3.2898e-01,  3.3552e-01,  7.8988e-03,\n",
       "                        2.6618e-01],\n",
       "                      [-7.3117e-03,  2.0843e-01,  1.4645e-01,  1.5866e-02,  6.1417e-02,\n",
       "                       -2.3541e-01],\n",
       "                      [ 2.4799e-01,  3.5416e-01, -1.8493e-01,  5.0815e-01, -3.2059e-02,\n",
       "                        4.5444e-02],\n",
       "                      [ 8.8483e-02, -1.1811e-01, -3.9514e-02,  7.6654e-02,  1.5466e-02,\n",
       "                       -2.3623e-01],\n",
       "                      [-3.3070e-01, -8.1915e-01, -1.0164e-01,  6.3112e-02, -2.4849e-02,\n",
       "                       -1.3132e-01],\n",
       "                      [ 2.8704e-02, -1.5077e-01,  1.1665e-01,  2.6798e-01,  5.7855e-02,\n",
       "                        5.6350e-02],\n",
       "                      [ 3.7302e-01, -3.9750e-01, -4.8323e-01,  4.2424e-01,  1.1485e-02,\n",
       "                        6.5239e-02],\n",
       "                      [-1.5502e-01,  2.0589e-02, -3.6512e-01, -6.4964e-02,  2.3097e-01,\n",
       "                       -1.4867e-03],\n",
       "                      [ 2.6858e-01, -2.9780e-01, -3.3943e-02, -1.9284e-01,  3.8496e-01,\n",
       "                       -1.8720e-01],\n",
       "                      [-4.9043e-01, -7.0489e-01,  2.4063e-02, -2.7603e-01, -9.9609e-03,\n",
       "                       -6.9475e-04],\n",
       "                      [ 9.1817e-03, -3.5817e-01, -1.8317e-01,  1.4374e-01, -1.5099e-01,\n",
       "                       -3.4676e-02],\n",
       "                      [-9.1674e-02,  2.7602e-01,  1.8892e-02, -1.9042e-01, -7.0342e-02,\n",
       "                        1.1038e-01],\n",
       "                      [-2.3661e-01,  6.2402e-01, -4.5745e-02, -2.6042e-01,  8.9314e-02,\n",
       "                        5.3332e-03],\n",
       "                      [-2.5273e-01,  3.4755e-01, -1.1454e-01, -2.3675e-01, -3.9860e-01,\n",
       "                       -2.1696e-02],\n",
       "                      [ 1.5367e-02,  3.8295e-01,  2.2427e-01,  3.6076e-02, -2.2613e-01,\n",
       "                       -4.0286e-03],\n",
       "                      [ 3.6489e-01,  9.5108e-01, -2.0347e-01,  2.9148e-02,  6.3846e-02,\n",
       "                        1.4756e-02],\n",
       "                      [-3.2902e-01,  1.9931e-01,  2.3287e-01, -2.9985e-02,  7.9527e-04,\n",
       "                       -2.3212e-01],\n",
       "                      [ 6.4272e-01,  1.1741e-01, -1.5196e-01,  1.5291e-01,  3.6083e-02,\n",
       "                        2.9608e-02],\n",
       "                      [ 1.6308e-01,  1.4523e-01,  2.1149e-02, -2.2000e-01, -2.2084e-03,\n",
       "                       -1.6223e-01],\n",
       "                      [-3.2763e-01,  2.0938e-01, -5.0156e-02, -4.3327e-01,  6.3463e-03,\n",
       "                       -8.4868e-03],\n",
       "                      [ 4.1024e-02,  1.0741e-01,  9.6088e-02, -1.8719e-01,  6.7250e-03,\n",
       "                        4.6631e-02],\n",
       "                      [-1.0441e+00, -2.9855e-01, -1.0177e-01,  2.0414e-01, -2.7699e-03,\n",
       "                        1.3944e-02],\n",
       "                      [-2.5202e-01, -3.6886e-01, -2.9880e-01,  3.5030e-01, -4.0956e-01,\n",
       "                       -2.3551e-02],\n",
       "                      [ 3.4855e-01,  4.0674e-01, -3.1081e-02,  6.9506e-02, -8.2093e-02,\n",
       "                       -1.3733e-01],\n",
       "                      [-8.1456e-03, -2.8375e-01,  2.2542e-01,  4.5842e-01, -3.6390e-03,\n",
       "                       -2.9288e-02],\n",
       "                      [-4.0463e-01, -2.3676e-01, -3.5435e-01, -1.3791e-02, -1.9308e-01,\n",
       "                       -1.1525e-01],\n",
       "                      [-4.8423e-01,  6.3571e-01, -3.0096e-01,  2.2331e-02, -8.4219e-02,\n",
       "                        3.2505e-04],\n",
       "                      [-1.2247e-01, -2.0352e-01, -2.5760e-01,  3.7823e-01,  1.5586e-01,\n",
       "                        8.5356e-02],\n",
       "                      [ 3.7661e-02,  4.8312e-01,  1.2765e-01, -4.5996e-01, -2.0358e-03,\n",
       "                        2.3051e-02],\n",
       "                      [-5.2721e-02,  2.8617e-01,  6.9367e-02, -1.7171e-01,  1.2300e-01,\n",
       "                        9.1059e-02],\n",
       "                      [-4.6752e-02,  2.8177e-01,  3.0004e-01, -2.7459e-01, -5.5876e-04,\n",
       "                        4.5660e-03],\n",
       "                      [-1.8221e-01,  3.5796e-02,  2.6436e-02,  4.5560e-01,  2.4010e-03,\n",
       "                        7.5020e-03],\n",
       "                      [-2.7328e-01,  8.1499e-01, -5.0463e-02, -1.2224e-01,  1.2826e-02,\n",
       "                        2.8019e-03],\n",
       "                      [-5.9975e-01, -1.5119e-01, -1.4236e-01,  7.1764e-02,  2.0973e-01,\n",
       "                        4.5376e-03],\n",
       "                      [-1.0192e-01,  5.4165e-01, -4.4826e-02, -1.1180e-01,  3.0331e-01,\n",
       "                        1.9417e-02],\n",
       "                      [ 2.8240e-01,  2.3105e-01, -1.1414e-01, -2.7958e-01, -2.0640e-01,\n",
       "                        9.1644e-02],\n",
       "                      [-5.2867e-01, -1.7363e-01, -3.3420e-01,  1.0212e-02, -1.5671e-02,\n",
       "                       -4.3356e-01],\n",
       "                      [-2.7052e-02, -1.0991e-01,  1.7637e-02,  1.5504e-01, -1.1046e-01,\n",
       "                        2.0046e-01],\n",
       "                      [-7.1942e-02, -1.5714e-01, -1.6392e-01, -6.0047e-02, -8.0600e-02,\n",
       "                        1.4895e-01],\n",
       "                      [-2.8996e-01,  1.9060e-02, -2.8941e-01, -4.5939e-02, -2.2341e-01,\n",
       "                        2.3440e-01],\n",
       "                      [ 9.4465e-01, -5.4671e-01, -5.6696e-02, -2.3830e-01,  3.5417e-03,\n",
       "                        1.9456e-02],\n",
       "                      [ 3.0762e-02,  1.1990e-01,  2.4374e-01,  6.5404e-02,  2.5106e-01,\n",
       "                       -7.3593e-03],\n",
       "                      [ 4.4135e-01,  5.2341e-03, -4.4022e-02,  2.3406e-01, -4.8872e-02,\n",
       "                        5.0014e-03],\n",
       "                      [-1.1818e-01, -9.9429e-02,  1.0910e-01, -1.6869e-02,  6.0207e-02,\n",
       "                       -5.6448e-02],\n",
       "                      [-1.5064e-01,  3.6312e-01, -1.7042e-02, -3.1770e-01, -1.7543e-02,\n",
       "                       -1.6024e-01],\n",
       "                      [-2.3970e-01, -7.2915e-02,  1.3232e-01,  5.4140e-01,  1.7316e-02,\n",
       "                        1.3497e-02],\n",
       "                      [-1.3088e-01, -1.5754e-01,  2.8058e-01,  3.6972e-01, -4.9791e-01,\n",
       "                        4.2961e-03],\n",
       "                      [-5.7883e-01,  1.2308e-01,  3.0232e-01,  5.1033e-02,  1.2600e-02,\n",
       "                        1.3142e-01],\n",
       "                      [ 2.5219e-01, -1.5647e-02,  1.0173e-01, -2.9734e-01,  2.5347e-01,\n",
       "                       -4.9697e-03],\n",
       "                      [ 3.7052e-01,  2.4856e-01,  9.2578e-02, -3.4531e-01,  2.1966e-01,\n",
       "                        9.6443e-02],\n",
       "                      [ 2.5240e-01, -5.3992e-01, -5.0729e-02,  7.1063e-02, -6.1118e-02,\n",
       "                        2.4002e-01],\n",
       "                      [-8.3578e-01,  1.4096e-01,  1.4236e-01, -1.3939e-01,  2.2139e-03,\n",
       "                        4.2433e-02],\n",
       "                      [ 2.1669e-01,  1.9362e-01, -2.3801e-01, -2.1792e-01,  5.0907e-02,\n",
       "                       -3.5347e-02],\n",
       "                      [ 3.1254e-01, -1.0247e-01, -8.5990e-02,  4.8851e-01, -2.1706e-02,\n",
       "                       -1.4421e-01],\n",
       "                      [ 3.3922e-01, -3.5546e-01,  1.9828e-01,  4.0280e-01,  6.1272e-03,\n",
       "                        2.5280e-02],\n",
       "                      [ 1.6138e-01, -3.0538e-02, -3.6524e-01, -2.5130e-01, -1.2059e-02,\n",
       "                       -3.3046e-01],\n",
       "                      [ 1.0502e-01, -1.0039e-01, -8.1892e-02,  6.1905e-02, -3.5478e-01,\n",
       "                        9.1963e-02],\n",
       "                      [-4.7506e-01,  8.3380e-02,  2.4490e-01,  1.2707e-01,  3.9620e-04,\n",
       "                        1.5313e-02],\n",
       "                      [-2.0428e-02, -1.4843e-01,  1.1862e-01,  8.3701e-03, -9.0004e-02,\n",
       "                       -1.0132e-01],\n",
       "                      [-3.8528e-03,  5.1247e-01, -2.5314e-02,  7.2162e-02, -1.2923e-03,\n",
       "                       -3.6330e-01],\n",
       "                      [-2.4475e-01, -2.7765e-01,  1.5250e-01,  7.1362e-02, -2.8555e-02,\n",
       "                       -2.1645e-01],\n",
       "                      [ 6.0877e-01,  6.0336e-01, -1.6173e-01,  3.8297e-01, -6.7168e-03,\n",
       "                       -4.2530e-02],\n",
       "                      [-1.1061e-02,  1.5581e-01, -6.6343e-02, -8.9925e-02, -1.9442e-03,\n",
       "                       -1.1169e-02],\n",
       "                      [-4.2844e-01,  2.6553e-01,  1.9922e-01, -5.3274e-01,  9.1524e-03,\n",
       "                        1.7225e-02],\n",
       "                      [-6.5509e-01,  5.5214e-01,  2.9790e-01,  1.0437e-01,  2.0378e-02,\n",
       "                        1.9329e-02],\n",
       "                      [ 5.4300e-01,  2.0470e-01, -1.5566e-01,  3.9456e-01, -1.4891e-02,\n",
       "                        2.0362e-02],\n",
       "                      [-2.2181e-01, -4.9541e-01, -8.7861e-02,  3.9987e-01, -1.4955e-02,\n",
       "                       -1.1071e-02],\n",
       "                      [ 2.3610e-01,  7.2847e-01,  1.6660e-01, -1.9838e-02, -1.3427e-01,\n",
       "                        1.0606e-02],\n",
       "                      [ 3.6259e-02,  6.4144e-01, -4.2124e-01, -4.6426e-02, -1.6556e-01,\n",
       "                        1.7502e-02],\n",
       "                      [-1.9517e-01,  5.2172e-02,  1.3244e-01,  3.6859e-01, -2.4721e-01,\n",
       "                       -3.4474e-02],\n",
       "                      [-7.7581e-01, -1.0286e-01, -7.5010e-02,  5.6693e-02, -4.5124e-03,\n",
       "                        4.3379e-03],\n",
       "                      [ 1.5670e-01, -5.3356e-01,  2.5449e-01, -1.0790e-01,  1.9434e-02,\n",
       "                        2.8058e-02],\n",
       "                      [-6.2050e-02,  5.8867e-01, -3.8559e-01, -3.8506e-01, -1.0804e-03,\n",
       "                        4.5792e-03],\n",
       "                      [ 5.6403e-02,  5.5876e-02, -1.5876e-01,  1.1681e-01, -2.2891e-01,\n",
       "                       -1.8495e-01],\n",
       "                      [ 2.3631e-01, -5.2203e-01, -2.7419e-01,  1.9851e-02,  6.7342e-02,\n",
       "                        2.6962e-01],\n",
       "                      [ 2.5419e-02,  2.9663e-01,  3.7921e-01, -3.2941e-01, -3.9288e-02,\n",
       "                       -3.7245e-02],\n",
       "                      [ 2.0869e-01, -2.7695e-01, -1.5194e-01,  1.3574e-01, -9.2625e-02,\n",
       "                       -1.4404e-01],\n",
       "                      [ 3.1239e-01, -9.9041e-02,  1.0142e-01, -4.3351e-03, -3.3673e-02,\n",
       "                        5.7311e-03],\n",
       "                      [ 2.8147e-01, -4.9362e-01,  1.0301e-01,  5.7780e-02, -1.5406e-02,\n",
       "                       -2.8299e-01],\n",
       "                      [-2.4437e-01,  4.6866e-01,  7.5520e-02, -6.5975e-02, -3.1261e-01,\n",
       "                        9.1704e-02],\n",
       "                      [-1.2401e-01, -4.3778e-01,  1.1814e-02,  4.0348e-01,  3.5765e-03,\n",
       "                        1.8422e-03],\n",
       "                      [-4.3988e-02, -2.5057e-01, -2.0383e-01,  2.2070e-01, -1.7135e-03,\n",
       "                       -1.0740e-01],\n",
       "                      [-2.7825e-01, -5.9888e-01, -5.2436e-02,  6.0839e-01, -3.5066e-03,\n",
       "                        6.7856e-03],\n",
       "                      [-1.5712e-01, -1.6098e-02,  1.8952e-01,  4.6425e-02, -2.7812e-01,\n",
       "                       -2.7905e-01],\n",
       "                      [-2.4518e-01,  8.8081e-02,  1.0742e-01,  1.8243e-01,  5.2609e-03,\n",
       "                       -2.1281e-01],\n",
       "                      [-1.3422e-01,  2.5609e-01, -6.9211e-03, -3.5617e-01,  9.4126e-02,\n",
       "                       -1.1310e-01],\n",
       "                      [ 2.3913e-02,  4.1960e-03, -1.2553e-01, -1.3285e-01,  9.4742e-02,\n",
       "                        6.0435e-02],\n",
       "                      [-4.7168e-03,  2.1664e-01, -2.8978e-02, -4.7185e-01, -7.2901e-03,\n",
       "                        5.2518e-03],\n",
       "                      [ 3.6712e-03,  1.0861e-01,  3.5065e-01,  1.1920e-01,  5.2959e-03,\n",
       "                        2.1170e-02],\n",
       "                      [ 7.9526e-02, -5.2348e-02,  2.3488e-01,  6.0956e-02, -3.9587e-01,\n",
       "                       -3.9827e-01],\n",
       "                      [-7.4572e-01,  4.5781e-01, -8.1917e-02,  1.6901e-02,  2.1929e-03,\n",
       "                        3.5506e-02],\n",
       "                      [ 2.4325e-02, -4.9248e-01,  7.2118e-02,  3.9992e-01, -5.1779e-02,\n",
       "                       -2.4498e-01],\n",
       "                      [ 4.8983e-02,  5.8084e-01,  4.4431e-02,  1.0622e-01,  2.1257e-01,\n",
       "                        1.2951e-01],\n",
       "                      [-2.5007e-01,  3.0480e-01, -1.0491e-01, -1.1938e-01,  3.2148e-01,\n",
       "                       -2.8923e-01],\n",
       "                      [ 5.6100e-02, -6.3902e-01,  9.2512e-02, -4.0963e-02, -4.3293e-03,\n",
       "                       -2.5852e-01],\n",
       "                      [-3.0786e-01, -1.9268e-01, -2.5835e-01,  1.0597e-01, -1.6285e-02,\n",
       "                       -2.8948e-01],\n",
       "                      [-1.9906e-01,  6.2709e-01, -4.4842e-01, -3.1317e-01, -1.0027e-02,\n",
       "                        1.2535e-01],\n",
       "                      [-5.7297e-02,  1.6842e-02, -3.5469e-02, -1.5911e-01, -4.8579e-03,\n",
       "                        7.9150e-02],\n",
       "                      [-4.5465e-01, -1.2170e-01, -1.2418e-01,  8.5435e-02, -4.8916e-02,\n",
       "                        1.2677e-01],\n",
       "                      [ 9.2321e-02,  1.2022e-01, -1.8194e-02, -2.9582e-01,  1.0838e-01,\n",
       "                        6.3149e-03],\n",
       "                      [ 7.1762e-01,  8.1050e-02,  2.1103e-02, -1.8242e-01,  1.3233e-02,\n",
       "                        1.3311e-03],\n",
       "                      [-3.0708e-01, -3.2792e-02, -3.1834e-01, -1.0511e-01, -6.3145e-04,\n",
       "                       -3.3517e-01],\n",
       "                      [-1.0646e-01,  4.1374e-01, -2.9914e-01,  3.2821e-01,  2.0690e-01,\n",
       "                       -2.7445e-01],\n",
       "                      [ 4.5790e-01,  7.7261e-02,  1.1229e-01, -1.8517e-01, -8.2475e-02,\n",
       "                        1.8620e-02],\n",
       "                      [ 2.5924e-01,  2.1197e-01,  2.0597e-01, -5.1662e-01, -3.9663e-01,\n",
       "                       -8.9147e-03],\n",
       "                      [-3.1250e-01, -4.8465e-01, -2.0031e-01,  5.7992e-02,  1.8030e-01,\n",
       "                        1.3593e-02],\n",
       "                      [ 4.2674e-01, -1.5005e-02,  2.2505e-01, -1.6568e-02, -1.8549e-02,\n",
       "                       -2.3997e-01],\n",
       "                      [ 3.0190e-01,  2.3105e-01,  2.9933e-03, -2.7853e-01,  1.6272e-01,\n",
       "                        7.8322e-02],\n",
       "                      [ 2.0214e-02,  8.0475e-01, -8.7691e-02, -2.6483e-01, -2.3349e-02,\n",
       "                       -3.6303e-02]], device='cuda:1')),\n",
       "             ('interactNN.layer1.bias',\n",
       "              tensor([-0.3535, -0.0929,  0.0232,  0.2435, -0.0990,  0.0658, -0.0169, -0.3454,\n",
       "                       0.0529, -0.3194, -0.0233, -0.3478,  0.3522, -0.0274, -0.0131, -0.3712,\n",
       "                      -0.0292,  0.0140,  0.0649, -0.4713, -0.1657, -0.0658,  0.0824, -0.0491,\n",
       "                      -0.1814,  0.3867,  0.1722, -0.4106, -0.2327, -0.3521, -0.0061, -0.1588,\n",
       "                      -0.1939, -0.2863, -0.0760, -0.1071, -0.0833, -0.0090, -0.0961, -0.3010,\n",
       "                       0.0987,  0.0226, -0.0035, -0.2017,  0.2200,  0.1479, -0.2079, -0.0145,\n",
       "                      -0.1261, -0.2507, -0.2218, -0.1801, -0.3146, -0.3035,  0.0614, -0.0404,\n",
       "                      -0.1303,  0.0955,  0.0625, -0.0938, -0.3803, -0.0877, -0.0376,  0.1257,\n",
       "                      -0.1717, -0.2042, -0.4839, -0.4156, -0.4086, -0.3387, -0.2224,  0.2380,\n",
       "                       0.0691, -0.1107,  0.2354,  0.0704,  0.0224,  0.0028, -0.1496, -0.1527,\n",
       "                      -0.4126,  0.0991, -0.0759, -0.3267,  0.1964, -0.0160, -0.2586,  0.2416,\n",
       "                      -0.2555, -0.0630, -0.3802,  0.2635,  0.1330, -0.2212, -0.3460, -0.3208,\n",
       "                       0.2288, -0.2671, -0.1663,  0.0606,  0.2090,  0.0433, -0.1277, -0.0875,\n",
       "                      -0.2178,  0.1041, -0.0982, -0.3073, -0.3373, -0.0900,  0.0021, -0.3868,\n",
       "                      -0.4017, -0.3099,  0.2543,  0.0527,  0.0049, -0.1757, -0.0838, -0.2856,\n",
       "                      -0.3575, -0.3326, -0.2470, -0.1823, -0.0826, -0.2611, -0.1637, -0.1035],\n",
       "                     device='cuda:1')),\n",
       "             ('interactNN.layer2.weight',\n",
       "              tensor([[ 1.6290e-02, -5.6964e-02,  3.8989e-02,  ...,  8.3282e-02,\n",
       "                       -5.0535e-02,  7.4995e-03],\n",
       "                      [-6.7584e-03,  1.4306e-02, -3.9022e-02,  ..., -9.2362e-04,\n",
       "                       -5.5000e-02, -2.3103e-02],\n",
       "                      [ 1.5391e-01, -8.6441e-02,  2.8696e-02,  ..., -5.4987e-02,\n",
       "                        3.7919e-02,  1.0563e-01],\n",
       "                      ...,\n",
       "                      [-1.3149e-02,  1.1133e-01, -1.5065e-05,  ..., -1.9492e-02,\n",
       "                        4.2792e-02,  4.9505e-02],\n",
       "                      [ 7.3852e-02, -9.3686e-02, -5.1714e-03,  ..., -4.9577e-02,\n",
       "                        1.3620e-02, -5.8598e-02],\n",
       "                      [ 5.4565e-02,  8.0057e-02, -1.3416e-02,  ...,  1.2658e-02,\n",
       "                       -6.8214e-02,  7.1442e-02]], device='cuda:1')),\n",
       "             ('interactNN.layer2.bias',\n",
       "              tensor([-0.0581, -0.0308, -0.2576, -0.0768,  0.1335, -0.4002, -0.1156, -0.1546,\n",
       "                      -0.0457,  0.0741, -0.0735,  0.2098,  0.0326, -0.1122, -0.0396, -0.2206,\n",
       "                      -0.1658, -0.1722, -0.1497, -0.2062, -0.1783, -0.0482, -0.1224, -0.5499,\n",
       "                      -0.0651,  0.0251, -0.0630,  0.0263, -0.1305, -0.3150, -0.0212, -0.0262,\n",
       "                       0.1075, -0.3717, -0.0609,  0.0059, -0.1214, -0.3883, -0.0039, -0.0493,\n",
       "                      -0.0311, -0.3148, -0.3220, -0.4658, -0.2906, -0.0543, -0.3322,  0.0345,\n",
       "                      -0.3870,  0.5001, -0.0836, -0.4041,  0.1056, -0.0202, -0.0058,  0.4486,\n",
       "                      -0.4407, -0.1771,  0.0079,  0.1264,  0.0044,  0.0016,  0.0724, -0.3993,\n",
       "                      -0.0748,  0.0512,  0.0013,  0.1194, -0.0060,  0.1766, -0.2446,  0.1645,\n",
       "                      -0.2766, -0.2205, -0.3087, -0.1011, -0.2910,  0.1208, -0.0447, -0.0946,\n",
       "                      -0.2137, -0.2694,  0.0192, -0.0725,  0.1230, -0.0698, -0.1454,  0.1868,\n",
       "                       0.2149, -0.6024, -0.1138,  0.2279,  0.2736,  0.2718,  0.2015, -0.0745,\n",
       "                      -0.0276, -0.2993, -0.0116, -0.0536, -0.1778, -0.3866, -0.1604,  0.0063,\n",
       "                      -0.2432, -0.0881, -0.4732, -0.4628, -0.0922, -0.4728, -0.1437, -0.0296,\n",
       "                       0.1311, -0.0228, -0.0851, -0.6146,  0.0826, -0.0548, -0.2448, -0.1183,\n",
       "                      -0.2937, -0.2867, -0.4059, -0.1101, -0.4677, -0.0771, -0.0163, -0.2492],\n",
       "                     device='cuda:1')),\n",
       "             ('interactNN.layer3.weight',\n",
       "              tensor([[-0.0236, -0.0672, -0.0114,  ..., -0.0651,  0.0056,  0.0604],\n",
       "                      [-0.0471, -0.0600, -0.0805,  ...,  0.0080, -0.0515,  0.0695],\n",
       "                      [-0.0719, -0.0529,  0.0321,  ..., -0.0375,  0.0345, -0.1416],\n",
       "                      ...,\n",
       "                      [-0.0305, -0.0693,  0.0643,  ...,  0.0280,  0.0097,  0.0549],\n",
       "                      [-0.0354,  0.0732,  0.0292,  ...,  0.0368, -0.0139, -0.0532],\n",
       "                      [-0.0297, -0.0528,  0.1152,  ..., -0.0056,  0.0499, -0.1162]],\n",
       "                     device='cuda:1')),\n",
       "             ('interactNN.layer3.bias',\n",
       "              tensor([ 0.0479,  0.0625, -0.2428,  0.0049,  0.3562, -0.1239,  0.2717,  0.3242,\n",
       "                      -0.0991, -0.2454,  0.3783,  0.0647,  0.3509,  0.1899, -0.1100,  0.1525,\n",
       "                      -0.1458, -0.0656,  0.2202, -0.0612,  0.3184, -0.0247, -0.0865, -0.0341,\n",
       "                      -0.1595, -0.1291,  0.0503, -0.1282,  0.2836, -0.0915,  0.2994,  0.3456,\n",
       "                      -0.0890,  0.3119,  0.3748, -0.1336, -0.0573,  0.0271, -0.3670, -0.2228,\n",
       "                      -0.3706, -0.2558, -0.1065, -0.2783, -0.0995,  0.1071,  0.2148, -0.0759,\n",
       "                      -0.3367, -0.3182, -0.1518, -0.0963, -0.2274,  0.0522,  0.1015,  0.3643,\n",
       "                       0.3747, -0.0927,  0.2998,  0.2681,  0.3052,  0.0256,  0.1395, -0.0277,\n",
       "                       0.0042,  0.2114, -0.0076, -0.0461, -0.0630, -0.0890,  0.1402,  0.3016,\n",
       "                       0.4235,  0.0964,  0.2497, -0.1194, -0.0847, -0.0357, -0.1692, -0.0440,\n",
       "                      -0.2771, -0.0054,  0.3946, -0.0405,  0.0737, -0.0521,  0.4052, -0.2370,\n",
       "                      -0.2150,  0.0758,  0.0199, -0.0889, -0.1948,  0.0508,  0.2672, -0.1201,\n",
       "                       0.1060, -0.0287,  0.1763, -0.0087, -0.1802, -0.1217,  0.1036, -0.3720,\n",
       "                      -0.0828, -0.3095, -0.0910,  0.3821, -0.0285, -0.1133, -0.0463,  0.3125,\n",
       "                       0.3551, -0.3649,  0.2347,  0.0626, -0.1037, -0.0070, -0.4200, -0.2242,\n",
       "                      -0.1396,  0.2553,  0.1046,  0.0266,  0.2786, -0.1577,  0.0137,  0.0882],\n",
       "                     device='cuda:1')),\n",
       "             ('interactNN.layer4.weight',\n",
       "              tensor([[ 0.2540, -0.0221,  0.2320,  0.2048, -0.1624, -0.0006,  0.1461,  0.2684,\n",
       "                       -0.0897,  0.3256,  0.1327,  0.0581,  0.2966, -0.2817, -0.0715,  0.0333,\n",
       "                       -0.0336,  0.0108, -0.3266, -0.0507, -0.4180,  0.1196,  0.0790,  0.0783,\n",
       "                       -0.2785,  0.3763,  0.2154, -0.1444, -0.2899,  0.1151,  0.2282,  0.3860,\n",
       "                       -0.1125,  0.0517,  0.3195,  0.2070, -0.0578,  0.1278,  0.2628, -0.2116,\n",
       "                        0.1832,  0.1799, -0.0961,  0.2907,  0.1729, -0.0788, -0.1415,  0.0035,\n",
       "                       -0.1760,  0.1527, -0.0443,  0.0858, -0.1554, -0.2646,  0.1392,  0.1309,\n",
       "                       -0.1570, -0.1702,  0.2540, -0.3732, -0.2786, -0.1310,  0.2950,  0.0139,\n",
       "                       -0.1307, -0.4102, -0.0934, -0.1371,  0.0583,  0.1362,  0.1452,  0.0128,\n",
       "                        0.0424, -0.5143,  0.3492, -0.0094,  0.0575,  0.0041, -0.1339,  0.0777,\n",
       "                       -0.3037,  0.0197, -0.2997,  0.2347,  0.0826,  0.0676, -0.2605, -0.1357,\n",
       "                       -0.1111, -0.2086, -0.1318,  0.0831, -0.1181,  0.2245, -0.1251, -0.1249,\n",
       "                        0.0678, -0.0749,  0.2108, -0.0618,  0.1638, -0.0678,  0.2147,  0.2099,\n",
       "                        0.0111,  0.0497,  0.0706, -0.3073, -0.2298,  0.0285,  0.0127,  0.1592,\n",
       "                       -0.2377, -0.2708,  0.2376,  0.2607, -0.0496, -0.0179, -0.2690,  0.1497,\n",
       "                       -0.1413, -0.2044,  0.0139,  0.1562, -0.3749, -0.0077, -0.0097,  0.0947],\n",
       "                      [-0.0600,  0.0525,  0.2167, -0.1450,  0.3012, -0.1559, -0.3652,  0.2629,\n",
       "                        0.1918,  0.1492, -0.3946,  0.1906, -0.2972,  0.2049, -0.1056, -0.3743,\n",
       "                       -0.1968,  0.1035,  0.0717, -0.0659, -0.2613, -0.1482, -0.0077,  0.1376,\n",
       "                        0.0043,  0.0310, -0.0383, -0.1038,  0.0888,  0.0112,  0.2647, -0.2310,\n",
       "                        0.1698,  0.3009,  0.1279,  0.1562, -0.0866,  0.0307, -0.0595,  0.0280,\n",
       "                        0.0934, -0.1269, -0.0393, -0.2604,  0.0277, -0.3653, -0.4180, -0.0685,\n",
       "                        0.3839, -0.0606,  0.0316,  0.1032, -0.1516,  0.1843, -0.1351,  0.3152,\n",
       "                        0.2856,  0.2005, -0.0739,  0.0866,  0.0521, -0.1219,  0.3401, -0.1238,\n",
       "                       -0.0721, -0.0117, -0.0760,  0.1728, -0.2168,  0.1485, -0.1255,  0.2224,\n",
       "                       -0.4833,  0.1507,  0.0691, -0.2169,  0.0258, -0.1949,  0.0158, -0.0278,\n",
       "                        0.2185,  0.0123, -0.3077, -0.1018,  0.0804,  0.0814, -0.2417,  0.1405,\n",
       "                       -0.0127, -0.1562,  0.0506,  0.1423,  0.2480,  0.0996,  0.4137, -0.1376,\n",
       "                        0.1406, -0.0344,  0.2470, -0.0332,  0.0329, -0.1569,  0.0281,  0.0826,\n",
       "                        0.0883,  0.1831, -0.0137, -0.0466,  0.0495, -0.0617, -0.0066,  0.1340,\n",
       "                        0.2255,  0.1804,  0.3328,  0.0775, -0.0479,  0.0333,  0.1661,  0.0215,\n",
       "                       -0.0704,  0.1185, -0.3824,  0.1503, -0.0107, -0.0786,  0.0113,  0.0144]],\n",
       "                     device='cuda:1')),\n",
       "             ('interactNN.layer4.bias',\n",
       "              tensor([-0.0381,  0.0040], device='cuda:1')),\n",
       "             ('thetaDotNN.layer1.weight',\n",
       "              tensor([[-1.8644e-01, -1.3041e-01, -3.9492e-02,  1.8479e-01,  2.9678e-01,\n",
       "                        2.1346e-02],\n",
       "                      [ 4.1687e-01, -3.5378e-01,  6.5985e-03,  3.3073e-01,  3.2155e-02,\n",
       "                       -9.0097e-02],\n",
       "                      [ 2.3730e-01, -2.1794e-02,  2.0178e-02,  2.1433e-01, -1.6688e-01,\n",
       "                        1.6892e-01],\n",
       "                      [ 7.3330e-01, -1.6645e-01, -9.4552e-02,  4.7550e-01,  2.8703e-01,\n",
       "                       -2.5294e-01],\n",
       "                      [-3.0474e-01,  1.5993e-02,  2.4007e-02,  2.8484e-01, -1.6416e-01,\n",
       "                        3.7859e-01],\n",
       "                      [-7.7407e-02, -5.0620e-01,  1.7341e-01,  1.4278e-01,  2.0165e-01,\n",
       "                       -5.6990e-02],\n",
       "                      [ 3.7142e-01, -1.9791e-01, -1.1654e-01, -4.6624e-01, -3.0904e-01,\n",
       "                        3.1652e-01],\n",
       "                      [ 5.4174e-01, -1.9103e-01,  3.0379e-01,  2.8543e-01, -2.2150e-01,\n",
       "                       -2.3910e-01],\n",
       "                      [-4.7270e-02, -2.9148e-01,  7.0176e-02, -1.1094e-01,  6.4148e-02,\n",
       "                        2.3018e-02],\n",
       "                      [-3.4024e-02, -2.3521e-01,  3.1106e-01, -1.1489e-01,  3.4115e-01,\n",
       "                       -3.3455e-01],\n",
       "                      [ 5.3549e-01, -2.4414e-01, -4.9256e-02, -9.5489e-02,  1.9446e-01,\n",
       "                        6.7384e-02],\n",
       "                      [-6.7899e-01, -2.8615e-01,  1.3522e-01, -1.3986e-01,  2.1407e-01,\n",
       "                       -1.2814e-02],\n",
       "                      [-3.0529e-01,  1.7025e-01,  2.2231e-01,  2.6625e-01, -2.2922e-02,\n",
       "                        2.3012e-01],\n",
       "                      [ 5.9518e-02, -4.1982e-01, -2.3816e-01, -4.5136e-01,  3.0058e-01,\n",
       "                       -4.7672e-02],\n",
       "                      [-4.0855e-02, -2.9973e-01, -2.2412e-01,  1.2729e-01, -3.3982e-01,\n",
       "                       -2.1892e-01],\n",
       "                      [-2.5861e-01, -2.5017e-01, -1.4807e-01, -1.6687e-01, -2.7010e-01,\n",
       "                       -1.3417e-02],\n",
       "                      [ 7.9819e-02, -1.0746e-02,  2.6675e-01,  1.6910e-01, -9.8894e-03,\n",
       "                        3.4698e-01],\n",
       "                      [-2.7155e-01, -2.1735e-01, -2.2057e-01, -2.1344e-01, -2.1493e-01,\n",
       "                        2.2422e-01],\n",
       "                      [ 2.3922e-01, -4.3913e-01, -1.5513e-01,  1.7848e-02, -1.6379e-03,\n",
       "                       -1.3945e-01],\n",
       "                      [-2.2471e-01,  4.1549e-02, -1.4114e-01,  5.3262e-01,  2.9909e-01,\n",
       "                       -3.6885e-01],\n",
       "                      [-1.9966e-01, -1.3150e-01, -2.0206e-01,  1.4639e-01,  6.1256e-02,\n",
       "                       -1.2729e-01],\n",
       "                      [-8.0443e-02, -5.3370e-01, -2.7409e-01, -1.1994e-01, -1.6526e-01,\n",
       "                       -1.1036e-02],\n",
       "                      [-3.8154e-01,  2.7197e-01, -1.3783e-01,  3.5873e-01, -5.9327e-02,\n",
       "                        2.8737e-01],\n",
       "                      [-5.0445e-02,  4.5359e-01, -2.0124e-01, -1.6848e-01,  1.7501e-01,\n",
       "                       -7.0898e-02],\n",
       "                      [ 1.7855e-01,  2.7264e-01, -6.4200e-02, -1.5324e-02, -3.1116e-01,\n",
       "                        3.4995e-01],\n",
       "                      [ 2.2106e-01,  7.8703e-01,  1.7228e-01, -4.2082e-01, -2.1074e-01,\n",
       "                        2.0113e-01],\n",
       "                      [-3.5549e-01, -2.0306e-01, -4.2796e-01,  1.9094e-01, -3.6504e-01,\n",
       "                       -8.0852e-02],\n",
       "                      [-2.8717e-02, -5.3730e-01, -1.5374e-01, -8.6296e-02,  1.3836e-01,\n",
       "                       -2.1385e-01],\n",
       "                      [ 4.5307e-01,  1.8836e-01,  8.4734e-02,  7.6192e-02, -3.2769e-01,\n",
       "                       -3.2806e-02],\n",
       "                      [ 2.2702e-01, -1.8732e-02,  2.5502e-01,  3.9501e-01, -2.5216e-01,\n",
       "                        6.9427e-02],\n",
       "                      [-4.3484e-01,  2.0373e-01, -3.5323e-01, -1.2538e-01, -2.5189e-01,\n",
       "                       -3.3662e-01],\n",
       "                      [-2.5899e-01, -1.8439e-01, -4.5089e-02, -3.8797e-01, -2.7664e-01,\n",
       "                       -1.0377e-02],\n",
       "                      [-2.0613e-01,  1.9960e-01, -2.5561e-01, -2.5906e-01,  2.1280e-01,\n",
       "                        3.6024e-01],\n",
       "                      [-1.5394e-01, -3.6223e-02,  7.7392e-02,  2.3961e-01,  1.3875e-01,\n",
       "                       -6.0206e-02],\n",
       "                      [-1.5319e-01, -1.3767e-01, -3.0529e-01,  1.0090e-01,  2.6092e-01,\n",
       "                        2.6975e-01],\n",
       "                      [ 5.0602e-01, -2.4709e-02,  1.0895e-01, -1.9454e-01,  2.2008e-01,\n",
       "                        8.0747e-02],\n",
       "                      [-4.4516e-01,  8.5159e-01, -2.3131e-01, -2.8423e-01,  5.6436e-02,\n",
       "                       -6.2751e-02],\n",
       "                      [ 4.7432e-01, -3.3701e-01,  3.2610e-01,  3.1134e-01, -1.1705e-01,\n",
       "                        1.7115e-01],\n",
       "                      [ 2.3082e-01,  2.2595e-01, -1.7260e-01, -1.5419e-01, -9.5850e-02,\n",
       "                        9.4416e-02],\n",
       "                      [-4.0665e-01, -1.2135e-01, -1.3305e-01, -2.1971e-01,  2.7917e-01,\n",
       "                        1.8593e-01],\n",
       "                      [-2.9436e-01, -3.4557e-01,  7.2326e-02, -1.0618e-01,  2.3337e-01,\n",
       "                        2.3857e-01],\n",
       "                      [-2.6674e-01, -1.7368e-01,  4.3216e-02, -3.9471e-01, -3.8781e-01,\n",
       "                       -5.2863e-02],\n",
       "                      [-1.0468e-01, -1.1500e-01, -1.6527e-01,  5.1163e-02,  3.2304e-01,\n",
       "                        3.8095e-01],\n",
       "                      [-5.6189e-01, -2.7814e-01,  6.1804e-02, -2.1948e-01, -2.0404e-01,\n",
       "                        3.3329e-02],\n",
       "                      [-4.3827e-01, -5.8962e-01,  2.0361e-02, -1.5024e-01,  2.8279e-01,\n",
       "                       -5.1301e-02],\n",
       "                      [ 1.6283e-01,  7.0188e-02, -6.1545e-02, -4.6991e-01, -8.7951e-02,\n",
       "                       -3.1075e-01],\n",
       "                      [-8.7659e-02, -6.5179e-01, -2.2242e-01,  4.1934e-01,  1.0941e-01,\n",
       "                        2.4138e-01],\n",
       "                      [-2.6809e-01,  2.9291e-01,  3.7071e-01,  4.5430e-01, -2.0403e-02,\n",
       "                       -1.4999e-02],\n",
       "                      [ 3.7160e-01,  2.2086e-02,  1.9349e-02, -3.8253e-01,  1.4028e-01,\n",
       "                        1.2731e-01],\n",
       "                      [ 3.2209e-01,  2.7578e-01, -3.4157e-01, -3.2083e-01, -2.9846e-01,\n",
       "                       -3.3421e-02],\n",
       "                      [ 3.9691e-01,  4.1293e-01, -1.3321e-01, -2.1006e-01, -1.8707e-01,\n",
       "                       -9.4114e-03],\n",
       "                      [ 3.3215e-01, -1.4924e-01,  1.7913e-01,  2.1710e-01, -3.2158e-01,\n",
       "                       -1.5499e-01],\n",
       "                      [ 4.4488e-01, -3.5989e-01,  2.7295e-02, -1.9867e-01,  3.1943e-01,\n",
       "                        4.1879e-02],\n",
       "                      [-5.0255e-01, -9.9522e-02, -8.4785e-02, -6.0222e-02, -3.3022e-01,\n",
       "                        3.3412e-02],\n",
       "                      [ 4.0057e-01, -4.4819e-01, -3.3358e-01, -4.3242e-01, -1.9904e-01,\n",
       "                        2.3904e-01],\n",
       "                      [ 2.2981e-01, -3.1431e-01, -2.7129e-01, -2.7735e-01, -2.1563e-01,\n",
       "                        2.1827e-01],\n",
       "                      [ 1.6396e-01,  3.5584e-01,  4.0743e-01,  4.0620e-01,  5.8902e-02,\n",
       "                       -3.9336e-02],\n",
       "                      [-6.3322e-01,  1.7511e-03,  5.0570e-01,  3.9277e-02, -1.6240e-01,\n",
       "                        2.4191e-01],\n",
       "                      [-6.9996e-01,  7.4894e-02,  9.9904e-02, -1.4549e-02,  1.6641e-02,\n",
       "                        3.8311e-02],\n",
       "                      [ 3.5100e-01,  4.3322e-01,  1.4416e-01,  2.6094e-01, -2.8461e-01,\n",
       "                       -1.8443e-02],\n",
       "                      [-3.4975e-01,  4.6151e-01,  9.4620e-02, -6.3706e-01,  1.6721e-01,\n",
       "                        1.4175e-01],\n",
       "                      [-2.5315e-02,  2.1683e-02,  4.8492e-02,  2.5758e-01,  1.1548e-01,\n",
       "                        9.0323e-02],\n",
       "                      [ 1.4453e-01, -1.9732e-01,  1.0936e-01, -3.8867e-01,  1.7675e-01,\n",
       "                       -2.6546e-01],\n",
       "                      [ 8.9504e-02, -1.6245e-02, -2.7379e-01,  4.7512e-01, -3.8317e-01,\n",
       "                        1.9405e-02],\n",
       "                      [-2.6660e-01,  5.5344e-02, -1.2407e-01,  1.8884e-01, -2.3310e-01,\n",
       "                        1.6383e-01],\n",
       "                      [-2.7076e-01, -4.2155e-01,  9.0864e-02, -2.3549e-01,  2.2233e-01,\n",
       "                       -3.8493e-02],\n",
       "                      [-4.2825e-01, -3.9150e-02, -1.2239e-01,  3.2054e-01, -1.8813e-01,\n",
       "                       -1.5236e-01],\n",
       "                      [-1.5841e-01,  3.3566e-01, -3.6847e-01, -2.1982e-01, -6.5863e-02,\n",
       "                       -2.4074e-01],\n",
       "                      [-4.8011e-01,  2.2082e-01, -2.4933e-01,  4.0070e-01,  2.5158e-01,\n",
       "                       -1.0587e-01],\n",
       "                      [-6.7245e-04,  3.1149e-01, -1.4018e-01,  4.8441e-01,  3.6090e-01,\n",
       "                        1.6176e-01],\n",
       "                      [-6.9872e-02,  2.9238e-01, -1.4838e-01, -2.9205e-01, -1.1656e-01,\n",
       "                        2.1369e-01],\n",
       "                      [ 2.4994e-02, -1.5967e-02, -2.1203e-01,  2.0581e-01,  1.6382e-01,\n",
       "                        1.3130e-01],\n",
       "                      [ 1.1029e-01,  1.8951e-01,  1.6158e-01,  4.0695e-01,  3.3533e-01,\n",
       "                       -1.5417e-02],\n",
       "                      [-3.1861e-01, -3.8629e-02,  1.5400e-01,  5.4355e-02,  4.5739e-02,\n",
       "                        2.8298e-01],\n",
       "                      [-1.1202e-01,  1.3758e-01, -2.4546e-01, -2.4260e-01, -5.1725e-02,\n",
       "                        1.7161e-01],\n",
       "                      [-2.2764e-01, -2.4600e-01,  3.3993e-01,  3.8100e-01, -4.2576e-01,\n",
       "                        8.2506e-02],\n",
       "                      [-7.4487e-02, -3.3837e-01,  2.9122e-01,  2.7025e-01, -4.1290e-02,\n",
       "                        3.9110e-01],\n",
       "                      [ 1.7191e-01,  1.7641e-01, -1.1283e-01, -2.0888e-01, -2.3398e-01,\n",
       "                        2.2566e-02],\n",
       "                      [-2.5761e-01, -4.3187e-01,  2.1647e-01, -4.6257e-02,  3.9870e-02,\n",
       "                        1.7134e-01],\n",
       "                      [-9.6804e-02, -4.0136e-01, -1.6579e-01, -2.2070e-01,  1.7818e-01,\n",
       "                        4.2853e-01],\n",
       "                      [ 3.0186e-01,  6.8517e-01,  4.2914e-02, -4.3283e-01,  2.2557e-01,\n",
       "                        1.2813e-01],\n",
       "                      [ 4.0663e-01, -2.9505e-01,  2.0510e-01, -1.7933e-01, -1.7894e-01,\n",
       "                        1.9049e-01],\n",
       "                      [ 4.4328e-01, -3.4538e-01,  3.1414e-01,  1.0154e-01, -2.0099e-01,\n",
       "                        1.9403e-02],\n",
       "                      [-1.6841e-01,  1.5864e-01, -2.6888e-01,  4.5944e-01, -1.0081e-01,\n",
       "                       -3.3307e-01],\n",
       "                      [-4.7709e-01,  3.3365e-02,  1.0420e-01, -1.8842e-01,  2.7701e-01,\n",
       "                       -2.2305e-01],\n",
       "                      [ 1.0261e-01, -4.5729e-01, -4.1651e-01,  4.1672e-01,  5.5912e-02,\n",
       "                       -3.3060e-01],\n",
       "                      [-3.7014e-01, -1.6037e-01,  2.5255e-01,  2.3760e-01,  5.0416e-02,\n",
       "                       -1.5327e-01],\n",
       "                      [-2.2596e-01, -5.7473e-02, -4.0866e-01, -1.7559e-01, -3.2803e-01,\n",
       "                       -1.4257e-01],\n",
       "                      [ 4.4158e-01,  4.4602e-01, -1.8531e-01,  3.4400e-01, -2.1262e-02,\n",
       "                        2.9582e-01],\n",
       "                      [ 2.6207e-01,  6.3425e-01, -7.7461e-03,  4.1121e-01,  4.9058e-02,\n",
       "                        8.2347e-02],\n",
       "                      [ 7.7411e-02, -5.6269e-01,  1.2731e-01, -9.8014e-02, -2.3464e-02,\n",
       "                        1.0750e-01],\n",
       "                      [-4.3912e-01,  1.9078e-01, -2.5888e-01,  3.6612e-01,  2.6048e-01,\n",
       "                        2.2925e-01],\n",
       "                      [-2.6612e-01, -3.4980e-01,  3.9250e-01, -1.5797e-01, -3.1633e-01,\n",
       "                       -3.2147e-01],\n",
       "                      [ 5.9733e-01, -3.0758e-02, -9.1857e-02,  3.8168e-02,  2.0657e-01,\n",
       "                       -1.0797e-01],\n",
       "                      [-2.9038e-01, -3.4739e-01, -2.4693e-01,  3.2069e-01, -1.0579e-01,\n",
       "                        3.7365e-01],\n",
       "                      [-3.4786e-01, -4.7729e-01, -1.2399e-02,  5.6244e-01, -4.6332e-02,\n",
       "                       -7.6690e-02],\n",
       "                      [-2.3421e-01,  4.1346e-01,  1.3248e-01,  2.7196e-01,  2.6058e-03,\n",
       "                       -1.1173e-01],\n",
       "                      [ 3.7514e-01,  7.8108e-02,  1.7441e-01, -3.8167e-01, -1.4992e-02,\n",
       "                       -2.3289e-01],\n",
       "                      [-3.1696e-01, -5.0738e-01, -3.1545e-01, -3.3391e-01,  1.6365e-01,\n",
       "                        3.6155e-01],\n",
       "                      [-1.9067e-01,  8.3763e-02,  1.6175e-01, -2.8418e-01,  3.1138e-01,\n",
       "                       -4.0889e-01],\n",
       "                      [ 6.5217e-03,  1.5682e-01,  5.9153e-02, -5.0025e-01, -1.9554e-01,\n",
       "                        1.5203e-01],\n",
       "                      [ 4.6817e-01, -1.7394e-01, -3.5557e-01, -1.6012e-01,  2.7478e-01,\n",
       "                       -2.5352e-01],\n",
       "                      [ 2.8054e-01, -2.7466e-01, -2.5153e-01, -3.8231e-01,  6.6085e-02,\n",
       "                        1.0802e-01],\n",
       "                      [-5.1280e-01, -2.5813e-02,  4.1938e-01, -2.3826e-01,  2.0768e-01,\n",
       "                       -3.2668e-01],\n",
       "                      [ 3.8602e-01,  2.0901e-01, -8.2277e-02,  3.0675e-01,  3.8051e-01,\n",
       "                        1.6646e-02],\n",
       "                      [-6.4390e-03, -1.9929e-01,  9.8811e-02,  2.4296e-01,  3.8153e-01,\n",
       "                       -1.2494e-01],\n",
       "                      [-8.7298e-02, -4.9079e-01,  2.6534e-01,  8.7014e-02,  2.5028e-01,\n",
       "                       -2.2388e-01],\n",
       "                      [-3.1297e-01, -2.7141e-01, -3.0076e-01,  4.4528e-01,  9.9280e-03,\n",
       "                       -3.0154e-01],\n",
       "                      [-1.1512e-03,  4.0682e-02, -5.8700e-02,  4.7836e-02,  2.9455e-02,\n",
       "                       -5.1983e-04],\n",
       "                      [-1.0997e-01, -1.8294e-01, -4.0187e-01,  1.9422e-01, -5.2728e-02,\n",
       "                        1.5216e-01],\n",
       "                      [-3.1513e-01,  7.1033e-02,  1.6639e-02, -3.3485e-01, -1.8963e-01,\n",
       "                       -3.3928e-01],\n",
       "                      [ 1.4940e-01,  2.7620e-01,  5.8502e-02,  4.1198e-01,  2.2654e-01,\n",
       "                       -3.4801e-01],\n",
       "                      [-3.7534e-02, -1.9650e-01, -1.7250e-02, -4.3837e-02,  7.5616e-02,\n",
       "                       -3.0796e-01],\n",
       "                      [ 2.4116e-01, -3.0188e-01, -3.5554e-01, -3.2116e-01,  2.1092e-02,\n",
       "                       -2.4559e-01],\n",
       "                      [-6.8039e-02,  1.5157e-01, -1.3372e-01, -8.3031e-02, -6.7558e-02,\n",
       "                       -1.0844e-01],\n",
       "                      [-4.1517e-01,  2.2589e-01,  1.8565e-01, -2.7684e-01,  4.0284e-02,\n",
       "                       -4.5291e-02],\n",
       "                      [ 2.3215e-01, -4.0771e-01, -1.9676e-01, -2.5257e-01, -3.4051e-02,\n",
       "                       -3.9769e-01],\n",
       "                      [ 1.9598e-01, -2.4085e-01, -9.0382e-02, -4.0526e-01, -1.5738e-02,\n",
       "                       -1.6175e-01],\n",
       "                      [ 4.3941e-02, -5.0890e-02, -3.7904e-02, -3.6341e-01,  1.2142e-01,\n",
       "                        1.3818e-01],\n",
       "                      [-4.2473e-01,  1.0857e-01,  3.2087e-01, -2.7927e-01,  3.0789e-01,\n",
       "                       -2.9320e-01],\n",
       "                      [ 6.8933e-01, -3.4539e-03,  3.4384e-02, -2.7894e-01,  1.5973e-01,\n",
       "                        3.4780e-01],\n",
       "                      [ 3.6719e-01,  4.1474e-01,  2.3261e-01, -1.5048e-01, -2.2764e-01,\n",
       "                       -1.2814e-01],\n",
       "                      [-5.3605e-01, -5.8124e-02,  2.3128e-01,  6.3641e-02, -1.7934e-01,\n",
       "                       -2.0362e-01],\n",
       "                      [ 4.1159e-02, -2.7301e-01,  4.2802e-02,  2.9011e-02, -1.7908e-01,\n",
       "                        2.3487e-01],\n",
       "                      [-3.5927e-01, -2.6393e-01, -8.2207e-02, -1.9584e-01,  3.5627e-01,\n",
       "                       -1.0192e-01],\n",
       "                      [ 4.4036e-01, -8.7399e-02,  4.0592e-02,  3.2424e-01,  1.1074e-02,\n",
       "                        1.6656e-01],\n",
       "                      [ 2.0035e-01, -8.2555e-03, -3.0758e-01,  9.3864e-02,  9.2428e-02,\n",
       "                        2.4124e-01],\n",
       "                      [ 3.1245e-01, -9.8543e-02,  2.6747e-01,  1.8126e-01,  7.5866e-02,\n",
       "                       -2.8489e-02]], device='cuda:1')),\n",
       "             ('thetaDotNN.layer1.bias',\n",
       "              tensor([-0.1340,  0.2718, -0.0200,  0.3526, -0.3636, -0.1732, -0.0473,  0.2598,\n",
       "                      -0.0699, -0.3085,  0.0746, -0.3106, -0.0480,  0.2202,  0.2289,  0.0277,\n",
       "                      -0.2500,  0.1602,  0.2609, -0.0499, -0.2855,  0.2848, -0.3239,  0.2010,\n",
       "                      -0.3653, -0.1447, -0.0213, -0.0230,  0.3860, -0.0707,  0.3533,  0.3559,\n",
       "                      -0.3174, -0.3270,  0.1500,  0.0896,  0.2312, -0.0698, -0.2891, -0.0461,\n",
       "                      -0.2258,  0.3976, -0.3853, -0.0359, -0.3613,  0.3462,  0.3070, -0.1309,\n",
       "                      -0.0194,  0.1892,  0.2805, -0.0765, -0.3084,  0.3981, -0.3238,  0.0825,\n",
       "                      -0.3019, -0.2040, -0.1552, -0.3612,  0.2328, -0.0464, -0.0113,  0.0314,\n",
       "                       0.1981, -0.0911, -0.0307, -0.1121,  0.1753, -0.1581,  0.1419,  0.0529,\n",
       "                      -0.1617, -0.2565, -0.1033,  0.3634,  0.5587,  0.0363, -0.2353, -0.0594,\n",
       "                       0.5556, -0.1449, -0.0761,  0.0946, -0.2410,  0.4167, -0.3165,  0.3910,\n",
       "                       0.0241, -0.3821, -0.2106,  0.1392, -0.1991,  0.1311, -0.3070,  0.4885,\n",
       "                      -0.1076,  0.0736, -0.1065,  0.1881, -0.0566,  0.0405,  0.0033, -0.1668,\n",
       "                      -0.1605, -0.3541,  0.5016,  0.2358,  0.4093,  0.1154,  0.2772,  0.0026,\n",
       "                      -0.0675,  0.1242, -0.1887, -0.4048, -0.0471, -0.0696,  0.0173, -0.3857,\n",
       "                      -0.1076,  0.0997,  0.1514, -0.1790,  0.0823,  0.0456,  0.2994,  0.1208],\n",
       "                     device='cuda:1')),\n",
       "             ('thetaDotNN.layer2.weight',\n",
       "              tensor([[-0.0485,  0.0092,  0.0166,  ...,  0.0406, -0.0389, -0.0194],\n",
       "                      [ 0.0033,  0.0052,  0.0568,  ...,  0.0793,  0.0426,  0.0597],\n",
       "                      [ 0.0239, -0.0280, -0.0629,  ...,  0.0606,  0.0130, -0.0546],\n",
       "                      ...,\n",
       "                      [-0.0960,  0.0155, -0.0561,  ..., -0.0709, -0.0793,  0.0847],\n",
       "                      [ 0.0464, -0.0786, -0.0157,  ..., -0.0256, -0.0402,  0.0389],\n",
       "                      [-0.0704, -0.0058,  0.0330,  ..., -0.0629,  0.0405,  0.0611]],\n",
       "                     device='cuda:1')),\n",
       "             ('thetaDotNN.layer2.bias',\n",
       "              tensor([ 0.0096,  0.0504,  0.0662, -0.1759,  0.2226, -0.0032, -0.0218,  0.1510,\n",
       "                      -0.0335, -0.0312,  0.0030, -0.0647, -0.0077, -0.0292,  0.2682,  0.3038,\n",
       "                      -0.0782,  0.0133, -0.1027, -0.0122, -0.1194,  0.0011, -0.0090, -0.0259,\n",
       "                      -0.1048, -0.1114, -0.0554, -0.0986, -0.1498, -0.0534, -0.0670, -0.0221,\n",
       "                      -0.0402, -0.1088, -0.0396, -0.0701, -0.0410, -0.0526,  0.1350, -0.0775,\n",
       "                       0.0170, -0.0163, -0.2669, -0.0268, -0.1439,  0.0461, -0.0594, -0.0825,\n",
       "                      -0.0052,  0.0218, -0.0618, -0.0677, -0.1212,  0.0485,  0.2351,  0.0114,\n",
       "                       0.0035, -0.0627,  0.1382, -0.0852,  0.0805,  0.2013, -0.1578, -0.1688,\n",
       "                      -0.0521,  0.0709,  0.0638, -0.0928, -0.1430, -0.1879, -0.0338, -0.1066,\n",
       "                      -0.0272, -0.0022, -0.0466,  0.0358,  0.0975,  0.0238, -0.1455, -0.2303,\n",
       "                       0.1096, -0.0099, -0.1428, -0.0173, -0.0938, -0.0863, -0.1508, -0.0912,\n",
       "                       0.0256,  0.1318,  0.0560, -0.1582, -0.0625,  0.0360, -0.1244,  0.0191,\n",
       "                      -0.0885,  0.0131,  0.1549, -0.1188, -0.0065, -0.2046, -0.0251, -0.0900,\n",
       "                      -0.0665,  0.0228, -0.0512,  0.0127, -0.0136, -0.0300, -0.0319, -0.0683,\n",
       "                      -0.1784,  0.0641,  0.2622,  0.0282, -0.0284, -0.1688, -0.0478, -0.0766,\n",
       "                      -0.0928,  0.0754, -0.0922,  0.0525, -0.0176, -0.0620, -0.0771, -0.0256],\n",
       "                     device='cuda:1')),\n",
       "             ('thetaDotNN.layer3.weight',\n",
       "              tensor([[-0.0300,  0.0486,  0.0408,  ..., -0.0224, -0.0018,  0.0879],\n",
       "                      [-0.0284,  0.0851,  0.0026,  ...,  0.0701,  0.0837, -0.0105],\n",
       "                      [ 0.0667, -0.0669,  0.0550,  ...,  0.0513, -0.0365,  0.0613],\n",
       "                      ...,\n",
       "                      [-0.0732, -0.0580, -0.0422,  ..., -0.0337, -0.0235, -0.0393],\n",
       "                      [ 0.0667, -0.0538, -0.0707,  ..., -0.0705,  0.0482, -0.0855],\n",
       "                      [-0.0691,  0.0735, -0.0114,  ..., -0.0731, -0.0585, -0.0074]],\n",
       "                     device='cuda:1')),\n",
       "             ('thetaDotNN.layer3.bias',\n",
       "              tensor([ 3.9700e-01, -1.9808e-01, -1.2822e-01, -2.3932e-02,  9.3957e-03,\n",
       "                      -9.0507e-02, -1.1004e-01,  1.1800e-01,  2.8524e-01, -1.2192e-01,\n",
       "                       2.4452e-01, -1.1028e-01,  1.7883e-01,  2.6021e-01, -3.4357e-02,\n",
       "                      -6.5764e-02, -2.9052e-01, -3.7995e-01, -5.9316e-02, -5.8677e-02,\n",
       "                      -7.3402e-02, -1.7969e-01, -2.6998e-01,  1.7672e-01, -4.5540e-02,\n",
       "                      -3.4073e-01, -1.2104e-01, -2.6995e-01,  2.2514e-01, -2.8381e-02,\n",
       "                      -1.1761e-01, -9.6617e-02, -9.7029e-03, -1.1080e-01, -9.0502e-02,\n",
       "                      -1.9686e-01, -1.8469e-02, -4.6608e-02,  3.8913e-01, -1.7420e-01,\n",
       "                       1.5885e-02,  2.2638e-01, -2.8385e-02, -7.3153e-02, -8.2369e-02,\n",
       "                      -8.3380e-02, -3.1857e-03,  7.7477e-02, -3.6411e-01,  5.9133e-02,\n",
       "                      -2.4726e-01, -1.2393e-01, -4.0052e-02, -6.5062e-02, -1.0431e-01,\n",
       "                      -3.4438e-02, -2.4539e-01, -1.4891e-03, -5.8953e-02,  2.2777e-04,\n",
       "                      -1.2945e-02,  1.6512e-01,  3.3532e-01, -1.3272e-02,  2.8924e-01,\n",
       "                      -2.0959e-01,  1.4080e-01,  1.9720e-01,  1.6392e-01, -2.4627e-01,\n",
       "                      -9.0562e-02, -2.3578e-01,  2.4302e-01, -5.1502e-02,  2.4072e-02,\n",
       "                      -1.4031e-01,  1.4517e-02, -6.7448e-02, -9.0732e-02, -1.9133e-02,\n",
       "                      -3.9028e-01, -3.1081e-02,  1.8319e-02, -2.3407e-01, -4.5175e-02,\n",
       "                       2.5437e-01,  3.1923e-01, -3.2937e-01,  1.6178e-01, -3.7538e-01,\n",
       "                      -1.9448e-01, -2.6688e-02, -7.3323e-02, -2.0079e-02,  2.8209e-01,\n",
       "                       3.9630e-02, -3.3590e-01,  2.5141e-02, -1.9855e-02,  2.5546e-01,\n",
       "                      -7.2041e-02,  8.5756e-02,  8.8821e-03,  1.0639e-01, -7.7264e-02,\n",
       "                       1.2053e-02,  1.1525e-02, -3.4199e-01,  3.1584e-01, -6.3230e-02,\n",
       "                      -3.7605e-01,  1.7983e-01, -1.0165e-01, -2.4135e-02, -9.3548e-02,\n",
       "                       9.0538e-02, -4.3277e-02,  1.4250e-01,  1.3306e-01, -2.8256e-01,\n",
       "                      -3.4297e-01, -2.5701e-01, -1.2031e-01, -1.7737e-01, -3.3538e-02,\n",
       "                      -2.4908e-02, -2.2014e-01,  2.7450e-02], device='cuda:1')),\n",
       "             ('thetaDotNN.layer4.weight',\n",
       "              tensor([[-0.2562,  0.1856,  0.0399,  0.0232,  0.0383, -0.0420,  0.1120,  0.0826,\n",
       "                       -0.3113,  0.0268,  0.2683, -0.1215,  0.2471, -0.2593, -0.0119, -0.1003,\n",
       "                        0.2634, -0.2931, -0.0872, -0.0758,  0.0773,  0.1120, -0.1519, -0.4697,\n",
       "                       -0.0986,  0.1150,  0.1365,  0.1121,  0.3026, -0.0576,  0.1106,  0.0738,\n",
       "                        0.0971,  0.1164, -0.0844, -0.1464,  0.0811,  0.0847, -0.2827,  0.1246,\n",
       "                        0.1156, -0.2776,  0.0041,  0.1004, -0.0711, -0.0110,  0.0579,  0.5445,\n",
       "                        0.3150,  0.0907,  0.1867,  0.0697, -0.1162, -0.0981, -0.0869, -0.0749,\n",
       "                       -0.2098, -0.1169,  0.0464, -0.0594,  0.0137,  0.2495, -0.2280, -0.0716,\n",
       "                        0.1850,  0.1360,  0.2280,  0.3228,  0.4602,  0.1621, -0.0808,  0.1246,\n",
       "                        0.2264,  0.0836, -0.1253,  0.0636, -0.0497, -0.0857,  0.0345,  0.0698,\n",
       "                       -0.2159, -0.0141, -0.1966,  0.2594, -0.0984,  0.3177, -0.1852,  0.1766,\n",
       "                        0.5103, -0.4056, -0.1285,  0.1461,  0.1208, -0.0361,  0.2421,  0.1158,\n",
       "                        0.2150,  0.1066, -0.1037,  0.2058,  0.0721,  0.2420,  0.0228, -0.3454,\n",
       "                       -0.0631, -0.1070, -0.1723, -0.2607, -0.2437,  0.0184, -0.2551, -0.1166,\n",
       "                       -0.0580,  0.0685, -0.0039, -0.1540,  0.0496,  0.2571, -0.2555,  0.2660,\n",
       "                        0.2788, -0.2340,  0.0925, -0.1081,  0.0129,  0.0504, -0.1886,  0.0316]],\n",
       "                     device='cuda:1')),\n",
       "             ('thetaDotNN.layer4.bias', tensor([-0.0193], device='cuda:1'))])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "44265599",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-01T14:16:03.095447Z",
     "start_time": "2023-03-01T14:15:54.834939Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss: [0.0102, 0.0055]\n"
     ]
    }
   ],
   "source": [
    "# モデルを評価モードに設定\n",
    "stored_model.eval()\n",
    "\n",
    "# 推論\n",
    "test_lossv = 0\n",
    "test_losstheta = 0\n",
    "test_count = 0\n",
    "with torch.no_grad():\n",
    "    for batch_x, batch_y, batch_i_dir in test_data:\n",
    "        for ib in range(batch_x.size(0)):\n",
    "            model.load_celltypes(celltype_lst[int(batch_i_dir[ib])].to(device))\n",
    "            test_out = model(batch_x[ib].to(device))\n",
    "            lv, ltheta = myLoss(test_out, batch_y[ib].to(device))\n",
    "            test_lossv = test_lossv + lv\n",
    "            test_losstheta = test_losstheta + ltheta\n",
    "        test_count = test_count + batch_x.size(0)\n",
    "test_lossv = test_lossv/test_count\n",
    "test_losstheta = test_losstheta/test_count\n",
    "print('test Loss: [%.4f, %.4f]' % (test_lossv.item(), test_losstheta.item()))\n",
    "test_loss = [test_lossv.item(), test_losstheta.item()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d0283033",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-01T14:16:03.104876Z",
     "start_time": "2023-03-01T14:16:03.098794Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import datetime\n",
    "\n",
    "now = datetime.datetime.now()\n",
    "nowstr = now.strftime('%Y%m%d_%H%M%S')\n",
    "\n",
    "os.makedirs(savedirName + nowstr + '/', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "445915ef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-01T14:16:03.115087Z",
     "start_time": "2023-03-01T14:16:03.106680Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0141, device='cuda:1')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stored_model.selfpropel.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "62154d31",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-01T14:16:03.151614Z",
     "start_time": "2023-03-01T14:16:03.118355Z"
    }
   },
   "outputs": [],
   "source": [
    "stored_model = stored_model.to('cpu')\n",
    "\n",
    "filename1 = savedirName + nowstr + '/' + nowstr + '_Model.pkl'\n",
    "with open(filename1, \"wb\") as f:\n",
    "    pickle.dump(stored_model, f)\n",
    "\n",
    "filename1_2 = savedirName + nowstr + '/' + nowstr + '_Model.pt'\n",
    "torch.save(stored_model, filename1_2)\n",
    "\n",
    "filename2 = savedirName + nowstr + '/' + nowstr\n",
    "torch.save(stored_model.interactNN.state_dict(), filename2 + '_interactNN.pkl')\n",
    "torch.save(stored_model.thetaDotNN.state_dict(), filename2 + '_thetaDotNN.pkl')\n",
    "torch.save(stored_model.selfpropel.detach(), filename2 + '_selfpropel.pkl')\n",
    "\n",
    "filename3 = savedirName + nowstr + '/' + nowstr + '_Separation.npz'\n",
    "np.savez(filename3, dr_thresh=dr_thresh, batch_size=batch_size,\n",
    "         train_inds=train_inds, valid_inds=valid_inds, test_inds=test_inds, \n",
    "         val_loss_log=val_loss_log, test_loss=test_loss)\n",
    "\n",
    "filename4 = savedirName + nowstr + '/' + nowstr + '_optimizer.pt'\n",
    "torch.save(optimizer, filename4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0bd69a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "270.85px",
    "left": "494px",
    "right": "20px",
    "top": "120px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
